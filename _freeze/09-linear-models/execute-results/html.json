{
  "hash": "d54494ade33f3693dd01bd53c63f2280",
  "result": {
    "engine": "knitr",
    "markdown": "# Linear models {#sec-linear}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nIn the last chapter we conducted a simple analysis of Darwin's maize data using R. We worked out confidence intervals 'by hand'. This simple method allowed us to learn more about analysis, estimates, standard error and confidence. But it is also slow, and it relied on the assumptions of a *z-distribution* to assess true differences between the groups. \n\nWe will now work through a much more efficient way to carry out comparisons, we will use the functions in R that let us perform a **linear model** analysis.\n\n### Packages\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(emmeans)\nlibrary(performance)\nlibrary(broom.helpers)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## A linear model analysis for calculating means\n\nR has a general function `lm()` for fitting linear models, this is part of base R (does not require the tidyverse packages). We will run through a few different iterations of the linear model increasing in complexity. We will often want to fit several models to our data, so a common way to work is to fit a model and assign it to a named R object, so that we can extract data from when we need it. \n\nIn the example below I have called the model `lsmodel0`, short for \"least-squares model 0\", this is because the linear-model uses a technique called [least squares](https://www.youtube.com/watch?v=PaFPbb66DxQ). \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlsmodel0 <- lm(formula = height ~ 1, data = darwin)\n```\n:::\n\n\n\n\n:::{.callout-tip}\nYou can pipe into the lm() function, but when we use functions that are \"outside\" of the tidyverse family we need to put a `_` where the data should go (as it is usually not the first argument).\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlsmodel0 <- darwin |> \n             lm(height ~ 1, data= _)\n```\n:::\n\n\n\n\n:::\n\nThe first argument of the `lm()` function is formula (we won't write this out in full in the future) - and this specifies we want to analyse a **response** variable (height) as a function of an **explanatory** variable using the *tilde* symbol (~).\n\nThe simplest possible model ignores any explanatory variables, instead the `1` indicates we just want to estimate an intercept. \n\nWithout explanatory variables this means the formula will just estimate the overall mean height of **all** the plants in the dataset.\n\n\n## Summaries for models\n\nWhen you have made a linear model, we can investigate a summary of the model using the base R function `summary()`. There is also a tidyverse option provided by the package `broom`(@R-broom).\n\n### Broom\n\nbroom summarizes key information about models in tidy `tibble()s`. broom provides three verbs to make it convenient to interact with model objects:\n\n* `broom::tidy()` summarizes information about model components\n\n* `broom::glance()` reports information about the entire model\n\n* `broom::augment()` adds informations about individual observations to a dataset and it can be used to model predictions onto a new dataset.\n\n### Model summary\n\n::: {.panel-tabset}\n\n## Base R\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(lsmodel0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = height ~ 1, data = darwin)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.8833 -1.3521 -0.0083  2.4917  4.6167 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  18.8833     0.5808   32.52   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.181 on 29 degrees of freedom\n```\n\n\n:::\n:::\n\n\n\n\n## Broom\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbroom::tidy(lsmodel0)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term        | estimate| std.error| statistic| p.value|\n|:-----------|--------:|---------:|---------:|-------:|\n|(Intercept) | 18.88333| 0.5807599|  32.51487|       0|\n\n</div>\n:::\n:::\n\n\n\n\n:::\n\n\nThe output above is called the *table of coefficients*. The 18.9 is the *estimate of the model coefficient* (in this case it is the overall mean), together with its standard error (SEM). The first row in any R model output is always labelled the 'Intercept' and the challenge is usually to workout what that represents. In this case we can prove that this is the same as the overall mean as follows:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmean(darwin$height)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18.88333\n```\n\n\n:::\n:::\n\n\n\n\n\nThis simple model allows us to understand what the `lm()` function does. \n\n### Compare means\n\nWhat we really want is a linear model that analyses the *difference* in average plant height (`type`) as a function of pollination type. We can use the `lm()` function to fit this as a linear model as follows:\n\n::: {.panel-tabset}\n\n## Base R\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlsmodel1 <- lm(height ~ type, data=darwin)\n\n# note that the following is identical\n\n# lsmodel1 <- lm(height ~ 1 + type, data=darwin)\n\nsummary(lsmodel1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = height ~ type, data = darwin)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.1917 -1.0729  0.8042  1.9021  3.3083 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  20.1917     0.7592  26.596   <2e-16 ***\ntypeSelf     -2.6167     1.0737  -2.437   0.0214 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.94 on 28 degrees of freedom\nMultiple R-squared:  0.175,\tAdjusted R-squared:  0.1455 \nF-statistic:  5.94 on 1 and 28 DF,  p-value: 0.02141\n```\n\n\n:::\n:::\n\n\n\n\n## Broom\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlsmodel1 <- lm(height ~ type, data=darwin)\n\nbroom::tidy(lsmodel1)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term        |  estimate| std.error| statistic|   p.value|\n|:-----------|---------:|---------:|---------:|---------:|\n|(Intercept) | 20.191667| 0.7592028| 26.595880| 0.0000000|\n|typeSelf    | -2.616667| 1.0736749| -2.437113| 0.0214145|\n\n</div>\n:::\n:::\n\n\n\n\n:::\n\n\nNow the model formula contains the pollination type in addition to an intercept.\n\n### Coefficients\n\nThis is where the model results are reported.\n\n- **Intercept**: This is the predicted height when the plant type is \"Cross\" fertilised, which is the reference category). The estimate is 20.1917, which means the average height for plants that are not self-fertilised is about 20.19 inches.\n\n- **typeSelf:** This coefficient tells us how the height of \"Self\" fertilized plants **differs from the reference group** (\"Cross\" fertilized plants). The estimate is -2.6167, meaning \"Self\" fertilized plants are, on average, 2.62 units shorter than \"Cross\" fertilized plants.\n\n:::{.callout-note}\n\nYou can confirm this for yourself: \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndarwin |> \n  group_by(type) |> \n  summarise(mean=mean(height))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|type  |     mean|\n|:-----|--------:|\n|Cross | 20.19167|\n|Self  | 17.57500|\n\n</div>\n:::\n:::\n\n\n\n\n:::\n\n\n- **The Std. Error** for each coefficient measures the uncertainty in the estimate. Smaller values indicate more precise estimates. For example, the standard error of 1.0737 for the \"typeSelf\" coefficient suggests there's some variability in how much shorter the \"Self\" fertilized plants are, but it’s reasonably precise.\n\n- **The t value** is the ratio of the estimate to its standard error ($\\frac{Mean}{Std. Error}$). The larger the t-value (either positive or negative), the more evidence there is that the estimate is different from zero.\n\n- **Pr(>|t|)** gives the p-value, which tells us the probability of observing a result as extreme as this, assuming that there is no real effect (i.e., the null hypothesis is true). In this case, a p-value of 0.0214 means there’s about a 2% chance that the difference in height between \"Self\" and \"Cross\" fertilized plants is due to random chance. Since this is below the typical cutoff of 0.05, it suggests that the difference is statistically significant.\n\n- **Significance codes**: These symbols next to the p-value indicate how strong the evidence is. In this case, one star (*) indicates a p-value below 0.05, meaning the effect is considered statistically significant.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Annotation of the summary function output](images/model summary.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n<div class='webex-solution'><button>The t-statistic</button>\n\n\nThe t-statistic is closely related to the z-statistic but is used in significance testing when the sample size is small or the population standard deviation is unknown.\nAs we almost never know the \"population\" standard deviation, it is always safer to use the t-distribution for significance tests. \n\n### The t-Distribution:\n\nThe t-distribution is similar to the normal distribution but has more spread (wider tails), especially with smaller sample sizes. As the sample size increases, the t-distribution approaches the normal distribution.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-linear-models_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n</div>\n\n\n\n### A simple write up\n\nFrom this summary model we can make conclusions about the effect of inbreeding and produce a simple write up:\n\n\"The maize plants that have been cross pollinated had an average height (mean ±S.E.) of 20.19 (± 0.76) inches and were taller on average than the self-pollinated plants, with a mean difference in height of 2.62 (±1.07) inches (t(28) = -2.44, p = 0.02)\"\n\n\n## Confidence intervals\n\nA confidence interval is a range of values that we expect to contain the true population parameter (like a mean or difference in means) with a certain level of confidence. In the case of a 95% confidence interval, this means we are 95% confident that the true parameter lies within the interval.\n\nThe confidence interval is closely tied to the standard error (SE), which measures how much variability we expect in the sample estimates due to sampling randomness. The standard error depends on the sample size and the variability in the data, and it’s computed as:\n\n$$\nStandard~Error(SE)= \\frac{SD}{\\sqrt(n)}\n$$\nFor a 95% confidence interval, we use the t-distribution to determine how many standard errors away from the sample estimate we should go to capture the true population parameter with 95% confidence. In most cases:\n\n$$\nConfidence~Interval=Mean(\\pm(criticial~t) \\times SE)\n$$\nWhere:\n\n- critical t = is the critical value from the t-distribution that corresponds to a 95% confidence level\n\n- Standard Error - the precision of the sample estimate\n\n### Confidence intervals and p-values\n\nA p-value and a confidence interval are inversely related, but they provide information in different ways:\n\n- A **p-value** tells you how likely it is to observe your data (or something more extreme) if the null hypothesis is true. It’s a single number that quantifies the evidence against the null hypothesis. If the p-value is less than the significance level (typically 0.05), you reject the null hypothesis.\n\n- A **confidence interval** provides a range of plausible values for the true parameter. If the confidence interval does not include 0 (or another hypothesized value, such as a difference of 0 between two groups), it suggests the parameter is significantly different from 0 at the corresponding confidence level (usually 95%).\n\nHere’s how they are inversely related:\n\n- **p-value < 0.05**: If the p-value is below 0.05, this typically means the null hypothesis (no effect or no difference) is rejected, suggesting a statistically significant result. In this case, a 95% confidence interval for the effect will not include 0, indicating the result is statistically significant.\n\n- **p-value > 0.05**: If the p-value is greater than 0.05, this means we do not reject the null hypothesis. In this scenario, the 95% confidence interval will likely include 0, indicating that the true effect could be zero and the result is not statistically significant.\n\n### Confidence intervals (CI) in R\n\nWith a wrapper function around our model we can generate accurate 95% confidence intervals from the SE and calculated t-distribution:\n\n::: {.panel-tabset}\n\n## Base R\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconfint(lsmodel1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               2.5 %     97.5 %\n(Intercept) 18.63651 21.7468231\ntypeSelf    -4.81599 -0.4173433\n```\n\n\n:::\n:::\n\n\n\n\n## Broom\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbroom::tidy(lsmodel1, conf.int=T)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term        |  estimate| std.error| statistic|   p.value| conf.low|  conf.high|\n|:-----------|---------:|---------:|---------:|---------:|--------:|----------:|\n|(Intercept) | 20.191667| 0.7592028| 26.595880| 0.0000000| 18.63651| 21.7468231|\n|typeSelf    | -2.616667| 1.0736749| -2.437113| 0.0214145| -4.81599| -0.4173433|\n\n</div>\n:::\n:::\n\n\n\n\n:::\n\nBecause this follows the same layout as the table of coefficients, the output intercept row gives a 95% CI for the height of the crossed plants and the second row gives a 95% interval for the *difference in height between crossed and selfed plants*. The lower and upper bounds are the 2.5% and 97.5% of a *t*-distribution. \n\nIt is this difference in height in which we are specifically interested. \n\n### Answering the question\n\nDarwin's original hypothesis was that self-pollination would reduce fitness (using height as a proxy for this). The null hypothesis is that there is no effect of pollination type, and therefore no difference in the average heights. \n\nWe must ask ourselves if our experiment is consistent with this null hypothesis or can we reject it? If we choose to reject the null hypothesis, with what level of confidence can we do so?\n\nTo do this, we can simply determine whether or not the predicted value of our null hypothesis (a  difference of zero) lies inside the 95% CI for the difference of the mean. \n\nIf our confidence intervals contain zero (or no difference), then we cannot establish a difference between our sample difference in height (-2.62 inches) from the null prediction of zero difference, given the level of variability (noise) in our data. \n\nIn this case we can see that the upper and lower bounds of the confidence intervals **do not contain zero**. The difference in height is consistent with Darwin's alternate hypothesis of inbreeding depression. \n\n### Plausible range of significant difference\n\n- A 95% CI provides a range of values where the true effect (like a difference between two group means) is likely to fall. Our analysis suggests that within the 95% confidence interval the true (population) difference could be between 0.42 and 4.8 inches in height difference between crossed and selfed plants. \n- **Minimum effect size**: the value closest to zero gives a sense of the **minimum effect size** that is plausible at 95% confidence. This is important because it shows the smallest effect you might expect. Here the minimum effect size is 0.42 inches. Or that the cost of inbreeding could be as little as a 0.42 inch height difference. \n\nThe `GGally` package has a handy `ggcoef_model()` function, that produces a graph of the estimated mean difference with an approx 95% CI. As we can see we are able to reject the null hypothesis at a 95% confidence level. \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Generate a coefficient plot for the linear regression model using ggcoef_model\nGGally::ggcoef_model(lsmodel1,\n                     show_p_values = FALSE,\n                     conf.level = 0.95)\n```\n\n::: {.cell-output-display}\n![](09-linear-models_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n::: {.callout-note}\nSet the confidence levels to 99%, do you think the difference between treatments is still statistically significant at an \\alpha of 0.01?\n:::\n\nIf we increase the level of confidence (from 95% to 99%, roughly 2 SE to 3 SE), then we may find that we cannot reject the null hypothesis at a higher threshold of confidence (p < 0.01). Try altering the `conf.level` argument above for yourself to see this in action. \n\nWe can also include this argument in the `tidy()` function if we wish to:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbroom::tidy(lsmodel1, conf.int=T, conf.level=0.99)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term        |  estimate| std.error| statistic|   p.value|  conf.low|  conf.high|\n|:-----------|---------:|---------:|---------:|---------:|---------:|----------:|\n|(Intercept) | 20.191667| 0.7592028| 26.595880| 0.0000000| 18.093790| 22.2895433|\n|typeSelf    | -2.616667| 1.0736749| -2.437113| 0.0214145| -5.583512|  0.3501789|\n\n</div>\n:::\n:::\n\n\n\n\n## Estimating means\n\nOne limitation of the table of coefficients output is that it doesn't provide the mean and standard error of the *other* treatment level (only the difference between them). If we wish to calculate the \"other\" mean and SE then we can get R to do this.\n\n### Changing the intercept\n\nOne way to do this is to change the levels of the type variable as a factor:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Perform linear regression on darwin data with Self as the intercept\ndarwin |> \n  # Convert 'type' column to a factor\n  mutate(type = factor(type)) |>\n  # Relevel 'type' column to specify the order of factor levels\n  mutate(type = fct_relevel(type, c(\"Self\", \"Cross\"))) |>\n  # Fit linear regression model with 'height' as the response variable and 'type' as the predictor\n  lm(height ~ type, data = _) |>\n  # Tidy the model summary\n  broom::tidy()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term        |  estimate| std.error| statistic|   p.value|\n|:-----------|---------:|---------:|---------:|---------:|\n|(Intercept) | 17.575000| 0.7592028| 23.149282| 0.0000000|\n|typeCross   |  2.616667| 1.0736749|  2.437113| 0.0214145|\n\n</div>\n:::\n:::\n\n\n\n\nAfter releveling, the self treatment is now taken as the intercept, and we get the estimate for it's mean and standard error\n\n### Emmeans\n\nWe could also use the package [`emmeans`](https://aosmith.rbind.io/2019/03/25/getting-started-with-emmeans/) and its function `emmeans()` to do a similar thing\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate estimated marginal means (EMMs) using emmeans package\nmeans <- emmeans::emmeans(lsmodel1, specs = ~ type)\n\nmeans\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n type  emmean    SE df lower.CL upper.CL\n Cross   20.2 0.759 28     18.6     21.7\n Self    17.6 0.759 28     16.0     19.1\n\nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\n\n\nThe advantage of emmeans is that it provides the mean, standard error and 95% confidence interval estimates of all levels from the model at once (e.g. it relevels the model multiple times behind the scenes). \n\n\n`emmeans` also gives us a handy summary to include in data visuals that combine raw data and statistical inferences. These are standard `ggplot()` outputs so can be [customised as much as you want](#intro-to-grammar).\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Convert the 'means' object to a tibble\nmeans |>\n  as_tibble() |>\n  # Create a plot using ggplot\n  ggplot(aes(x = type, y = emmean)) +\n  # Add point estimates with error bars\n  geom_pointrange(aes(ymin = lower.CL, ymax = upper.CL))\n```\n\n::: {.cell-output-display}\n![](09-linear-models_files/figure-html/unnamed-chunk-20-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\nNotice that no matter how we calculate the estimated SE (and therefore the 95% CI) of both treatments is the same. This is because as mentioned earlier the variance is a pooled estimate, e.g. variance is not being calculate separately for each group. The only difference you should see in SE across treatments will be if there is a difference in *sample size* between groups. \n\n:::{.callout-tip}\n\nNotice how the Confidence Intervals of the estimated means strongly overlap, there is a difference between the two SEMs and the SED we have calculated. So overlapping error bars cannot be used to infer significance. \n\n:::\n\nBecause of this pooled variance, there is an assumption that variance is equal across the groups, this and other assumptions of the linear model should be checked. We cannot trust our results if the assumptions of the model are not adequately met. \n\n## Summary\n\nSo remember a linear model sets one factor level as the 'intercept' estimates its mean, then draws a line from the first treatment to the second treatment, the slope of the line is the difference in means between the two treatments. \n\nThe difference in means is always accompanied by a standard error of the difference (SED), and this can be used to calculate a 95% confidence interval. If this confidence interval does not contain the intercept value, we can reject the null hypothesis that there is 'no effect'. \n\nLinear models make a variety of assumptions, including that the noise (residual differences) are approximately normally distributed, with roughly equal (homogenous) variance. \n\n## Write-up\n\nCan you write an Analysis section? Add calculated 95% confidence intervals on top of the summary we produced earlier\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\nThe maize plants that have been cross pollinated had an average height of 20.19 inches [18.63 - 21.74] and were taller on average than the self-pollinated plants, with a mean difference in height of 2.62 [0.42, 4.82] inches (mean [95% CI]) (t(28) = -2.44, p = 0.02).\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Convert the 'means' object to a tibble\nmeans |>\n  as_tibble() |>\n  # Create a plot using ggplot\n  ggplot(aes(x = type, y = emmean, fill = type)) +\n    # Add raw data\n  geom_jitter(data = darwin,\n              aes(x = type,\n                  y = height),\n              width = 0.1,\n              pch = 21,\n              alpha = 0.4) +\n  # Add point estimates with 95% confidence error bars\n  geom_pointrange(aes(ymin = lower.CL, \n                      ymax = upper.CL),\n                  pch = 21) +\n  theme_classic()+\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](09-linear-models_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n</div>\n\n\n:::{.callout-note}\n\nThis will be different to your previous manual calculations on two counts. One, we are using a t-distribution for our confidence intervals. Two this example is a two-sample t-test, our previous example was closer to a paired t-test we will see how to implement a linear model with a paired design in subsequent chapters.\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}