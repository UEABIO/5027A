{
  "hash": "67ec721ca24a6a41f17aeb790b4890f2",
  "result": {
    "engine": "knitr",
    "markdown": "# Paired tests {#sec-pair}\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n\n### Packages\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(emmeans)\nlibrary(performance)\nlibrary(broom.helpers)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n\n\nThe structure of our linear model so far has produced the output for a standard two-sample Student's *t*-test. However, when we *first* [calculated our estimates by hand](#differences-between-groups) - we started by making an average of the paired differences in height. To generate the equivalent of a paired *t*-test, we simply have to add the factor for pairs to our linear model formula:\n\n::: {.panel-tabset}\n\n## Base T\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlsmodel_darwin <- lm(height ~ type + factor(pair), data = darwin)\nsummary(lsmodel_darwin)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = height ~ type + factor(pair), data = darwin)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4958 -0.9021  0.0000  0.9021  5.4958 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     21.7458     2.4364   8.925 3.75e-07 ***\ntypeSelf        -2.6167     1.2182  -2.148   0.0497 *  \nfactor(pair)2   -4.2500     3.3362  -1.274   0.2234    \nfactor(pair)3    0.0625     3.3362   0.019   0.9853    \nfactor(pair)4    0.5625     3.3362   0.169   0.8685    \nfactor(pair)5   -1.6875     3.3362  -0.506   0.6209    \nfactor(pair)6   -0.3750     3.3362  -0.112   0.9121    \nfactor(pair)7   -0.0625     3.3362  -0.019   0.9853    \nfactor(pair)8   -2.6250     3.3362  -0.787   0.4445    \nfactor(pair)9   -3.0625     3.3362  -0.918   0.3742    \nfactor(pair)10  -0.6250     3.3362  -0.187   0.8541    \nfactor(pair)11  -0.6875     3.3362  -0.206   0.8397    \nfactor(pair)12  -0.9375     3.3362  -0.281   0.7828    \nfactor(pair)13  -3.0000     3.3362  -0.899   0.3837    \nfactor(pair)14  -1.1875     3.3362  -0.356   0.7272    \nfactor(pair)15  -5.4375     3.3362  -1.630   0.1254    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.336 on 14 degrees of freedom\nMultiple R-squared:  0.469,\tAdjusted R-squared:  -0.09997 \nF-statistic: 0.8243 on 15 and 14 DF,  p-value: 0.6434\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Tidyverse\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndarwin |>  \n  mutate(pair = as_factor(pair)) |>  \n  lm(height ~ type + pair, data = _) |>  \n  broom::tidy()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term        |  estimate| std.error|  statistic|   p.value|\n|:-----------|---------:|---------:|----------:|---------:|\n|(Intercept) | 21.745833|  2.436389|  8.9254352| 0.0000004|\n|typeSelf    | -2.616667|  1.218195| -2.1479875| 0.0497029|\n|pair2       | -4.250000|  3.336163| -1.2739185| 0.2234352|\n|pair3       |  0.062500|  3.336163|  0.0187341| 0.9853176|\n|pair4       |  0.562500|  3.336163|  0.1686069| 0.8685176|\n|pair5       | -1.687500|  3.336163| -0.5058206| 0.6208532|\n|pair6       | -0.375000|  3.336163| -0.1124046| 0.9120984|\n|pair7       | -0.062500|  3.336163| -0.0187341| 0.9853176|\n|pair8       | -2.625000|  3.336163| -0.7868320| 0.4444963|\n|pair9       | -3.062500|  3.336163| -0.9179707| 0.3741786|\n|pair10      | -0.625000|  3.336163| -0.1873410| 0.8540813|\n|pair11      | -0.687500|  3.336163| -0.2060750| 0.8396990|\n|pair12      | -0.937500|  3.336163| -0.2810114| 0.7828120|\n|pair13      | -3.000000|  3.336163| -0.8992366| 0.3837329|\n|pair14      | -1.187500|  3.336163| -0.3559478| 0.7271862|\n|pair15      | -5.437500|  3.336163| -1.6298663| 0.1254148|\n\n</div>\n:::\n:::\n\n\n\n\n\n\n:::\n\n> Note that I have made pair a factor - pair 2 is not greater than pair 1 - so it does not make sense to treat these as number values.\n\n\nThe table of coefficients suddenly looks a lot more complicated! This is because **now** the intercept is the height of the crossed plant from pair 1:\n\n* The second row now compares the average heights difference of Crossed and Selfed plants **when they are in the same pair** \n\n* rows three to 16 compare the average difference of each pair (Crossed and Selfed combined) against pair 1\n\nAgain the linear model computes every possible combination of *t*-statistic and *P*-value, however the only one we care about is the difference in Cross and Self-pollinated plant heights. If we ignore the pair comparisons the second row gives us a *paired t*-test. 'What is the difference in height between Cross and Self-pollinated plants when we hold pairs constant.'\n\nQ. By including pairs we are making sure we deal with <select class='webex-select'><option value='blank'></option><option value=''>Missing data bias</option><option value='answer'>Omitted variable bias</option><option value=''>Survivorship bias</option></select>\n\n## Activity 1: Compare paired model designs\n\nWe previously compared the difference in plant heights by subtracting the difference between each pair of plants. Check our reasoning on the model design above by making another linear model based on the difference between pairs and check whether this is significantly different from 0, `lm(difference ~ 1)`\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndarwin_wide <- darwin |>  \n  # pivot/reshape the data from long to wide format\n  pivot_wider(names_from = type, values_from = height) |>  \n  # Calculate the difference between 'Cross' and 'Self' heights for each pair\n  mutate(difference = Cross - Self)\n\nwide_model <- lm(difference ~ 1, data = darwin_wide)\n\nsummary(wide_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = difference ~ 1, data = darwin_wide)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9917  -1.2417   0.3833   3.0083   6.7583 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    2.617      1.218   2.148   0.0497 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.718 on 14 degrees of freedom\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n</div>\n\n\nFor completeness let's generate the confidence intervals for the *paired t*-test and compare them to our *unpaired t*-test. \n\n::: {.panel-tabset}\n\n## Unpaired\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(height ~ type, data = darwin) |> \n   broom::tidy(conf.int=T) |> \n    slice(1:2) \n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term        |  estimate| std.error| statistic|   p.value| conf.low|  conf.high|\n|:-----------|---------:|---------:|---------:|---------:|--------:|----------:|\n|(Intercept) | 20.191667| 0.7592028| 26.595880| 0.0000000| 18.63651| 21.7468231|\n|typeSelf    | -2.616667| 1.0736749| -2.437113| 0.0214145| -4.81599| -0.4173433|\n\n</div>\n:::\n:::\n\n\n\n\n\n\n## Paired\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(height ~ type + factor(pair), data = darwin) |>  \n  broom::tidy(conf.int=T) |> \n  slice(1:2) # just show first two rows\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term        |  estimate| std.error| statistic|   p.value|  conf.low|  conf.high|\n|:-----------|---------:|---------:|---------:|---------:|---------:|----------:|\n|(Intercept) | 21.745833|  2.436389|  8.925435| 0.0000004| 16.520298| 26.9713683|\n|typeSelf    | -2.616667|  1.218195| -2.147988| 0.0497029| -5.229434| -0.0038992|\n\n</div>\n:::\n:::\n\n\n\n\n\n\n:::\n\nWe can see that estimate of the mean difference is identical but the 95% confidence intervals are now slightly different. So in this particular version we have actually increased our level of uncertainty by including the pair parameter. \n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm1 <- lm(height ~ type, data = darwin) |>  \n  broom::tidy( conf.int=T) |>  \n  slice(2:2) |>  \n  mutate(model=\"unpaired\")\n\nm2 <- lm(height ~ type + factor(pair), data = darwin) |>  \n  broom::tidy(conf.int=T) |>  \n  slice(2:2) |>  \n  mutate(model=\"paired\")\n\nrbind(m1,m2) |>  \n  ggplot(aes(model, estimate))+\n  geom_pointrange(aes(ymin=conf.high, ymax=conf.low))+\n  geom_hline(aes(yintercept=0), linetype=\"dashed\")+\n  theme_minimal()+\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](10-paired-design_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell layout-align=\"center\" type='info'}\n<div class=\"info\">\n<p>Choosing the right model</p>\n<p>In this case we had a good <em>a priori</em> reason to include pair\nin our model, so I would argue that it should stay - alternative\napproaches are model simplification and stepwise removal. We will\ndiscuss these later.</p>\n</div>\n:::\n\n\n\n\n\n\n## Effect sizes\n\nWe have discussed the importance of using confidence intervals to talk about effect sizes. When our 95% confidence intervals do not overlap the intercept, this indicates we have difference in our means which is significant at $\\alpha$ = 0.05. More interestingly than this it allows us to talk about the 'amount of difference' between our treatments, the lower margin of our confidence intervals is the smallest/minimum effect size. On the response scale of our variables this is very useful, we can report for example that there is *at least* a 0.4 inch height difference between self and crossed fertilised plants at $\\alpha$ = 0.05. \n\n\n## Type 1 and Type 2 errors\n\nThe repeatability of results is a key part of the scientific method. Unfortunately there is often an emphasis in the literature on 'novel findings', which means that unusual/interesting results that happen to reach statistical significance may be more likely to be published. The reality is that we know if we set an $\\alpha$ = 0.05, that we run the risk of rejecting the null hypothesis incorrectly in 1 in 20 of our experiments (A Type 1 error). \n\nType 2 errors. Statistical tests provide you with the probability of making a Type 1 error (rejecting the null hypothesis incorrectly) in the form of *P*. But what about Type 2 errors? Keeping the null hypothesis, when we should be rejecting it? Or not finding an effect.\n\nThe probability of making a Type 2 error is known as $1-\\beta$, where $\\beta$ refers to your statistical 'power'. Working out statistical power is is very straightforward for simple tests, and then becomes rapidly more diffcult as the complexity of your analysis increases... but it is an important concept to understand. \n\nOn the other side of the coin is experimental *power* - this is strength of your experiment to detect a statistical effect *when there is one*. Power is expressed as 1-$\\beta$. You want beta error typically to be less than 20%. So, you want a power of about 80%. That is you have an 80% chance of finding an effect **if it's there**. \n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\" type='info'}\n<div class=\"info\">\n<p>All experiments/statistical analyses will become <em>statistically\nsignificant</em> if you make the sample size large enough. In this\nrespect it shows how misleading a significant result can be. It is not\nthat interesting if a result is statistically significant, but the\neffect size is tiny.</p>\n</div>\n:::\n\n\n\n\n\n\n\n\n## Activity 2: Write-up\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.task .cell-code}\nCan you write a summary of the **Results**?\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.solution .cell-code}\n\nWe analysed the difference in heights between selfed and cross-pollinated maize plants using a paired analysis of 15 pairs of plants. On average we found plants that have been cross pollinated (20.2 inches [18.3, 22], (mean [95% CI])) were taller than the self-pollinated plants (t(14) = -2.148, p = 0.0497), with a mean difference in height of 2.62 [0.004, 5.23] inches .\n\n```\n:::\n\n\n\n\n\n\n\n## Summary\n\nThis chapter finally allowed us to calculate *P*-values and test statistical significance for our experiments using linear models. We also compared the linear model structures for producing a paired vs. unpaired *t*-test. \n\nHowever we also learned to appreciate the potential issues around making Type 1 and Type 2 errors, and how an appreciation of confidence intervals and standardised effect sizes can be used to assess these. \n\nA single experiment is never definitive, and a reliance on reporting *P*-values is uninformative and can be misleading. Instead reporting estimates and confidence intervals allows us to report our levels of uncertainty, and provides results which are more informative for comparative studies. \n\n\n",
    "supporting": [
      "10-paired-design_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}