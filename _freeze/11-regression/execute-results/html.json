{
  "hash": "7e71cda177c60efeea8d207b5ed91cf4",
  "result": {
    "engine": "knitr",
    "markdown": "# Regression {#sec-reg}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n:::{.callout-warning}\n\nSet this up as a new R project, with organised folders. \n\nThis week's assignment will be to submit this analysis, so make it clear and well organised! \n\nMake sure to include some data checking in your script.\n\n:::\n\n\n## Introduction to Regression\n\nSo far we have used linear models for analyses between *two* 'categorical' explanatory variables e.g. *t*-tests. But what about when we have a 'continuous' explanatory variable? For that we need to use a regression analysis, luckily this is just another 'special case' of the linear model, so we can use the same `lm()` function we have already been using, and we can interpret the outputs in the same way. \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rstatix)\nlibrary(performance)\n```\n:::\n\n\n\n\n## Linear regression\n\nMuch like the *t*-test we have generating from our linear model, the regression analysis is interpreting the strength of the 'signal' (the change in mean values according to the explanatory variable), vs the amount of 'noise' (variance around the mean). \n\nWe would normally visualise a regression analysis with a scatter plot, with the explanatory (predictor, independent) variable on the x-axis and the response (dependent) variable on the y-axis. Individual data points are plotted, and we attempt to draw a straight-line relationship throught the cloud of data points. This line is the 'mean', and the variability around the mean is captured by calculated standard errors and confidence intervals from the variance. \n\nThe equation for the linear regression model is:\n\n$$ y = a + bx $$\nYou may also note this is basically identical to the equation for a straight fit line $y = mx +c$. \n\nHere: \n\n* *y* is the predicted value of the response variable\n\n* *a* is the regression intercept (the value of *y* when *x* = 0)\n\n* *b* is the slope of the regression line\n\n* *x* is the value of the explanatory variable\n\nThis formula explains the mean, you would need to include the unexplained residual error as a term to include our measure of uncertainty\n\n$$ y = a + bx + e $$\n\nThe regression uses two values to fit a straight line. First we need a starting point, known as the regression intercept. For categorical predictors this is the mean value of *y* for one of our categories, for a regression this is the mean value of *y* when *x* = 0. We then need a gradient (how the value of *y* changes when the value of *x* changes). This allows us to draw a regression line. \n\nA linear model analysis estimates the values of the intercept and gradient in order to predict values of *y* for given values of *x*. \n\n## Data\n\nHere we are going to use example data from the Australian forestry industry, recording the density and hardness of 36 samples of wood from different tree species. Wood density is a fundamental property that is relatively easy to measure, timber hardness, is quantified as the 'the amount of force required to embed a 0.444\" steel ball into the wood to half of its diameter'. \n\nWith regression, we can test the biological hypothesis that wood density can be used to predict timber hardness, and use this regression to predict timber hardness for new samples of known density. \n\nTimber hardness is quantified using the 'Janka scale', and the data we are going to use today comes originally from an R package `SemiPar`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<a href=\"https://raw.githubusercontent.com/UEABIO/data-sci-v1/main/book/files/janka.csv\">\n<button class=\"btn btn-success\"><i class=\"fa fa-save\"></i> Download Janka data as csv</button>\n</a>\n```\n\n:::\n:::\n\n::: {.cell layout-align=\"center\" type='try'}\n<div class=\"try\">\n<p>Check the data is imported correctly and make sure it is ‘tidy’ with\nno obvious errors or missing data</p>\n</div>\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\njanka <- read_csv(\"data/janka.csv\")\n```\n:::\n\n\n\n\n## Activity 1: Exploratory Analysis\n\n:::{.callout-note}\nMake a plot. Is there any visual evidence for a linear association between wood density and timber hardness?\n:::\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\njanka |> \n  ggplot(aes(x=dens, y=hardness))+\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](11-regression_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\nWood density and timber hardness appear to be positively related, and the relationship appears to be fairly linear. We can look at a simple strength of this association between dens and hardness using correlation\n\n\n</div>\n\n\n\n## Activity 2: Correlation - Generate Pearson's R\n\nCan you work out the code needed to generate Pearson's R? - Try using a google search, then check your code and answer against the solution.\n\n> Hint try the rstatix package?\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# cor() does not have a data option so need to use the with() function\nwith(janka, cor(dens, hardness))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9743345\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rstatix)\n\njanka |> \n  cor_test(dens, hardness)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|var1 |var2     |  cor| statistic|  p| conf.low| conf.high|method  |\n|:----|:--------|----:|---------:|--:|--------:|---------:|:-------|\n|dens |hardness | 0.97|  25.23845|  0| 0.949849| 0.9869454|Pearson |\n\n</div>\n:::\n:::\n\n\n\n\n\n</div>\n\n\n\nCorrelation coefficients range from -1 to 1 for perfectly negative to perfectly positive linear relationships. The relationship here appears to be strongly positive. Correlation looks at the **association** between two variables, but we want to go further - we are arguing that wood density *causes* higher values of timber hardness. In order to test that hypothesis we need to go further than correlation and use regression.\n\n## Regression in R\n\nWe can fit the regression model in exactly the same way as we fit the linear model for Darwin's maize data. The *only* difference is that here our predictor variable is continuous rather than categorical. \n\n\n\n\n::: {.cell layout-align=\"center\" type='warning'}\n<div class=\"warning\">\n<p>Be careful when ordering variables here:</p>\n<ul>\n<li><p>the left of the ‘tilde’ is the response variable,</p></li>\n<li><p>on the right is the predictor.</p></li>\n</ul>\n<p>Get them the wrong way round and it will reverse your hypothesis.</p>\n</div>\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\njanka_ls1 <- lm(hardness ~ dens, data = janka) \n```\n:::\n\n\n\n\nThis linear model will estimate a 'line of best fit' using the method of 'least squares' to minimise the error sums of squares (the average distance between the data points and the regression line). \n\nWe can add a regression line to our ggplots very easily with the function `geom_smooth()`.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# specify linear model method for line fitting\n\njanka |> \n  ggplot(aes(x=dens, y=hardness))+\n  geom_point()+\n  geom_smooth(method=\"lm\")\n```\n\n::: {.cell-output-display}\n![](11-regression_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n**Q. The blue line represents the regression line, and the shaded interval is the 95% confidence interval band. What do you notice about the width of the interval band as you move along the regression line?**\n\n\n<div class='webex-solution'><button>Explain this</button>\n\n\nThe 95% confidence interval band is narrowest in the middle and widest at either end of the regression line. But why?\n\nWhen performing a linear regression, there are **two** types of uncertainty in the prediction.\n\nFirst is the prediction of the overall mean of the estimate (ie the center of the fit). The second is the uncertainly in the estimate calculating the slope.\n\nSo when you combine both uncertainties of the prediction there is a spread between the high and low estimates. The further away from the center of the data you get (in either direction), the uncertainty of the slope becomes a large and more noticeable factor, thus the limits widen.\n\n\n</div>\n\n\n#### Summary\n\n\n::: {.panel-tabset}\n\n\n## Base R\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(janka_ls1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = hardness ~ dens, data = janka)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-338.40  -96.98  -15.71   92.71  625.06 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1160.500    108.580  -10.69 2.07e-12 ***\ndens           57.507      2.279   25.24  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 183.1 on 34 degrees of freedom\nMultiple R-squared:  0.9493,\tAdjusted R-squared:  0.9478 \nF-statistic:   637 on 1 and 34 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Tidyverse\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\njanka_ls1 |> \n  broom::tidy()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term        |    estimate|  std.error| statistic| p.value|\n|:-----------|-----------:|----------:|---------:|-------:|\n|(Intercept) | -1160.49970| 108.579605| -10.68801|       0|\n|dens        |    57.50667|   2.278534|  25.23845|       0|\n\n</div>\n:::\n:::\n\n\n\n\n:::\n\nThis output should look very familiar to you, because it's the same output produced for the analysis of the maize data. Including a column for the coefficient estimates, standard error, *t*-statistic and *P*-value. The first row is the intercept, and the second row is the difference in the mean from the intercept caused by our explanatory variable. \n\n\nIn a test of difference, the intercept represented the mean of one group and the coefficient tells you how much higher or lower the average of one group is compared to another group. In regression analysis, the intercept is the value when the predictor variable is *zero* and the coefficient shows how much the response variable changes when the predictor variable increases by one unit, while keeping other predictors constant. This might mean the effect looks small - but it is cumulative.\n\n::: {.callout-info}\n\nIn many ways the intercept makes more intuitive sense in a regression model than a difference model. Here the intercept describes the value of *y* (timber hardness) when *x* (wood density) = 0. The standard error is standard error of this calculated mean value. The only wrinkle here is that that value of *y* is an impossible value - timber hardness obviously cannot be a negative value (anti-hardness???). This does not affect the fit of our line, it just means a regression line (being an infinite straight line) can move into impossible value ranges.\n\nOne way in which the intercept can be made more valuable is to use a technique known as 'centering'. By subtracting the average (mean) value of *x* from every data point, the intercept (when *x* is 0) can effectively be right-shifted into the centre of the data. This is known as mean-centered regression \n\n:::\n\n### Activity 3:  Mean-centering\n\n::: {.callout-note}\n\nCan you use your data wrangling skills to mean centre the density predictor and produce a more realistic model intercept?\n\n:::\n\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\njanka_centered <-  janka |> \n  mutate(centered_dens = dens - mean(dens))\n\ncentered_model <- lm(hardness ~ dens, data = janka_centered) \n\nsummary(centered_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = hardness ~ dens, data = janka_centered)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-338.40  -96.98  -15.71   92.71  625.06 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1160.500    108.580  -10.69 2.07e-12 ***\ndens           57.507      2.279   25.24  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 183.1 on 34 degrees of freedom\nMultiple R-squared:  0.9493,\tAdjusted R-squared:  0.9478 \nF-statistic:   637 on 1 and 34 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\n</div>\n\n\n#### Confidence intervals\n\nJust like with the maize data, we can produce upper and lower bounds of confidence intervals: \n\n::: {.panel-tabset}\n\n## Base R\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconfint(janka_ls1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  2.5 %     97.5 %\n(Intercept) -1381.16001 -939.83940\ndens           52.87614   62.13721\n```\n\n\n:::\n:::\n\n\n\n\n## Tidyverse\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbroom::tidy(janka_ls1, conf.int=T, conf.level=0.95)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term        |    estimate|  std.error| statistic| p.value|    conf.low|  conf.high|\n|:-----------|-----------:|----------:|---------:|-------:|-----------:|----------:|\n|(Intercept) | -1160.49970| 108.579605| -10.68801|       0| -1381.16001| -939.83940|\n|dens        |    57.50667|   2.278534|  25.23845|       0|    52.87614|   62.13721|\n\n</div>\n:::\n:::\n\n\n\n\n:::\n\n\n\n- What would you say is the minimum effect size (at 95% confidence) of density on the janka scale?\n\n\n<select class='webex-select'><option value='blank'></option><option value='answer'>52.9</option><option value=''>62.1</option><option value=''>57.5</option><option value=''>2.28</option></select>\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\nHere we can say that at $\\alpha$ = 0.05 we think there is at least a 52.9 unit increase on the janka scale for every unit increase in density ($\\rho$). Because our 95% confidence intervals do not span 0, we know that there is a significant relationship at $\\alpha$ = 0.05. \n\n\n</div>\n\n\n### Effect size\n\nWith a regression model, we can also produce a standardised effect size. The estimate and 95% confidence intervals are the amount of change being observed, but just like with the maize data we can produce a standardised measure of how strong the relationship is. This value is represented by $R^2$ : the proportion of the variation in the data explained by the linear regression analysis. \n\nThe value of $R^2$ can be found in the model summaries as follows\n\n::: {.panel-tabset}\n\n## Base R\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(janka_ls1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = hardness ~ dens, data = janka)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-338.40  -96.98  -15.71   92.71  625.06 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1160.500    108.580  -10.69 2.07e-12 ***\ndens           57.507      2.279   25.24  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 183.1 on 34 degrees of freedom\nMultiple R-squared:  0.9493,\tAdjusted R-squared:  0.9478 \nF-statistic:   637 on 1 and 34 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Tidyverse\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\njanka_ls1 |> \n  broom::glance()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| r.squared| adj.r.squared|    sigma| statistic| p.value| df|    logLik|      AIC|      BIC| deviance| df.residual| nobs|\n|---------:|-------------:|--------:|---------:|-------:|--:|---------:|--------:|--------:|--------:|-----------:|----:|\n| 0.9493278|     0.9478374| 183.0595|  636.9794|       0|  1| -237.6061| 481.2123| 485.9628|  1139366|          34|   36|\n\n</div>\n:::\n:::\n\n\n\n\n:::\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; \">\n<caption>R squared effect size</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Effect size </th>\n   <th style=\"text-align:right;\"> r^2 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> small </td>\n   <td style=\"text-align:right;\"> 0.1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> medium </td>\n   <td style=\"text-align:right;\"> 0.3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> large </td>\n   <td style=\"text-align:right;\"> 0.5 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\n## Prediction\n\nUsing the coefficients of the intercept and the slope we can make predictions on new data. \nThe estimates of the intercept and the slope are:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncoef(janka_ls1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)        dens \n-1160.49970    57.50667 \n```\n\n\n:::\n:::\n\n\n\n\n\nNow imagine we have a new wood samples with a density of 65, how can we use the equation for a linear regression to predict what the timber hardness for this wood sample should be?\n\n$$ y = a + bx $$\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# a + bx\n\n-1160.49970 + 57.50667 * 65\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2577.434\n```\n\n\n:::\n:::\n\n\n\n\n\n</div>\n\n\n\nRather than work out the values manually, we can also use the coefficients of the model directly\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncoef(janka_ls1)[1] + coef(janka_ls1)[2] * 65\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n   2577.434 \n```\n\n\n:::\n:::\n\n\n\n\n\nBut most of the time we are unlikely to want to work out predicted values by hand, instead we can use functions like `predict()` and `broom::augment()`\n\n::: {.panel-tabset}\n\n## Base R\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(janka_ls1, newdata=list(dens=c(22,35,65)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         2         3 \n 104.6471  852.2339 2577.4342 \n```\n\n\n:::\n:::\n\n\n\n\n## Tidyverse\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbroom::augment(janka_ls1, \n               newdata=tibble(dens=c(22,35,65)))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| dens|   .fitted|\n|----:|---------:|\n|   22|  104.6471|\n|   35|  852.2339|\n|   65| 2577.4342|\n\n</div>\n:::\n:::\n\n\n\n\n:::\n\n\n\n### Adding confidence intervals\n\n#### Standard error \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbroom::augment(janka_ls1, newdata = tibble(dens=c(22,35,65)), se=TRUE)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| dens|   .fitted|  .se.fit|\n|----:|---------:|--------:|\n|   22|  104.6471| 62.09026|\n|   35|  852.2339| 39.10197|\n|   65| 2577.4342| 53.46068|\n\n</div>\n:::\n:::\n\n\n\n\n#### 95% Confidence Intervals\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbroom::augment(janka_ls1, newdata=tibble(dens=c(22,35,65)), interval=\"confidence\")\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| dens|   .fitted|     .lower|    .upper|\n|----:|---------:|----------:|---------:|\n|   22|  104.6471|  -21.53544|  230.8297|\n|   35|  852.2339|  772.76915|  931.6987|\n|   65| 2577.4342| 2468.78899| 2686.0793|\n\n</div>\n:::\n:::\n\n\n\n\n\nI really like the `emmeans` package - it is very good for producing quick predictions for categorical data - it can also do this for continuous variables. By default it will produce a single mean-centered prediction. But a list can be provided - it will produce confidence intervals as standard.\n\n\n<div class='webex-solution'><button>emmeans</button>\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nemmeans::emmeans(janka_ls1, \n                 specs = \"dens\", \n                 at = list(dens = c(22, 35, 65)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n dens emmean   SE df lower.CL upper.CL\n   22    105 62.1 34    -21.5      231\n   35    852 39.1 34    772.8      932\n   65   2577 53.5 34   2468.8     2686\n\nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\n\n\n\n</div>\n\n\n\n## Activity 4\n\n- Can you write a summary of the **Results**?\n\n\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\nWe analysed the relationship between wood density and timber hardness on the janka scale with a linear regression model and found that wood density is an excellent predictor of timber harndess (R^2 = 0.95). With an average wood density of 45.7, this produced a timber hardness of 1469 [1407, 1531]  (mean [95% CI]) on the janka scale, and for every unit of density, timber hardness increases by 57.5 [52.9, 62.1] (t(34) = 25.24, p <0.001).\n\n\n</div>\n\n\n## Summary\n\nLinear model analyses can extend beyond testing differences of means in categorical groupings to test relationships with continuous variables. This is known as linear regression, where the relationship between the explanatory variable and response variable are modelled with the equation for a straight line. The intercept is the value of *y* when *x* = 0, often this isn't that useful, and we can use 'mean-centered' values if we wish to make the intercept more intuitive. \nAs with all linear models, regression assumes that the unexplained variability around the regression line, is normally distributed and has constant variance. \n\nOnce the regression has been fitted it is possible to predict values of *y* from values of *x*, the uncertainty around these predictions can be captured with confidence intervals. \n",
    "supporting": [
      "11-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/font-awesome-6.5.2/js/script.js\"></script>\n<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}