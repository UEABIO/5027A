[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "5027A",
    "section": "",
    "text": "Overview\nThis course will introduce scientists and practitioners interested in applying statistical approaches in their daily routine using R as a working environment. Participants will be introduced into R and R Studio while learning how to perform common statistical analyses. After a short introduction on R and its principles, the focus will be on questions that could be addressed using common statistical analyses, both for descriptive statistics and for statistical inference.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "5027A",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nUnderstand how to read, interpret and write scripts in R.\nLearn how to check and clean data\nLearn statistical tools to address common questions in research activities.\nAn introduction to efficient, readable and reproducible analyses\nBeing comfortable with using R when performing both descriptive and inferential statistics.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "5027A",
    "section": "How to use this book",
    "text": "How to use this book\nFor many of the chapters, we will provide the code you need to use. You can copy and paste from the book using the clipboard in the top-right corner.\nWe also provide the solutions to many of the activities. No-one is going to check whether you tried to figure it out yourself rather than going straight to the solution but remember this: if you copy and paste without thinking, you will learn nothing.\nFinally, on occasion we will make updates to the book such as fixing typos and including additional detail or activities and as such this book should be considered a living document. Please tell me if you find any mistakes.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "5027A",
    "section": "Course Structure",
    "text": "Course Structure\nWe have:\n\n\nOne workshop per week, these both timetabled in-person sessions, and you should check Timetabler for up to-date information on scheduling. However, everything you need to complete workshops will be available on this site.\n\nIf you feel unwell, or cannot attend a session in-person don’t worry you can access everything, and follow along in real time, or work at your own pace.\n\n\nOne assignment per week - this will be in the form of quizzes or short assignments, each assignment is worth 2% of your module grade (to a maximum of 15%) and there will be 10 assignments total.\nOne data analysis project - this will be a single piece of coursework (details after reading week) worth 20% of the the module grade",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "04-penguin-project.html",
    "href": "04-penguin-project.html",
    "title": "1  Meet the Penguins",
    "section": "",
    "text": "1.1 Meet the Penguins\nThis data, taken from the palmerpenguins (Horst et al. (2022)) package was originally published by Gorman et al. (2014). In our course we will work with real data that has been shared by other researchers.\nThe palmer penguins data contains size measurements, clutch observations, and blood isotope ratios for three penguin species observed on three islands in the Palmer Archipelago, Antarctica over a study period of three years.\nThese data were collected from 2007 - 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network. The data were imported directly from the Environmental Data Initiative (EDI) Data Portal, and are available for use by CC0 license (“No Rights Reserved”) in accordance with the Palmer Station Data Policy. We gratefully acknowledge Palmer Station LTER and the US LTER Network. Special thanks to Marty Downs (Director, LTER Network Office) for help regarding the data license & use. Here is our intrepid package co-author, Dr. Gorman, in action collecting some penguin data:\nHere is a map of the study site",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Meet the Penguins</span>"
    ]
  },
  {
    "objectID": "04-penguin-project.html#activity-1-organising-our-workspace",
    "href": "04-penguin-project.html#activity-1-organising-our-workspace",
    "title": "1  Meet the Penguins",
    "section": "\n1.2 Activity 1: Organising our workspace",
    "text": "1.2 Activity 1: Organising our workspace\nBefore we can begin working with the data, we need to do some set-up.\n\nGo to RStudio Cloud and open the Penguins R project\n\nCreate the following folders using the + New Folder button in the Files tab\n\ndata\noutputs\nscripts\n\n\n\n\n\n\nR is case-sensitive so type everything EXACTLY as printed here\n\n\n\n\ndir.create(\"data\",\n           showWarnings = FALSE)\n\ndir.create(\"outputs\",\n           showWarnings = FALSE)\n\ndir.create(\"scripts\",\n           showWarnings = FALSE)\n\n# or this can be run using apply\nlapply(c(\"data\", \"outputs\", \"scripts\"), function(dir_name) {\n  dir.create(dir_name, showWarnings = FALSE)\n})\n\nHaving these separate subfolders within our project helps keep things tidy, means it’s harder to lose things, and lets you easily tell R exactly where to go to retrieve data.\nThe next step of our workflow is to have a well organised project space. RStudio Cloud does a lot of the hard work for you, each new data project can be set up with its own Project space.\nWe will define a project as a series of linked questions that uses one (or sometimes several) datasets. For example a coursework assignment for a particular module would be its own project, a series of linked experiments or particular research project might be its own project.\nA Project will contain several files, possibly organised into sub-folders containing data, R scripts and final outputs. You might want to keep any information (wider reading) you have gathered that is relevant to your project.\n\n\n\n\nAn example of a typical R project set-up\n\n\n\nWithin this project you will notice there is already one file .Rproj. This is an R project file, this is a very useful feature, it interacts with R to tell it you are working in a very specific place on the computer (in this case the cloud server we have dialed into). It means R will automatically treat the location of your project file as the ‘working directory’ and makes importing and exporting easier. More on projects can be found in the [R4DS]((https://r4ds.had.co.nz/workflow-projects.html) book.\n\n\n\nIt is very important to NEVER to move the .Rproj file, this may prevent your workspace from opening properly.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Meet the Penguins</span>"
    ]
  },
  {
    "objectID": "04-penguin-project.html#activity-2-access-our-data",
    "href": "04-penguin-project.html#activity-2-access-our-data",
    "title": "1  Meet the Penguins",
    "section": "\n1.3 Activity 2: Access our data",
    "text": "1.3 Activity 2: Access our data\nNow that we have a project workspace, we are ready to import some data.\n\nUse the link below to open a page in your browser with the data open\nRight-click Save As to download in csv format to your computer (Make a note of where the file is being downloaded to e.g. Downloads)\n\n\n\n\n Download penguin data as csv\n\n\n\n\n\n\n\nTop image: Penguins data viewed in Excel, Bottom image: Penguins data in native csv format\n\n\n\nIn raw format, each line of a CSV is separated by commas for different values. When you open this in a spreadsheet program like Excel it automatically converts those comma-separated values into tables and columns.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Meet the Penguins</span>"
    ]
  },
  {
    "objectID": "04-penguin-project.html#activity-3-upload-our-data",
    "href": "04-penguin-project.html#activity-3-upload-our-data",
    "title": "1  Meet the Penguins",
    "section": "\n1.4 Activity 3: Upload our data",
    "text": "1.4 Activity 3: Upload our data\n\nThe data is now in your Downloads folder on your computer\nWe need to upload the data to our remote cloud-server (RStudio Cloud), select the upload files to server button in the Files tab\nPut your file into the data folder - if you make a mistake select the tickbox for your file, go to the cogs button and choose the option Move.\n\n\n\n\n\nHighlighted the buttons to upload files, and more options\n\n\n\n\n1.4.1 Read data from a url\nIt is also possible to use a url as a filepath\n\nurl &lt;- \"https://raw.githubusercontent.com/UEABIO/data-sci-v1/main/book/files/penguins_raw.csv\"\nread_csv(url)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Meet the Penguins</span>"
    ]
  },
  {
    "objectID": "04-penguin-project.html#activity-4-make-a-script",
    "href": "04-penguin-project.html#activity-4-make-a-script",
    "title": "1  Meet the Penguins",
    "section": "\n1.5 Activity 4: Make a script",
    "text": "1.5 Activity 4: Make a script\nLet’s now create a new R script file in which we will write instructions and store comments for manipulating data, developing tables and figures. Use the File &gt; New Script menu item and select an R Script.\nAdd the following:\n\n#___________________________----\n# SET UP ----\n# An analysis of the bill dimensions of male and female \n# Adelie, Gentoo and Chinstrap penguins\n\n# Data first published in  Gorman, KB, TD Williams, and WR Fraser. \n# 2014. \n# “Ecological Sexual Dimorphism and Environmental Variability \n# Within a Community of Antarctic Penguins (Genus Pygoscelis).” \n# PLos One 9 (3): e90081.\n# https://doi.org/10.1371/journal.pone.0090081. \n#__________________________----\n\nThen load the following add-on package to the R script, just underneath these comments. Tidyverse isn’t actually one package, but a bundle of many different packages that play well together - for example it includes ggplot2 a package for making figures.\nAdd the following to your script:\n\n# PACKAGES ----\nlibrary(tidyverse) # tidy data packages\nlibrary(janitor) # cleans variable names\n#__________________________----\n\nSave this file inside the scripts folder and call it 01_import_penguins_data.R\n\n\n\nClick on the document outline button (top right of script pane). This will show you how the use of the visual outline\n\n\nAllows us to build a series of headers and subheaders, this is very useful when using longer scripts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Meet the Penguins</span>"
    ]
  },
  {
    "objectID": "04-penguin-project.html#activity-5-read-in-data",
    "href": "04-penguin-project.html#activity-5-read-in-data",
    "title": "1  Meet the Penguins",
    "section": "\n1.6 Activity 5: Read in data",
    "text": "1.6 Activity 5: Read in data\nNow we can read in the data. To do this we will use the function readr::read_csv() that allows us to read in .csv files. There are also functions that allow you to read in .xlsx files and other formats, however in this course we will only use .csv files.\n\nFirst, we will create an object called penguins_data that contains the data in the penguins_raw.csv file.\nAdd the following to your script, and check the document outline:\n\n\n# IMPORT DATA ----\npenguins_raw &lt;- read_csv (\"data/penguins_raw.csv\")\n\n# penguins_raw &lt;- read_csv(here(\"data\", \"penguins_raw.csv\"))\n\n# check the data has loaded\nhead(penguins_raw)\n#__________________________----\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/2007\n39.1\n18.7\n181\n3750\nMALE\nNA\nNA\nNot enough blood for isotopes.\n\n\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/2007\n39.5\n17.4\n186\n3800\nFEMALE\n8.94956\n-24.69454\nNA\n\n\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n16/11/2007\n40.3\n18.0\n195\n3250\nFEMALE\n8.36821\n-25.33302\nNA\n\n\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n16/11/2007\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nAdult not sampled.\n\n\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n16/11/2007\n36.7\n19.3\n193\n3450\nFEMALE\n8.76651\n-25.32426\nNA\n\n\nPAL0708\n6\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A2\nYes\n16/11/2007\n39.3\n20.6\n190\n3650\nMALE\n8.66496\n-25.29805\nNA\n\n\n\n\n\n\n\n\n\nNote the differences between read.csv() and read_csv. We covered this in differences between tibbles and dataframes - here most obviously is a difference in column names.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Meet the Penguins</span>"
    ]
  },
  {
    "objectID": "04-penguin-project.html#activity-check-your-script",
    "href": "04-penguin-project.html#activity-check-your-script",
    "title": "1  Meet the Penguins",
    "section": "\n1.7 Activity: Check your script",
    "text": "1.7 Activity: Check your script\n\n\nSolution\n\n\n#___________________________----\n# SET UP ----\n# An analysis of the bill dimensions of male and female \n# Adelie, Gentoo and Chinstrap penguins\n\n# Data first published in  Gorman, KB, TD Williams, and WR Fraser. \n# 2014. \n# “Ecological Sexual Dimorphism and Environmental Variability \n# Within a Community of Antarctic Penguins (Genus Pygoscelis).” \n# PLos One 9 (3): e90081.\n# https://doi.org/10.1371/journal.pone.0090081. \n#__________________________----\n\n# PACKAGES ----\n# tidy data packages\nlibrary(tidyverse) \n# cleans variable names\nlibrary(janitor) \n# make sure dates are processed properly\nlibrary(lubridate) \n#__________________________----\n\n# IMPORT DATA ----\npenguins_raw &lt;- read_csv (\"data/penguins_raw.csv\")\n\n# check the data has loaded, prints first 10 rows of dataframe\nhead(penguins_raw) \n#__________________________----",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Meet the Penguins</span>"
    ]
  },
  {
    "objectID": "04-penguin-project.html#activity-test-yourself",
    "href": "04-penguin-project.html#activity-test-yourself",
    "title": "1  Meet the Penguins",
    "section": "\n1.8 Activity: Test yourself",
    "text": "1.8 Activity: Test yourself\nQuestion 1. In order to make your R project reproducible what filepath should you use?\n\nAbsolute filepath\nRelative filepath\nQuestion 2. Which of these would be acceptable to include in a raw datafile?\n\nHighlighting some blocks of cells\nExcel formulae\nA column of observational notes from the field\na mix of ddmmyy and yymmdd date formats\nQuestion 3. What should always be the first set of functions in our script? ?()\n\nQuestion 4. When reading in data to R we should use\n\nread_csv()\nread.csv()\nQuestion 5. What format is the penguins_raw data in?\n\nmessy data\ntidy data\n\n\nExplain This Answer\n\n\nEach column is a unique variable and each row is a unique observation so this data is in a long (tidy) format\n\n\nQuestion 6. The working directory for your projects is by default set to the location of?\n\nyour data files\nthe .Rproj file\nyour R script\nQuestion 7. Using the filepath \"data/penguins_raw.csv\" is an example of\n\nan absolute filepath\na relative filepath\nQuestion 8. What operator do I need to use if I wish to assign the output of the read_csv function to an R object (rather than just print the dataframe into the console)?\n\n\n\n\n\nGorman, K., Williams, T., & Fraser, W. (2014). Ecological sexual dimorphism and environmental variability within a community of antarctic penguins (genus pygoscelis). PLos One, 9(3), e90081. https://doi.org/10.1371/journal.pone.0090081\n\n\nHorst, A., Hill, A., & Gorman, K. (2022). Palmerpenguins: Palmer archipelago (antarctica) penguin data. https://allisonhorst.github.io/palmerpenguins/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Meet the Penguins</span>"
    ]
  },
  {
    "objectID": "05-dplyr.html",
    "href": "05-dplyr.html",
    "title": "2  Data cleaning",
    "section": "",
    "text": "2.1 Introduction to dplyr\nIn this section we will be introduced to some of the most commonly used data wrangling functions, these come from the dplyr package (part of the tidyverse). These are functions you are likely to become very familiar with.\nverb\naction\n\n\n\nselect()\nchoose columns by name\n\n\nfilter()\nselect rows based on conditions\n\n\narrange()\nreorder the rows\n\n\nsummarise()\nreduce raw data to user defined summaries\n\n\ngroup_by()\ngroup the rows by a specified column\n\n\nmutate()\ncreate a new variable",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "05-dplyr.html#introduction-to-dplyr",
    "href": "05-dplyr.html#introduction-to-dplyr",
    "title": "2  Data cleaning",
    "section": "",
    "text": "Important\n\n\n\nTry running the following functions directly in your consoleThe R console is the interactive interface within the R environment where users can type and execute R code. It is the place where you can directly enter commands, see their output, and interact with the R programming language in real-time. or make a scraps.R scrappy file to mess around in.\n\n\n\n\n2.1.1 Select\nIf we wanted to create a dataset that only includes certain variables, we can use the dplyr::select() function from the dplyr package.\nFor example I might wish to create a simplified dataset that only contains species, sex, flipper_length_mm and body_mass_g.\nRun the below code to select only those columns\n\nselect(\n   # the data object\n  .data = penguins_raw,\n   # the variables you want to select\n  `Species`, `Sex`, `Flipper Length (mm)`, `Body Mass (g)`)\n\nAlternatively you could tell R the columns you don’t want e.g. \n\nselect(.data = penguins_raw,\n       -`studyName`, -`Sample Number`)\n\nNote that select() does not change the original penguins tibble. It spits out the new tibble directly into your console.\nIf you don’t save this new tibble, it won’t be stored. If you want to keep it, then you must create a new object.\nWhen you run this new code, you will not see anything in your console, but you will see a new object appear in your Environment pane.\n\nnew_penguins &lt;- select(.data = penguins_raw, \n       `Species`, `Sex`, `Flipper Length (mm)`, `Body Mass (g)`)\n\n\n2.1.2 Filter\nHaving previously used dplyr::select() to select certain variables, we will now use dplyr::filter() to select only certain rows or observations. For example only Adelie penguins.\nWe can do this with the equivalence operator ==\n\nfilter(.data = new_penguins, \n       `Species` == \"Adelie Penguin (Pygoscelis adeliae)\")\n\nWe can use several different operators to assess the way in which we should filter our data that work the same in tidyverse or base R.\n\n\n\nBoolean expressions\n\nOperator\nName\n\n\n\nA &lt; B\nless than\n\n\nA &lt;= B\nless than or equal to\n\n\nA &gt; B\ngreater than\n\n\nA &gt;= B\ngreater than or equal to\n\n\nA == B\nequivalence\n\n\nA != B\nnot equal\n\n\nA %in% B\nin\n\n\n\n\n\nIf you wanted to select all the Penguin species except Adelies, you use ‘not equals’.\n\nfilter(.data = new_penguins, \n       `Species` != \"Adelie Penguin (Pygoscelis adeliae)\")\n\nThis is the same as\n\nfilter(.data = new_penguins, \n       `Species` %in% c(\"Chinstrap penguin (Pygoscelis antarctica)\",\n                      \"Gentoo penguin (Pygoscelis papua)\")\n       )\n\nYou can include multiple expressions within filter() and it will pull out only those rows that evaluate to TRUE for all of your conditions.\nFor example the below code will pull out only those observations of Adelie penguins where flipper length was measured as greater than 190mm.\n\nfilter(.data = new_penguins, \n       `Species` == \"Adelie Penguin (Pygoscelis adeliae)\", \n       `Flipper Length (mm)` &gt; 190)\n\n\n2.1.3 Arrange\nThe function arrange() sorts the rows in the table according to the columns supplied. For example\n\narrange(.data = new_penguins, \n        `Sex`)\n\nThe data is now arranged in alphabetical order by sex. So all of the observations of female penguins are listed before males.\nYou can also reverse this with desc()\n\narrange(.data = new_penguins, \n        desc(`Sex`))\n\nYou can also sort by more than one column, what do you think the code below does?\n\narrange(.data = new_penguins,\n        `Sex`,\n        desc(`Species`),\n        desc(`Flipper Length (mm)`))\n\n\n2.1.4 Mutate\nSometimes we need to create a new variable that doesn’t exist in our dataset. For example we might want to figure out what the flipper length is when factoring in body mass.\nTo create new variables we use the function mutate().\nNote that as before, if you want to save your new column you must save it as an object. Here we are mutating a new column and attaching it to the new_penguins data oject.\n\nnew_penguins &lt;- mutate(.data = new_penguins,\n                     body_mass_kg = `Body Mass (g)`/1000)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "05-dplyr.html#pipes",
    "href": "05-dplyr.html#pipes",
    "title": "2  Data cleaning",
    "section": "\n2.2 Pipes",
    "text": "2.2 Pipes\n\n\n\n\n\n\n\n\nPipes look like this: |&gt; , a pipeAn operator that allows you to chain multiple functions together in a sequence. allows you to send the output from one function straight into another function. Specifically, they send the result of the function before |&gt; to be the first argument of the function after |&gt;. As usual, it’s easier to show, rather than tell so let’s look at an example.\n\n# this example uses brackets to nest and order functions\narrange(.data = filter(\n  .data = select(\n  .data = penguins_raw, \n  species, `Sex`, `Flipper Length (mm)`), \n  `Sex` == \"MALE\"), \n  desc(`Flipper Length (mm)`))\n\n\n# this example uses sequential R objects \nobject_1 &lt;- select(.data = penguins_raw, \n                   `Species`, `Sex`, `Flipper Length (mm)`)\nobject_2 &lt;- filter(.data = object_1, \n                   `Sex` == \"MALE\")\narrange(object_2, \n        desc(`Flipper Length (mm)`))\n\n\n# this example is human readable without intermediate objects\npenguins_raw |&gt;  \n  select(`Species`, `Sex`, `Flipper Length (mm)`) |&gt;  \n  filter(`Sex` == \"MALE\") |&gt;  \n  arrange(`Flipper Length (mm)`))\n\nThe reason that this function is called a pipe is because it ‘pipes’ the data through to the next function. When you wrote the code previously, the first argument of each function was the dataset you wanted to work on. When you use pipes it will automatically take the data from the previous line of code so you don’t need to specify it again.\n\n2.2.1 Task\nTry and write out as plain English what the |&gt; above is doing? You can read the |&gt; as THEN\n\n\nSolution\n\nTake the penguins data AND THEN Select only the species, sex and flipper length columns AND THEN Filter to keep only those observations labelled as sex equals male AND THEN Arrange the data from HIGHEST to LOWEST flipper lengths.\n\n\n\n\nFrom R version 4 onwards there is now a “native pipe” |&gt;\n\n\nThis doesn’t require the tidyverse magrittr package and the “old pipe” %&gt;% or any other packages to load and use.\n\n\nYou may be familiar with the magrittr pipe or see it in other tutorials, and website usages. The native pipe works equivalntly in most situations but if you want to read about some of the operational differences, this site does a good job of explaining .",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "05-dplyr.html#clean-the-penguin-data",
    "href": "05-dplyr.html#clean-the-penguin-data",
    "title": "2  Data cleaning",
    "section": "\n2.3 Clean the Penguin Data",
    "text": "2.3 Clean the Penguin Data\n\n\n\n\n\n\nWarning\n\n\n\nRe-open your 01_import_penguins_data.R started in Chapter 1 and start to add these commands to your data importing and cleaning script:\n\n\n\n2.3.1 Activity 1: Explore data structure\nBefore working with your data, it’s essential to understand its underlying structure and content. In this section, we’ll use powerful functions like glimpse(), str(), summary(), head(),tail()and the add-on functionskimr::skim()` to thoroughly examine your dataset. These tools provide insights into data types, variable distributions, and sample records, helping you identify initial issues such as missing values or inconsistent data types. By gaining a clear understanding of your data’s structure, you’ll be better equipped to address any problems and proceed confidently with data cleaning and analysis.\nWhen we run glimpse() we get several lines of output. The number of observations “rows”, the number of variables “columns”. Check this against the csv file you have - they should be the same. In the next lines we see variable names and the type of data.\n\nglimpse(penguins_raw)\n\nRows: 344\nColumns: 17\n$ studyName             &lt;chr&gt; \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL…\n$ `Sample Number`       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ Species               &lt;chr&gt; \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie P…\n$ Region                &lt;chr&gt; \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\"…\n$ Island                &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgerse…\n$ Stage                 &lt;chr&gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adu…\n$ `Individual ID`       &lt;chr&gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", …\n$ `Clutch Completion`   &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", …\n$ `Date Egg`            &lt;chr&gt; \"11/11/2007\", \"11/11/2007\", \"16/11/2007\", \"16/11…\n$ `Culmen Length (mm)`  &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34…\n$ `Culmen Depth (mm)`   &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18…\n$ `Flipper Length (mm)` &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190,…\n$ `Body Mass (g)`       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 34…\n$ Sex                   &lt;chr&gt; \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\"…\n$ `Delta 15 N (o/oo)`   &lt;dbl&gt; NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18…\n$ `Delta 13 C (o/oo)`   &lt;dbl&gt; NA, -24.69454, -25.33302, NA, -25.32426, -25.298…\n$ Comments              &lt;chr&gt; \"Not enough blood for isotopes.\", NA, NA, \"Adult…\n\n\nWe can see a dataset with 345 rows (including the headers) and 17 variables It also provides information on the type of data in each column\n\n&lt;chr&gt; - means character or text data\n&lt;dbl&gt; - means numerical data\n\nWhen we run summary() we get similar information, in addition for any numerical values we get summary statistics such as mean, median, min, max, quartile ranges and any missing (NA) values\n\nsummary(penguins_raw)\n\n  studyName         Sample Number      Species             Region         \n Length:344         Min.   :  1.00   Length:344         Length:344        \n Class :character   1st Qu.: 29.00   Class :character   Class :character  \n Mode  :character   Median : 58.00   Mode  :character   Mode  :character  \n                    Mean   : 63.15                                        \n                    3rd Qu.: 95.25                                        \n                    Max.   :152.00                                        \n                                                                          \n    Island             Stage           Individual ID      Clutch Completion \n Length:344         Length:344         Length:344         Length:344        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Date Egg         Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm)\n Length:344         Min.   :32.10      Min.   :13.10     Min.   :172.0      \n Class :character   1st Qu.:39.23      1st Qu.:15.60     1st Qu.:190.0      \n Mode  :character   Median :44.45      Median :17.30     Median :197.0      \n                    Mean   :43.92      Mean   :17.15     Mean   :200.9      \n                    3rd Qu.:48.50      3rd Qu.:18.70     3rd Qu.:213.0      \n                    Max.   :59.60      Max.   :21.50     Max.   :231.0      \n                    NA's   :2          NA's   :2         NA's   :2          \n Body Mass (g)      Sex            Delta 15 N (o/oo) Delta 13 C (o/oo)\n Min.   :2700   Length:344         Min.   : 7.632    Min.   :-27.02   \n 1st Qu.:3550   Class :character   1st Qu.: 8.300    1st Qu.:-26.32   \n Median :4050   Mode  :character   Median : 8.652    Median :-25.83   \n Mean   :4202                      Mean   : 8.733    Mean   :-25.69   \n 3rd Qu.:4750                      3rd Qu.: 9.172    3rd Qu.:-25.06   \n Max.   :6300                      Max.   :10.025    Max.   :-23.79   \n NA's   :2                         NA's   :14        NA's   :13       \n   Comments        \n Length:344        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\nFinally the add-on package skimr provides the function skimr::skim() provides an easy to view set of summaries including column types, completion rate, number of unique variables in each column and similar statistical summaries along with a small histogram for each numeric variable.\n\nlibrary(skimr)\nskim(penguins_raw)\n\n\nData summary\n\n\nName\npenguins_raw\n\n\nNumber of rows\n344\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nstudyName\n0\n1.00\n7\n7\n0\n3\n0\n\n\nSpecies\n0\n1.00\n33\n41\n0\n3\n0\n\n\nRegion\n0\n1.00\n6\n6\n0\n1\n0\n\n\nIsland\n0\n1.00\n5\n9\n0\n3\n0\n\n\nStage\n0\n1.00\n18\n18\n0\n1\n0\n\n\nIndividual ID\n0\n1.00\n4\n6\n0\n190\n0\n\n\nClutch Completion\n0\n1.00\n2\n3\n0\n2\n0\n\n\nDate Egg\n0\n1.00\n10\n10\n0\n50\n0\n\n\nSex\n11\n0.97\n4\n6\n0\n2\n0\n\n\nComments\n290\n0.16\n18\n68\n0\n10\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nSample Number\n0\n1.00\n63.15\n40.43\n1.00\n29.00\n58.00\n95.25\n152.00\n▇▇▆▅▃\n\n\nCulmen Length (mm)\n2\n0.99\n43.92\n5.46\n32.10\n39.23\n44.45\n48.50\n59.60\n▃▇▇▆▁\n\n\nCulmen Depth (mm)\n2\n0.99\n17.15\n1.97\n13.10\n15.60\n17.30\n18.70\n21.50\n▅▅▇▇▂\n\n\nFlipper Length (mm)\n2\n0.99\n200.92\n14.06\n172.00\n190.00\n197.00\n213.00\n231.00\n▂▇▃▅▂\n\n\nBody Mass (g)\n2\n0.99\n4201.75\n801.95\n2700.00\n3550.00\n4050.00\n4750.00\n6300.00\n▃▇▆▃▂\n\n\nDelta 15 N (o/oo)\n14\n0.96\n8.73\n0.55\n7.63\n8.30\n8.65\n9.17\n10.03\n▃▇▆▅▂\n\n\nDelta 13 C (o/oo)\n13\n0.96\n-25.69\n0.79\n-27.02\n-26.32\n-25.83\n-25.06\n-23.79\n▆▇▅▅▂\n\n\n\n\n\nQ Based on our summary functions are any variables assigned to the wrong data type (should be character when numeric or vice versa)?\n\nYes\nNo\n\n\nExplanation\n\nAlthough some columns like date might not be correctly treated as character variables, they are not strictly numeric either, all other columns appear correct\n\nQ Based on our summary functions do we have complete data for all variables?\n\nYes\nNo\n\n\nExplanation\n\nNo, they are 2 missing data points for body measurements (culmen, flipper, body mass), 11 missing data points for sex, 13/14 missing data points for blood isotopes (Delta N/C) and 290 missing data points for comments\n\n\n2.3.2 Activity 2: Clean column names\n\n# CHECK DATA----\n# check the data\ncolnames(penguins_raw)\n#__________________________----\n\n [1] \"studyName\"           \"Sample Number\"       \"Species\"            \n [4] \"Region\"              \"Island\"              \"Stage\"              \n [7] \"Individual ID\"       \"Clutch Completion\"   \"Date Egg\"           \n[10] \"Culmen Length (mm)\"  \"Culmen Depth (mm)\"   \"Flipper Length (mm)\"\n[13] \"Body Mass (g)\"       \"Sex\"                 \"Delta 15 N (o/oo)\"  \n[16] \"Delta 13 C (o/oo)\"   \"Comments\"           \n\n\nWhen we run colnames() we get the identities of each column in our dataframe\n\nStudy name: an identifier for the year in which sets of observations were made\nRegion: the area in which the observation was recorded\nIsland: the specific island where the observation was recorded\nStage: Denotes reproductive stage of the penguin\nIndividual ID: the unique ID of the individual\nClutch completion: if the study nest observed with a full clutch e.g. 2 eggs\nDate egg: the date at which the study nest observed with 1 egg\nCulmen length: length of the dorsal ridge of the bird’s bill (mm)\nCulmen depth: depth of the dorsal ridge of the bird’s bill (mm)\nFlipper Length: length of bird’s flipper (mm)\nBody Mass: Bird’s mass in (g)\nSex: Denotes the sex of the bird\nDelta 15N : the ratio of stable Nitrogen isotopes 15N:14N from blood sample\nDelta 13C: the ratio of stable Carbon isotopes 13C:12C from blood sample\n\n\n2.3.2.1 Clean column names\nOften we might want to change the names of our variables. They might be non-intuitive, or too long. Our data has a couple of issues:\n\nSome of the names contain spaces\nSome of the names have capitalised letters\nSome of the names contain brackets\n\nR is case-sensitive and also doesn’t like spaces or brackets in variable names, because of this we have been forced to use backticks `Sample Number` to prevent errors when using these column names\n\n# CLEAN DATA ----\n\n# clean all variable names to snake_case \n# using the clean_names function from the janitor package\n# note we are using assign &lt;- \n# to overwrite the old version of penguins \n# with a version that has updated names\n# this changes the data in our R workspace \n# but NOT the original csv file\n\n# clean the column names\n# assign to new R object\npenguins_clean &lt;- janitor::clean_names(penguins_raw) \n\n# quickly check the new variable names\ncolnames(penguins_clean) \n\n [1] \"study_name\"        \"sample_number\"     \"species\"          \n [4] \"region\"            \"island\"            \"stage\"            \n [7] \"individual_id\"     \"clutch_completion\" \"date_egg\"         \n[10] \"culmen_length_mm\"  \"culmen_depth_mm\"   \"flipper_length_mm\"\n[13] \"body_mass_g\"       \"sex\"               \"delta_15_n_o_oo\"  \n[16] \"delta_13_c_o_oo\"   \"comments\"         \n\n\n\n\nImport and clean names\n\nWe can combine data import and name repair in a single step if we want to:\n\npenguins_clean &lt;- read_csv (\"data/penguins_raw.csv\",\n                      name_repair = janitor::make_clean_names)\n\n\n\n2.3.2.2 Rename columns (manually)\nThe clean_names function quickly converts all variable names into snake caseSnake case is a naming convention in computing that uses underscores to replace spaces between words, and writes words in lowercase. It’s commonly used for variable names, filenames, and database table and column names.. The N and C blood isotope ratio names are still quite long though, so let’s clean those with dplyr::rename() where “new_name” = “old_name”.\n\n# shorten the variable names for isotope blood samples\n# use rename from the dplyr package\npenguins_clean &lt;- rename(penguins_clean,\n         \"delta_15n\"=\"delta_15_n_o_oo\",  \n         \"delta_13c\"=\"delta_13_c_o_oo\")\n\n\n2.3.2.3 Rename text values manually\nSometimes we may want to rename the values in our variables in order to make a shorthand that is easier to follow. This is changing the values in our columns, not the column names.\n\n# use mutate and case_when \n# for a statement that conditionally changes \n# the names of the values in a variable\npenguins &lt;- penguins_clean |&gt; \n  mutate(species = case_when(\n  species == \"Adelie Penguin (Pygoscelis adeliae)\" ~ \"Adelie\",\n  species == \"Gentoo penguin (Pygoscelis papua)\" ~ \"Gentoo\",\n  species == \"Chinstrap penguin (Pygoscelis antarctica)\" ~ \"Chinstrap\",\n  .default = as.character(species)\n  )\n  )\n\n\n# use mutate and if_else\n# for a statement that conditionally changes \n# the names of the values in a variable\npenguins &lt;- penguins |&gt; \n  mutate(sex = if_else(\n    sex == \"MALE\", \"Male\", \"Female\"\n  )\n  )\n\n\n\n\n\n\n\nWarning\n\n\n\nNotice from here on out I am assigning the output of my code to the R object penguins, this means any new code “overwrites” the old penguins dataframe. This is because I ran out of new names I could think of, its also because my Environment is filling up with lots of data frame variants.\nBe aware that when you run code in this way, it can cause errors if you try to run the same code twice e.g. in the example above once you have changed MALE to Male, running the code again could cause errors as MALE is no longer present!\nIf you make any mistakes running code in this way, re-start your R session and run the code from the start to where you went wrong.\n\n\n\n\n\nHave you checked that the above code block worked? Inspect your new tibble and check the variables have been renamed as you wanted.\n\n\n\n\n2.3.2.4 Rename text values with stringr\nDatasets often contain words, and we call these words “(character) strings”.\nOften these aren’t quite how we want them to be, but we can manipulate these as much as we like. Functions in the package stringr, are fantastic. And the number of different types of manipulations are endless!\nBelow we repeat the outcomes above, but with string matching:\n\n# use mutate and case_when \n# for a statement that conditionally changes \n# the names of the values in a variable\npenguins &lt;- penguins_clean |&gt; \n  mutate(species = stringr::word(species, 1)\n  ) |&gt; \n  mutate(sex = stringr::str_to_title(sex))\n\nAlternatively we could decide we want simpler species names but that we would like to keep the latin name information, but in a separate column. To do this we are using regex. Regular expressions are a concise and flexible tool for describing patterns in strings\n\npenguins_clean |&gt; \n    separate(\n        species,\n        into = c(\"species\", \"full_latin_name\"),\n        sep = \"(?=\\\\()\"\n    ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudy_name\nsample_number\nspecies\nfull_latin_name\nregion\nisland\nstage\nindividual_id\nclutch_completion\ndate_egg\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\ndelta_15n\ndelta_13c\ncomments\n\n\n\nPAL0708\n1\nAdelie Penguin\n(Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/2007\n39.1\n18.7\n181\n3750\nMALE\nNA\nNA\nNot enough blood for isotopes.\n\n\nPAL0708\n2\nAdelie Penguin\n(Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/2007\n39.5\n17.4\n186\n3800\nFEMALE\n8.94956\n-24.69454\nNA\n\n\nPAL0708\n3\nAdelie Penguin\n(Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n16/11/2007\n40.3\n18.0\n195\n3250\nFEMALE\n8.36821\n-25.33302\nNA\n\n\nPAL0708\n4\nAdelie Penguin\n(Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n16/11/2007\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nAdult not sampled.\n\n\nPAL0708\n5\nAdelie Penguin\n(Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n16/11/2007\n36.7\n19.3\n193\n3450\nFEMALE\n8.76651\n-25.32426\nNA\n\n\nPAL0708\n6\nAdelie Penguin\n(Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A2\nYes\n16/11/2007\n39.3\n20.6\n190\n3650\nMALE\n8.66496\n-25.29805\nNA\n\n\n\n\n\n\n\n2.3.3 Activity 2: Checking for duplications\nIt is very easy when inputting data to make mistakes, copy something in twice for example, or if someone did a lot of copy-pasting to assemble a spreadsheet (yikes!). We can check this pretty quickly\n\n# check for whole duplicate \n# rows in the data\npenguins |&gt; \n  duplicated() |&gt;  \n  sum() \n\n[1] 0\nGreat!\nIf I did have duplications I could investigate further and extract these exact rows:\n\n# Inspect duplicated rows\npenguins |&gt; \n    filter(duplicated(penguins))\n\nA tibble:0 × 17\n0 rows | 1-8 of 17 columns\n\n# Keep only unduplicated data\npenguins |&gt; \n    filter(!duplicated(penguins))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudy_name\nsample_number\nspecies\nregion\nisland\nstage\nindividual_id\nclutch_completion\ndate_egg\nculmen_length_mm\nculmen_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\ndelta_15n\ndelta_13c\ncomments\n\n\n\nPAL0708\n1\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/2007\n39.1\n18.7\n181\n3750\nMale\nNA\nNA\nNot enough blood for isotopes.\n\n\nPAL0708\n2\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/2007\n39.5\n17.4\n186\n3800\nFemale\n8.94956\n-24.69454\nNA\n\n\nPAL0708\n3\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n16/11/2007\n40.3\n18.0\n195\n3250\nFemale\n8.36821\n-25.33302\nNA\n\n\nPAL0708\n4\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n16/11/2007\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nAdult not sampled.\n\n\nPAL0708\n5\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n16/11/2007\n36.7\n19.3\n193\n3450\nFemale\n8.76651\n-25.32426\nNA\n\n\nPAL0708\n6\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A2\nYes\n16/11/2007\n39.3\n20.6\n190\n3650\nMale\n8.66496\n-25.29805\nNA\n\n\n\n\n\n\n\n2.3.4 Activity 3: Checking for typos\nWe can also look for typos by asking R to produce all of the distinct values in a variable. This is more useful for categorical data, where we expect there to be only a few distinct categories\n\n# Print only unique character strings in this variable\npenguins |&gt;  \n  distinct(sex)\n\n\n\n\nsex\n\n\n\nMale\n\n\nFemale\n\n\nNA\n\n\n\n\n\n\nHere if someone had mistyped e.g. ‘FMALE’ it would be obvious. We could do the same thing (and probably should have before we changed the names) for species.\nWe can also trim leading or trailing empty spaces with stringr::str_trim. These are often problematic and difficult to spot e.g.\n\ndf2 &lt;- tibble(label=c(\"penguin\", \" penguin\", \"penguin \")) \ndf2 # make a test dataframe\n\n\n\n\nlabel\n\n\n\npenguin\n\n\npenguin\n\n\npenguin\n\n\n\n\n\n\nWe can easily imagine a scenario where data is manually input, and trailing or leading spaces are left in. These are difficult to spot by eye - but problematic because as far as R is concerned these are different values. We can use the function distinct to return the names of all the different levels it can find in this dataframe.\n\ndf2 |&gt; \n  distinct()\n\n\n\n\nlabel\n\n\n\npenguin\n\n\npenguin\n\n\npenguin\n\n\n\n\n\n\nIf we pipe the data throught the str_trim function to remove any gaps, then pipe this on to distinct again - by removing the whitespace, R now recognises just one level to this data.\n\ndf2 |&gt; \n  mutate(label=str_trim(label, side=\"both\")) |&gt; \n  distinct()\n\n\n\n\nlabel\n\n\npenguin",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "05-dplyr.html#working-with-dates",
    "href": "05-dplyr.html#working-with-dates",
    "title": "2  Data cleaning",
    "section": "\n2.4 Working with dates",
    "text": "2.4 Working with dates\nWorking with dates can be tricky, treating date as strictly numeric is problematic, it won’t account for number of days in months or number of months in a year.\nAdditionally there’s a lot of different ways to write the same date:\n\n13-10-2019\n10-13-2019\n13-10-19\n13th Oct 2019\n2019-10-13\n\nThis variability makes it difficult to tell our software how to read the information, luckily we can use the functions in the lubridate package.\n\n\n\nIf you get a warning that some dates could not be parsed, then you might find the date has been inconsistently entered into the dataset.\n\n\nPay attention to warning and error messages\n\n\n\nDepending on how we interpret the date ordering in a file, we can use ymd(), ydm(), mdy(), dmy()\n\n\nQuestion What is the appropriate function from the above to use on the date_egg variable?\n\n\nymd()ydm()mdy()dmy()\n\n\n\nSolution\n\n\npenguins &lt;- penguins |&gt;\n  mutate(date_egg = lubridate::dmy(date_egg))\n\n\nHere we use the mutate function from dplyr to create a new variable called date_egg_proper based on the output of converting the characters in date_egg to date format. The original variable is left intact, if we had specified the “new” variable was also called date_egg then it would have overwritten the original variable.\nOnce we have established our date data, we are able to perform calculations or extract information. Such as the date range across which our data was collected.\n\n2.4.1 Calculations with dates\n\npenguins |&gt; \n  summarise(min_date=min(date_egg),\n            max_date=max(date_eggr))\n\nWe can also extract and make new columns from our date column - such as a simple column of the year when each observation was made:\n\npenguins &lt;- penguins |&gt; \n  mutate(year = lubridate::year(date_egg))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "05-dplyr.html#factors",
    "href": "05-dplyr.html#factors",
    "title": "2  Data cleaning",
    "section": "\n2.5 Factors",
    "text": "2.5 Factors\nIn R, factors are a class of data that allow for ordered categories with a fixed set of acceptable values.\nTypically, you would convert a column from character or numeric class to a factor if you want to set an intrinsic order to the values (“levels”) so they can be displayed non-alphabetically in plots and tables, or for use in linear model analyses (more on this later).\nWorking with factors is easy with the forcats package:\nUsing across - we can apply functions to columns based on selected criteria - here within mutate we are changing each column in the .cols argument and applying the function forcats::as_factor()\n\npenguins |&gt; \n  mutate(\n    across(.cols = c(\"species\", \"region\", \"island\", \"stage\", \"sex\"),\n           .fns = forcats::as_factor)\n  ) |&gt; \n  select(where(is.factor)) |&gt; \n  glimpse()\n\nRows: 344\nColumns: 5\n$ species &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie…\n$ region  &lt;fct&gt; Anvers, Anvers, Anvers, Anvers, Anvers, Anvers, Anvers, Anvers…\n$ island  &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgers…\n$ stage   &lt;fct&gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stag…\n$ sex     &lt;fct&gt; Male, Female, Female, NA, Female, Male, Female, Male, NA, NA, …\n\n\n\n\n\n\n\n\nImportant\n\n\n\nUnless we assign the output of this code to an R object it will just print into the console, in the above I am demonstrating how to change variables to factors but we aren’t “saving” this change.\n\n\n\n2.5.1 Setting factor levels\nIf we want to specify the correct order for a factor we can use forcats::fct_relevel\n\npenguins &lt;- penguins |&gt; \n  mutate(mass_range = case_when(\n    body_mass_g &lt;= 3500 ~ \"smol penguin\",\n    body_mass_g &gt;3500 & body_mass_g &lt; 4500 ~ \"mid penguin\",\n    body_mass_g &gt;= 4500 ~ \"chonk penguin\",\n    .default = NA)\n  )\n\nIf we make a barplot, the order of the values on the x axis will typically be in alphabetical order for any character data\n\npenguins |&gt; \n  drop_na(mass_range) |&gt; \n  ggplot(aes(x = mass_range))+\n  geom_bar()\n\n\n\n\n\n\n\nTo convert a character or numeric column to class factor, you can use any function from the forcats package. They will convert to class factor and then also perform or allow certain ordering of the levels - for example using forcats::fct_relevel() lets you manually specify the level order.\nThe function as_factor() simply converts the class without any further capabilities.\n\npenguins &lt;- penguins |&gt; \n  mutate(mass_range = as_factor(mass_range))\n\n\nlevels(penguins$mass_range)\n\n[1] \"mid penguin\"   \"smol penguin\"  \"chonk penguin\"\n\n\nBelow we use mutate() and as_factor() to convert the column flipper_range from class character to class factor.\n\n# Correct the code in your script with this version\npenguins &lt;- penguins |&gt; \n  mutate(mass_range = fct_relevel(mass_range, \n                                  \"smol penguin\", \n                                  \"mid penguin\", \n                                  \"chonk penguin\")\n         )\n\nlevels(penguins$mass_range)\n\n[1] \"smol penguin\"  \"mid penguin\"   \"chonk penguin\"\n\n\nNow when we call a plot, we can see that the x axis categories match the intrinsic order we have specified with our factor levels.\n\npenguins |&gt; \n  drop_na(mass_range) |&gt;  \n  ggplot(aes(x = mass_range))+\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nFactors will also be important when we build linear models a bit later. The reference or intercept for a categorical predictor variable when it is read as a &lt;chr&gt; is set by R as the first one when ordered alphabetically. This may not always be the most appropriate choice, and by changing this to an ordered &lt;fct&gt; we can manually set the intercept.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "05-dplyr.html#summary",
    "href": "05-dplyr.html#summary",
    "title": "2  Data cleaning",
    "section": "\n2.6 Summary",
    "text": "2.6 Summary\nIn this chapter we have successfully imported and checked our data for typos and small errors, we have also been introduce to some of the key functions in the dplyr package for data wrangling. Now that we have confidence in the format and integrity of our data, next time we will start to make insights and understand patterns.\n\n2.6.1 Save scripts\n\nMake sure you have saved your script 💾 and given it the filename 01_import_penguins_data.R it should be saved in your scripts folder\n\n\n\n\nCheck your script\n\n\n#___________________________----\n# SET UP ----\n## An analysis of the bill dimensions of male and female Adelie, Gentoo and Chinstrap penguins ----\n\n### Data first published in  Gorman, KB, TD Williams, and WR Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLos One 9 (3): e90081. https://doi.org/10.1371/journal.pone.0090081. ----\n#__________________________----\n\n# PACKAGES ----\nlibrary(tidyverse) # tidy data packages\nlibrary(janitor) # cleans variable names\n#__________________________----\n# IMPORT DATA ----\npenguins_raw &lt;- read_csv (\"data/penguins_raw.csv\")\n\nattributes(penguins_raw) # reads as tibble\n\nhead(penguins_raw) # check the data has loaded, prints first 10 rows of dataframe\n\n# CLEAN DATA ----\n\n# clean all variable names to snake_case \n# using the clean_names function from the janitor package\n# note we are using assign &lt;- \n# to overwrite the old version of penguins \n# with a version that has updated names\n# this changes the data in our R workspace \n# but NOT the original csv file\n\n# clean the column names\n# assign to new R object\npenguins_clean &lt;- janitor::clean_names(penguins_raw) \n\n# quickly check the new variable names\ncolnames(penguins_clean) \n\n# shorten the variable names for N and C isotope blood samples\n\npenguins &lt;- rename(penguins_clean,\n         \"delta_15n\"=\"delta_15_n_o_oo\",  # use rename from the dplyr package\n         \"delta_13c\"=\"delta_13_c_o_oo\")\n\n# use mutate and case_when for a statement that conditionally changes the names of the values in a variable\npenguins &lt;- penguins_clean |&gt; \n  mutate(species = case_when(species == \"Adelie Penguin (Pygoscelis adeliae)\" ~ \"Adelie\",\n                             species == \"Gentoo penguin (Pygoscelis papua)\" ~ \"Gentoo\",\n                             species == \"Chinstrap penguin (Pygoscelis antarctica)\" ~ \"Chinstrap\"))\n\n# use mutate and if_else\n# for a statement that conditionally changes \n# the names of the values in a variable\npenguins &lt;- penguins |&gt; \n  mutate(sex = if_else(\n    sex == \"MALE\", \"Male\", \"Female\"\n  )\n  )\n\n# use lubridate to format date and extract the year\npenguins &lt;- penguins |&gt;\n  mutate(date_egg = lubridate::dmy(date_egg))\n\npenguins &lt;- penguins |&gt; \n  mutate(year = lubridate::year(date_egg))\n\n# Set body mass ranges\npenguins &lt;- penguins |&gt; \n  mutate(mass_range = case_when(\n    body_mass_g &lt;= 3500 ~ \"smol penguin\",\n    body_mass_g &gt;3500 & body_mass_g &lt; 4500 ~ \"mid penguin\",\n    body_mass_g &gt;= 4500 ~ \"chonk penguin\",\n    .default = NA)\n  )\n\n# Assign these to an ordered factor\n\npenguins &lt;- penguins |&gt; \n  mutate(mass_range = fct_relevel(mass_range, \n                                  \"smol penguin\", \n                                  \"mid penguin\", \n                                  \"chonk penguin\")\n         )\n\n\n\nDoes your workspace look like the below?\n\n\n\n\n\nMy neat project layout\n\n\n\n\n\n\n\nMy scripts and file subdirectory\n\n\n\n\nDoes your script run from a blank slateR projects are set not to store their .Rhistory file, which means everything required to recreate your analysis is contained in your scripts. without errors as described in {#sec-workflow}\n\n2.6.2 Checklist for data checking\n\nIs our dataframe in a tidy dataTidy data refers to a specific format for organizing datasets that makes data easier to work with for analysis and visualization in R, especially using the tidyverse. The concept was popularized by Hadley Wickham in his paper “Tidy Data” and is an essential principle for effective data manipulation. format?\n\nIs each column assigned to the correct data type?\n\nAre dates formatted correctly?\nAre factors set where needed, are the levels in the correct order?\n\n\nAre variables consistently named (e.g. using a naming convention such as snake_case)?\nAre text values in an appropriate format?\nDo we have any data duplication?\nAre there any typos or mistakes in character strings?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "05-dplyr.html#activity-test-yourself",
    "href": "05-dplyr.html#activity-test-yourself",
    "title": "2  Data cleaning",
    "section": "\n2.7 Activity: Test yourself",
    "text": "2.7 Activity: Test yourself\nQuestion 1. In order to subset a data by rows I should use the function \nselect()\nfilter()\ngroup_by()\nQuestion 2. In order to subset a data by columns I should use the function \nselect()\nfilter()\ngroup_by()\nQuestion 3. In order to make a new column I should use the function \ngroup_by()\nselect()\nmutate()\narrange()\nQuestion 4. Which operator should I use to send the output from line of code into the next line? \n+\n&lt;-)\n|&gt;\n%in%\nQuestion 5. What will be the outcome of the following line of code?\n\npenguins |&gt; \n  filter(species == \"Adelie\")\n\n\nThe penguins dataframe object is reduced to include only Adelie penguins from now on\nA new filtered dataframe of only Adelie penguins will be printed into the console\n\n\nExplain this answer\n\nUnless the output of a series of functions is “assigned” to an object using &lt;- it will not be saved, the results will be immediately printed. This code would have to be modified to the below in order to create a new filtered object penguins_filtered\n\npenguins_filtered &lt;- penguins |&gt; \n  filter(species == \"Adelie\")\n\n\n\nQuestion 6. What is the main point of a data “pipe”?\n\nThe code runs faster\nThe code is easier to read\nQuestion 7. The naming convention outputted by the function `janitor::clean_names() is \nsnake_case\ncamelCase\nSCREAMING_SNAKE_CASE\nkebab-case\nQuestion 8. Which package provides useful functions for manipulating character strings?\n\nstringr\nggplot2\nlubridate\nforcats\nQuestion 9. Which package provides useful functions for manipulating dates?\n\nstringr\nggplot2\nlubridate\nforcats\nQuestion 10. If we do not specify a character variable as a factor, then ordering will default to what?\n\nnumerical\nalphabetical\norder in the dataframe",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "05-dplyr.html#glossary",
    "href": "05-dplyr.html#glossary",
    "title": "2  Data cleaning",
    "section": "\n2.8 Glossary",
    "text": "2.8 Glossary\n\n\n\n\nterm\ndefinition\n\n\n\nblank slate\nR projects are set not to store their .Rhistory file, which means everything required to recreate your analysis is contained in your scripts.\n\n\nconsole\nThe R console is the interactive interface within the R environment where users can type and execute R code. It is the place where you can directly enter commands, see their output, and interact with the R programming language in real-time.\n\n\npipe\nAn operator that allows you to chain multiple functions together in a sequence.\n\n\nsnake case\nSnake case is a naming convention in computing that uses underscores to replace spaces between words, and writes words in lowercase. It's commonly used for variable names, filenames, and database table and column names.\n\n\ntidy data\nTidy data refers to a specific format for organizing datasets that makes data easier to work with for analysis and visualization in R, especially using the tidyverse. The concept was popularized by Hadley Wickham in his paper \"Tidy Data\" and is an essential principle for effective data manipulation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "05-dplyr.html#reading",
    "href": "05-dplyr.html#reading",
    "title": "2  Data cleaning",
    "section": "\n2.9 Reading",
    "text": "2.9 Reading\n\nDplyr\nLubridate\nStringr",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "06-data-bias.html",
    "href": "06-data-bias.html",
    "title": "3  Data bias",
    "section": "",
    "text": "3.1 Sampling Bias",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data bias</span>"
    ]
  },
  {
    "objectID": "06-data-bias.html#sampling-bias",
    "href": "06-data-bias.html#sampling-bias",
    "title": "3  Data bias",
    "section": "",
    "text": "3.1.1 Explanation:\nSampling bias occurs when the data collected does not properly represent the population you are studying. This often happens if certain groups are over- or underrepresented. For example, if you survey only college students to understand a city’s general population, your data will be biased because it doesn’t capture other demographics like older adults or people who do not attend college.\n\n3.1.2 Why it matters:\nSampling bias leads to incorrect generalisations about the entire population, as conclusions are based on a skewed subset of data.\n\n3.1.3 Example:\nThere are three locations with native foxgloves Digitalis purpurea and about 1000 plants in each site.\n\n\n\n\n\n\n\n\nIf we measured every single plant in each site we might find there is a slightly different average height at each location:\n\n\n\n\n\nGroup\nmean\nsd\n\n\n\nLocation 1\n50.16128\n9.916950\n\n\nLocation 2\n60.42465\n10.096742\n\n\nLocation 3\n69.79887\n9.783575\n\n\n\n\n\n\n\n3.1.4 Sampling\nIn a more likely scenario we will “sample”, measuring a subset of individuals from each location - however if we do this in an unrepresentative way (e.g. not taking 1/3 of our samples from each location) then we may skew or bias our results:\n\n\n\n\n\n\n\n\nWhen we compare our results from an even sampling distribution to an uneven sampling distribution you can see we have biased our findings - in Location 3 the plants are slightly taller, but they are underrepresented in our sampling and so we have a smaller estimate of plant height.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data bias</span>"
    ]
  },
  {
    "objectID": "06-data-bias.html#missing-not-at-random-mnar-bias",
    "href": "06-data-bias.html#missing-not-at-random-mnar-bias",
    "title": "3  Data bias",
    "section": "\n3.2 Missing Not at Random (MNAR) Bias",
    "text": "3.2 Missing Not at Random (MNAR) Bias\n\n3.2.1 1. Missing Completely at Random (MCAR):\nDefinition: Missing values occur entirely by chance, with no relationship to any other data in the dataset. Example in Palmer Penguins: Imagine if a researcher accidentally forgot to record the flipper length of some penguins on random days. The missing data isn’t related to the penguins’ species, size, or other characteristics. This is MCAR.\nWhy it matters: If data is MCAR, the missing values are less of a problem because they are truly random and do not introduce bias into the analysis.\n\n3.2.2 2. Missing at Random (MAR):\nDefinition: The missingness is related to other observed data, but not to the missing value itself. Example in Palmer Penguins: Suppose the flipper length is more likely to be missing for certain species or on specific islands, but within those groups, it’s random. For example, maybe the flipper length is more often missing for penguins from the Adelie species. This would be MAR.\nWhy it matters: While the missing values aren’t completely random, they can be predicted based on other variables. If we know which variables are related to the missingness (e.g., species or island), we can handle it using imputation techniques.\n\n3.2.3 3. Missing Not at Random (MNAR):\nDefinition: The missingness is related to the actual value of the missing data.\nExample in Palmer Penguins: Imagine if the flipper length is missing because the researcher only skipped recording measurements for penguins with very small or very large flippers. This is MNAR because the missing values depend on the value itself (in this case, extreme flipper lengths).\n\n3.2.4 Why it matters:\nThis type of missingness is the hardest to deal with because the missing data is biased and not random. Special techniques or assumptions are required to handle it correctly.\n\nMCAR: Missing by pure chance, unrelated to any data.\nMAR: Missingness depends on other known variables (e.g., species).\nMNAR: Missingness depends on the value that is missing (e.g., missing small flipper lengths).\n\n3.2.5 Explanation:\nMNAR bias arises when data is systematically missing due to the value of the missing data itself. For instance, if people with extremely high incomes are less likely to report their income in a survey, this creates MNAR bias. The missing data is not random—it’s directly related to the variable being measured.\n\n3.2.6 Why it matters:\nMNAR bias distorts conclusions because certain trends are hidden in the missing data. Ignoring this can lead to underestimating the variability or misunderstanding the true patterns in the data.\n\n3.2.7 Example:\nA weather monitoring station cuts out/fails to record at extreme high and low temperatures. This is a good example of MNAR, the missing value temperature, is the thing that causes recording failure. In the example below we have the original, incomplete measurements:\n\n\n\n\n\n\n\n\nAnd here we see the difference when the missing data is included, our original trend line was too flat, because it missed some seasonal fluctuations of high and low temperatures - we would have concluded temperature is more stable across the year than it really is.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data bias</span>"
    ]
  },
  {
    "objectID": "06-data-bias.html#survivorship-bias",
    "href": "06-data-bias.html#survivorship-bias",
    "title": "3  Data bias",
    "section": "\n3.3 Survivorship Bias",
    "text": "3.3 Survivorship Bias\n\n3.3.1 Explanation:\nSurvivorship bias occurs when you focus on the data points that survive a process and ignore those that did not. A famous example is from World War II, where analysts looked at the bullet holes on planes that returned from battle and suggested reinforcing areas where they saw damage. They overlooked the planes that didn’t return, which were hit in critical areas not visible on surviving planes.\n\n3.3.2 Why it matters:\nFocusing only on surviving or successful subjects can lead to false conclusions, as the failure cases (which provide crucial insights) are excluded from the analysis.\n\n3.3.3 Example\nDuring World War II, the military wanted to reinforce fighter planes to reduce the number of planes lost in combat. Engineers examined the planes that returned from battle and noted where the bullet holes were most concentrated. These planes had more damage in areas like the wings, tail, and fuselage, but relatively few bullet holes in the engine or cockpit areas.\nAt first, it seemed logical to reinforce the parts of the planes that had the most bullet holes, because that’s where the damage was most common. However, this would have been a mistake.\n\n\n\n\n\n\n\n\nThe Realization (Correct Conclusion):\nThe key insight came by realising they were only looking at planes that survived and returned from battle. The planes that had been shot in critical areas, such as the engine or cockpit, did not return — they were shot down. Therefore, the fact that the returning planes had little damage in those areas indicated that hits to these parts were fatal and led to planes being lost.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data bias</span>"
    ]
  },
  {
    "objectID": "06-data-bias.html#outlier-bias",
    "href": "06-data-bias.html#outlier-bias",
    "title": "3  Data bias",
    "section": "\n3.4 Outlier Bias",
    "text": "3.4 Outlier Bias\n\n3.4.1 Explanation:\nOutlier bias happens when extreme values (outliers) unduly influence the results of an analysis. Outliers can occur due to data entry errors, measurement errors, or true but rare events. For example, if you’re analyzing average income and a few extremely high incomes are present in the data, they can raise the average, making it seem like the typical person earns more than they actually do.\n\n3.4.2 Why it matters:\nOutliers can distort the results, especially when using statistical methods like the mean or regression. This can lead to misleading conclusions unless the outliers are properly handled.\n\n3.4.3 Example:\nIn this trend line we can see how just a few extreme data points can alter the slope of association between two variables\n\n\n\n\n\n\n\n\n\n3.4.4 What to do about it\nDealing with outliers is difficult - what we should never do is drop them from the dataset without careful consideration. First we should attempt to determine if they are impossible or simply improbable.\n\n3.4.5 Example:\nA measurement of human heights finds several values that are extremely large, more than 3 standard deviations from the mean or greater than 1.5X the IQR. This should prompt us to pay close attention to this data, but while it is extreme we should not discount it. However, by contrast a negative value or a height so large or small as to be impossible for a human being can be safely removed as an impossible value.\n\n\n\n\n\n\n\n\nWhen we have excluded impossible values we should record this, implausible values will be kept for now until we know if they affect our analyses.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data bias</span>"
    ]
  },
  {
    "objectID": "06-data-bias.html#omitted-variable-bias",
    "href": "06-data-bias.html#omitted-variable-bias",
    "title": "3  Data bias",
    "section": "\n3.5 Omitted variable bias",
    "text": "3.5 Omitted variable bias\n\n3.5.1 Explanation:\nOmitted variable bias occurs when a relevant variable is left out of an analysis, leading to incorrect conclusions. For example, a drug may appear ineffective if you don’t account for gender differences, but including gender in the analysis may reveal that the drug works well for women but not for men.\n\n3.5.2 Why it matters:\nIgnoring important variables can mask the true relationships between variables, leading to faulty interpretations and conclusions.\n\n3.5.3 Example:\nA new drug is being tested in a clinical trial setting and initial analysis indicates a weak positive effect\n\n\n\n\n\n\n\n\nBut when separated into clinically relevant subgroups such as gender we see that the drug does produce strong responses, but only in women, not including other important control variables can lead to over or underestimating effects, depending on how they interact.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data bias</span>"
    ]
  },
  {
    "objectID": "06-data-bias.html#summary",
    "href": "06-data-bias.html#summary",
    "title": "3  Data bias",
    "section": "\n3.6 Summary",
    "text": "3.6 Summary\nUnderstanding bias in data and analysis is crucial because it helps ensure that conclusions drawn from data are accurate and reliable. Bias can distort results, leading to incorrect interpretations, faulty decisions, and misleading insights. Whether it’s selection bias, survivorship bias, or other forms, failing to account for bias can cause analysts to overlook critical information or make assumptions that don’t reflect the full reality. Recognising and addressing bias is key to maintaining the integrity of analysis and making well-informed, data-driven decisions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data bias</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html",
    "href": "07-data-insights.html",
    "title": "4  Data insights",
    "section": "",
    "text": "4.1 Understanding the Variables\nBefore jumping into the analysis, it’s crucial to take a step back and first understand the variables we’re working with and how they might relate to each other. In this case, our key variables are:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#understanding-the-variables",
    "href": "07-data-insights.html#understanding-the-variables",
    "title": "4  Data insights",
    "section": "",
    "text": "Culmen Length (bill length): This measures the length of the penguin’s bill, an important feature related to feeding.\nCulmen Depth (bill depth): This measures the thickness of the penguin’s bill.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#what-should-we-consider-in-our-analysis",
    "href": "07-data-insights.html#what-should-we-consider-in-our-analysis",
    "title": "4  Data insights",
    "section": "\n4.2 What should we consider in our analysis?",
    "text": "4.2 What should we consider in our analysis?\n\n4.2.1 1. Distribution of Data Types\nUnderstanding the type of data we’re dealing with is the first step in any analysis. Both culmen length and depth are continuous numerical variables, meaning they can take a wide range of values. However, our dataset may also include categorical variables like species and island, which will help us group the data and see how these groups influence our measurements.\nWhy this matters: Knowing whether a variable is numerical or categorical will influence how we visualize it and which statistical methods we use. For instance, numerical data might be plotted using histograms, while categorical data could be displayed using bar charts or boxplots.\n\n4.2.2 2. Central Tendency\nNext, we’ll want to understand the central tendency of our data. Central tendency refers to the average or most common values in a dataset. The mean, median, and mode are common measures used to describe central tendency.\n\nMean: The average value (sum of all values divided by the number of values).\nMedian: The middle value when data is sorted from lowest to highest, which is especially useful if the data is skewed or contains outliers.\nMode: The most frequently occurring value in the dataset (not always relevant for continuous data).\n\nWhy this matters: Understanding the average bill length and depth helps us get a sense of what is “normal” for our penguins. Additionally, comparing these statistics across different species or islands could reveal differences in penguin populations.\n\n4.2.3 3. Relationship Between Variables\nNow, we dive into the heart of our analysis: exploring the relationship between culmen length and culmen depth. To investigate this, we can use visual tools like scatterplots , which allow us to plot one variable against the other and visually check for patterns or trends.\nWe might also want to calculate a correlation coefficientA numerical value ranging from -1 to 1 that measures the strength and direction of the linear relationship between two variables. to quantify the strength of the relationship between bill length and bill depth. If we find a strong correlation, we can say that these two measurements tend to move together — for example, as the bill length increases, so might the bill depth.\nWhy this matters: Identifying the relationship between variables is critical to making meaningful conclusions. If bill length and depth are highly correlated, we can begin to explore why this might be the case — perhaps certain penguin species have distinct bill shapes that differ from others.\n\n4.2.4 4. Confounding Variables\nWhile exploring the relationship between bill length and depth, we must also be aware of the potential impact of confounding variableA variable that affects both the independent and dependent variables, potentially distorting the observed relationship between them. . A confounding variable is one that influences both the dependent and independent variables, potentially leading to a false conclusion about their relationship.\nIn this dataset, potential confounders might include:\n\nSpecies: Different species may naturally have different bill shapes, so the relationship between culmen length and depth could vary depending on the species.\nIsland or Location: Penguins from different islands might experience different environmental pressures that could affect bill size and shape.\n\nTo account for this, we can group or subset the data by species or island and re-examine the relationship between bill length and depth within those groups. This helps us determine if the observed relationship holds true across different groups, or if it is being influenced by these other variables.\nWhy this matters: Ignoring confounding variables could lead us to incorrect conclusions. For instance, if we see a strong relationship between bill length and depth, it could simply be due to species differences, rather than a true relationship across all penguins.\n\n4.2.5 Asking New Questions\nEvery step of the analysis leads to new questions. This is a natural part of the data exploration process. For example:\n\nAfter looking at the distribution of culmen length and depth, we might wonder, do certain species have longer bills than others?\nAfter finding a relationship between the variables, we might ask, is this relationship consistent across different islands or years?\nIf we spot outliers, we might ask, why are some penguins’ measurements so different from the rest?\n\n4.2.6 Data checking\nBefore diving into analysis, it’s crucial to have a clear understanding of the structure and quality of your dataset. This step is part of the data wrangling process and ensures that your data is accurate, clean, and ready for exploration. Key tasks include:\n\nThe number of variables: Know how many variables (columns) are present in the dataset. This helps in understanding the scope of the data and ensuring that all necessary information is available.\nThe data format of each variable: Check whether each variable is in the correct format (e.g., numeric, categorical, date). Mistakes in data types, such as a numeric variable being stored as text, can cause errors during analysis.\nChecked for missing data: Identify any missing values and decide how to handle them. Ignoring missing data can skew results, so it’s important to either remove, impute, or investigate why data might be missing.\nChecked for typos, duplications, or other data errors: Manually inspect the data or use automated tools to detect any typos, duplicate entries, or inconsistencies that could impact your analysis. For example, species names should be consistent across all rows.\nCleaned column or factor names: Ensure that all column names are properly labeled and easy to understand. This includes correcting any typos or formatting issues, such as spaces or special characters in names, which could cause problems when referencing them in code.\n\nThese data checking steps provide a strong foundation for accurate and effective analysis, ensuring that your dataset is reliable and well-structured and luckily we carried this out in {Chapter 2}.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#explore-the-distribution-of-categories",
    "href": "07-data-insights.html#explore-the-distribution-of-categories",
    "title": "4  Data insights",
    "section": "\n4.3 Explore the distribution of categories",
    "text": "4.3 Explore the distribution of categories\nUnderstanding how your data is distributed across important grouping variables is essential for context. In this case, the Palmer Penguins dataset includes key groupings such as species, island, and year, which might have significant effects on the relationships we want to study (e.g., between bill length and bill depth). By summarizing and visualizing the distribution of these variables, we can ensure that our analyses account for group-level differences, leading to more robust insights.\n\n4.3.1 Frequency\nBy grouping the data according to the species variable, we can calculate the total count (n) of penguins within each species. This summary provides a clear understanding of how the penguins are distributed across the different species in the dataset. Knowing the sample size for each species is crucial because it informs us about the representation of each group, which affects the reliability of any comparisons or analyses we plan to perform. If one species has far fewer observations than others, for example, it may require special consideration when interpreting results, as smaller sample sizes can lead to less robust conclusions. Understanding this distribution is a foundational step for any further analysis or comparison between species.\n\npenguins |&gt; \n  # calculations applied per species\n  group_by(species) |&gt; \n  # summarise the number of observations in each group\n  summarise(n = n())\n\n\n\n\nspecies\nn\n\n\n\nAdelie\n152\n\n\nChinstrap\n68\n\n\nGentoo\n124\n\n\n\n\n\n\nQuestion Are there 152 different penguins in our dataset? \nTRUE\nFALSE\n\n\nExplanation\n\nThe functions above count the number of rows of data - we need to determine if these are repeated or independent measures. We should remember that there is a column called individual_id if we use the n_distinct() function we can count how many unique IDs we have\n\npenguins |&gt; \n  # calculations applied per species\n  group_by(species) |&gt; \n  # summarise the number of observations in each group\n  summarise(n = n_distinct(individual_id))\n\n\n\n\nspecies\nn\n\n\n\nAdelie\n132\n\n\nChinstrap\n58\n\n\nGentoo\n94\n\n\n\n\n\n\nNow we can see that there are only 132 different Adelie penguins in our data\n\n\n4.3.2 Relative Frequency\nIn addition to counting the total number of penguins in each species, calculating the relative frequency can be a useful summary. Relative frequency shows the proportion or percentage of observations in each category relative to the total number of observations. This helps us understand not only the absolute count but also how common each species is compared to the others.\n\nprob_obs_species &lt;- penguins |&gt; \n  group_by(species) |&gt; \n  summarise(n = n()) |&gt; \n  # use mutate to make a new column relative frequency\n  mutate(prob_obs = n/sum(n))\n\nprob_obs_species\n\n\n\n\nspecies\nn\nprob_obs\n\n\n\nAdelie\n152\n0.4418605\n\n\nChinstrap\n68\n0.1976744\n\n\nGentoo\n124\n0.3604651\n\n\n\n\n\n\nSo about 44% of our sample is made up of observations from Adelie penguins. When it comes to making summaries about categorical data, that’s about the best we can do, we can make observations about the most common categorical observations, and the relative proportions.\n\npenguins |&gt; \n  mutate(species=fct_relevel(species, \n                             \"Adelie\",\n                             \"Gentoo\",\n                             \"Chinstrap\")) |&gt; \n  # set as factor and provide levels\n  ggplot()+\n  geom_bar(aes(x=species),\n           fill=\"steelblue\",\n           width=0.8)+\n  labs(x=\"Species\",\n       y = \"Number of observations\")+\n  geom_text(data=prob_obs_species,\n            aes(y=(n+10),\n                x=species,\n                label=scales::percent(prob_obs)))+\n  coord_flip()\n\n\n\n\n\n\n\nThis is an example of a figure we might use in a report or paper. Having cleaned up the theme, added some simple colour, made sure our labels are clear and descriptive, ordered our categories in ascending frequency order, and included some simple text of percentages to aid readability.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#two-or-more-categories",
    "href": "07-data-insights.html#two-or-more-categories",
    "title": "4  Data insights",
    "section": "\n4.4 Two or more categories",
    "text": "4.4 Two or more categories\nWhen investigating the distribution of observations, it’s important to consider how other variables might influence this relationship, such as sex, species or observations across years.\n\n\nSolution\n\nUnderstanding how frequency is broken down by island, species year and sex might be useful.\n\npenguins |&gt; \n  group_by(island, year, species, sex) |&gt; \n  summarise(n = n()) \n\n\n\n\nisland\nyear\nspecies\nsex\nn\n\n\n\nBiscoe\n2007\nAdelie\nFemale\n5\n\n\nBiscoe\n2007\nAdelie\nMale\n5\n\n\nBiscoe\n2007\nGentoo\nFemale\n16\n\n\nBiscoe\n2007\nGentoo\nMale\n17\n\n\nBiscoe\n2007\nGentoo\nNA\n1\n\n\nBiscoe\n2008\nAdelie\nFemale\n9\n\n\nBiscoe\n2008\nAdelie\nMale\n9\n\n\nBiscoe\n2008\nGentoo\nFemale\n22\n\n\nBiscoe\n2008\nGentoo\nMale\n23\n\n\nBiscoe\n2008\nGentoo\nNA\n1\n\n\nBiscoe\n2009\nAdelie\nFemale\n8\n\n\nBiscoe\n2009\nAdelie\nMale\n8\n\n\nBiscoe\n2009\nGentoo\nFemale\n20\n\n\nBiscoe\n2009\nGentoo\nMale\n21\n\n\nBiscoe\n2009\nGentoo\nNA\n3\n\n\nDream\n2007\nAdelie\nFemale\n9\n\n\nDream\n2007\nAdelie\nMale\n10\n\n\nDream\n2007\nAdelie\nNA\n1\n\n\nDream\n2007\nChinstrap\nFemale\n13\n\n\nDream\n2007\nChinstrap\nMale\n13\n\n\nDream\n2008\nAdelie\nFemale\n8\n\n\nDream\n2008\nAdelie\nMale\n8\n\n\nDream\n2008\nChinstrap\nFemale\n9\n\n\nDream\n2008\nChinstrap\nMale\n9\n\n\nDream\n2009\nAdelie\nFemale\n10\n\n\nDream\n2009\nAdelie\nMale\n10\n\n\nDream\n2009\nChinstrap\nFemale\n12\n\n\nDream\n2009\nChinstrap\nMale\n12\n\n\nTorgersen\n2007\nAdelie\nFemale\n8\n\n\nTorgersen\n2007\nAdelie\nMale\n7\n\n\nTorgersen\n2007\nAdelie\nNA\n5\n\n\nTorgersen\n2008\nAdelie\nFemale\n8\n\n\nTorgersen\n2008\nAdelie\nMale\n8\n\n\nTorgersen\n2009\nAdelie\nFemale\n8\n\n\nTorgersen\n2009\nAdelie\nMale\n8\n\n\n\n\n\n\n\npenguins |&gt; \n   ggplot(aes(x = species,\n             fill = sex))+\n   geom_bar(width=0.8,\n            position = position_dodge())+\n  labs(x=\"Species\",\n       y = \"Number of observations\")+\n  facet_grid(island ~ year)\n\n\n\n\n\n\n\n\nBy thinking about how categories might interact, we can identify interesting patterns or potential issues that could impact our analysis or data interpretation. Careful consideration of these interactions helps us ensure the validity of our findings. For instance:\n\nSex Distribution: Looking at the patterns of male and female penguins, I’m reassured that the number of males and females observed is roughly equal across species and locations. This balance is important because unequal representation of sexes could bias our results, particularly in biological traits like bill length or body mass. Since there’s no imbalance, we can confidently proceed without worrying about gender skewing our insights.\nSpecies Distribution by Island: There are different numbers of species on different islands, but this variation remains consistent across years. This suggests that the observed species distributions likely reflect true ecological patterns, rather than inconsistencies in data collection. If this trend were erratic across years, we might suspect sampling issues, but the consistency provides confidence in the accuracy of species representation.\nConsistency Across Years: The number of penguins observed is consistent across years, meaning we don’t see major fluctuations in sample size. This consistency is important because significant changes could suggest variations in data collection methods or environmental factors affecting penguin populations. A steady count ensures that any trends or patterns we observe are less likely to be the result of sampling inconsistencies.\nMissing Data for Sex: While there are some missing values for the sex variable, these gaps are small and don’t fit any obvious pattern. This suggests that the missing data is likely random and not linked to a particular species, location, or time period. Since the missing values are few and do not show a clear bias, we can choose to ignore these gaps in the analysis with reasonable confidence that they won’t heavily skew our results.\n\nBy reflecting on these patterns, we can be more confident in the integrity of the data and proceed with analysis, knowing that key variables are well-represented and that any missing data or inconsistencies are minimal and manageable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#missing-data",
    "href": "07-data-insights.html#missing-data",
    "title": "4  Data insights",
    "section": "\n4.5 Missing data",
    "text": "4.5 Missing data\nIn the previous section, we identified some missing data, particularly for the sex variable, and noted that it was relatively small and likely random, meaning we could safely remove it from calculations without concern for biasA systematic error or distortion in data collection or analysis that leads to inaccurate conclusions or results. . However, when it comes to our variables of interest, such as culmen length and culmen depth, it’s worth taking a closer look at the missing values to understand if they occur in any particular patterns or groupings.\nWe previously used functions like skimr::skim() and summary() to find that there are two missing values each for culmen length and depth. Although this small amount of missing data is unlikely to introduce significant bias, it’s still a good practice to investigate where these missing values are located in the dataset. Identifying patterns or specific groups (e.g., certain species or islands) associated with the missing values can help ensure that the missing data is not clustered in a way that could subtly affect our analysis.\n\npenguins |&gt;\n  # Filter rows where culmen length is NA\n  filter(is.na(culmen_length_mm)) |&gt; \n  # Group by species, sex and island\n  group_by(species, sex, year, island) |&gt;                 \n  summarise(n_missing = n())    \n\npenguins |&gt;\n  filter(is.na(culmen_depth_mm)) |&gt;         \n  group_by(species, sex, year, island) |&gt;               \n  summarise(n_missing = n())    \n\n\n\n\nspecies\nsex\nyear\nisland\nn_missing\n\n\n\nAdelie\nNA\n2007\nTorgersen\n1\n\n\nGentoo\nNA\n2009\nBiscoe\n1\n\n\n\n\n\n\n\nspecies\nsex\nyear\nisland\nn_missing\n\n\n\nAdelie\nNA\n2007\nTorgersen\n1\n\n\nGentoo\nNA\n2009\nBiscoe\n1\n\n\n\n\n\n\nThere is data missing from one Adelie penguin in 2007 on Torgersen, and one value missing from a Gentoo penguin in 2009 on Biscoe. This is true for both culmen length and depth, it is probably the same penguin in both cases, but how could we change our code to be sure?\n\n\nSolution\n\n\npenguins |&gt;\n  filter(is.na(culmen_length_mm)) |&gt; \n# ADD individual id into group_by arguments\n  group_by(species, sex, year, island, individual_id) |&gt;                 \n  summarise(n_missing = n())    \n\npenguins |&gt;\n  filter(is.na(culmen_depth_mm)) |&gt;         \n  group_by(species, sex, year, island, individual_id) |&gt;               \n  summarise(n_missing = n())    \n\n\n\n\nspecies\nsex\nyear\nisland\nindividual_id\nn_missing\n\n\n\nAdelie\nNA\n2007\nTorgersen\nN2A2\n1\n\n\nGentoo\nNA\n2009\nBiscoe\nN38A2\n1\n\n\n\n\n\n\n\nspecies\nsex\nyear\nisland\nindividual_id\nn_missing\n\n\n\nAdelie\nNA\n2007\nTorgersen\nN2A2\n1\n\n\nGentoo\nNA\n2009\nBiscoe\nN38A2\n1\n\n\n\n\n\n\n\nAgain here we could safely drop these from our data and regard these as missing at random.\n\n4.5.1 remove na\nIf we are confident that missing data is not causing bias, then there are a few different ways we can deal with missing data so that it doesn’t affect our calculations:\n\n4.5.1.1 drop_na() on everything before we start.\nUsing drop_na() to remove rows that contain any missing values across all variables can be useful, but it also runs the risk of losing a large portion of the data.\n\nPros: It’s a simple, all-in-one solution for datasets where missing values are widespread and problematic.\nCons: This method may remove entire rows of data, even if the missing value is only in a single, unimportant column. This can lead to significant data loss if many rows have missing values in non-critical variables.\n\n4.5.1.2 drop_na() on a particular variable.\nAnother approach is to remove rows with missing data in a specific variable, such as body_mass_g. This is less drastic but still needs to be done thoughtfully, especially if you overwrite the original dataset with the modified version. It’s often better to drop NAs temporarily for specific analyses or calculations.\n\nPros: You retain more data because you only remove rows with missing values in the column that matters for your current analysis.\nCons: You might still lose useful information from other columns. If you overwrite the dataset permanently (e.g., using penguins &lt;- penguins |&gt; drop_na(body_mass_g)), you can’t recover the dropped rows without reloading the data.\n\n4.5.1.3 Use arguments inside functions\nA more cautious approach is to handle missing data within summary or calculation functions, using the na.rm = TRUE argument. Many summary functions (like mean(), median(), or sum()) include this argument, which, when set to TRUE, excludes NA values from the calculation. This allows you to keep missing values in the dataset but ignore them only when performing calculations.\n\npenguins |&gt; \n  summarise(\n    mean_body_mass = mean(body_mass_g, na.rm = T)\n  )\n\n\nPros: This is the least destructive option because it allows you to handle missing data without removing any rows from the dataset. You only exclude NA values during specific calculations.\nCons: This doesn’t remove missing data, so you need to ensure that they won’t cause issues later in other analyses (e.g., regressions or visualizations).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#continuous-distributions",
    "href": "07-data-insights.html#continuous-distributions",
    "title": "4  Data insights",
    "section": "\n4.6 Continuous distributions",
    "text": "4.6 Continuous distributions\nA continuous variable is a type of numerical variable that can take an infinite number of values within a given range. Unlike categorical variables, which represent distinct groups or categories, continuous variables can represent quantities that can be measured with fine precision. Examples include height, weight, temperature, or in the case of the Palmer Penguins dataset, culmen length and culmen depth.\n\n4.6.1 Summarizing a Continuous Variable\nTo summarize a continuous variable, we typically focus on two key aspects:\n\ncentral tendencyA statistical measure that identifies the center or typical value in a dataset, often represented by the mean, median, or mode. (such as the mean)\nvariationA measure of how spread out or dispersed the data points are in relation to each other or the central value. (such as standard deviation)\n\n\n4.6.1.1 Mean (Central Tendency)\nThe mean is the average value of a continuous variable. It is calculated by summing all the values and then dividing by the number of observations. The mean gives us a sense of the central location of the data, or where most of the data points cluster around.\n\nExample: The mean culmen length of penguins gives us an average bill length, which represents the central value of the dataset.\n\n4.6.1.2 Variation (Spread)\nVariation describes how spread out the data points are around the mean.\nThis concept is critical in data analysis because every variable exhibits its own unique pattern of variation. Understanding these patterns can provide important insights into the data. For example, the variation in penguin body mass or bill length across different species may reveal ecological or biological differences that are crucial to your analysis.\nThe best way to explore and understand the pattern of variation is by visualizing the distribution of the variable’s values. By plotting histograms, boxplots, or density plots, you can observe how the data is spread out, whether it is concentrated around a central value, or if there are any outliers. These visualizations allow you to quickly identify the extent of variation and gain insights into the behavior of the variable.\n\n\n\n\n\n\nImportant\n\n\n\nIn the examples below I will look at the distributions of culmen length, you should modify and duplicate the code so that you check both culmen length and culmen depth.\n\n\n\n4.6.2 Histograms\nThis is the script to plot a frequency distribution, we only specify an x variable, because we intend to plot a histogramA graphical representation of the distribution of a dataset by grouping data into bins and showing the frequency of observations within each bin. , and the y variable is always the count of observations. Here we ask the data to be presented in 10 equally sized bins of data. In this case chopping the x axis range into 10 equal parts and counting the number of observations that fall within each one.\nHistograms are helpful because they show the distribution of a single numerical variable. They divide the data into “bins” (ranges of values) and count how many data points fall into each bin. This helps you:\nSee the shape of the data: You can easily spot if the data is normally distributed, skewed, or has multiple peaks (modes). Identify data spread: Histograms help you understand the spread or range of your data values. Detect outliers: Unusually high or low values become more noticeable in a histogram.\n\npenguins |&gt; \n  ggplot()+\n  geom_histogram(aes(x=culmen_length_mm),\n                 bins=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\nChange the bin argument\n\n\n\nChange the value specified to the bins argument and observe how the figure changes. It is usually a very good idea to try more than one set of bins in order to have better insights into the data\n\n\n\n4.6.3 Boxplots\nA boxplotA compact visual summary of a datasets distribution, showing the median, interquartile range (IQR), and outliers. is a powerful tool for visualizing the distribution of a numerical variable in a compact and easy-to-read format. They provide a wealth of information about the data, including its central tendency, spread, and presence of outliers, while also allowing for comparisons across different groups. Here’s why boxplots are particularly useful:\n\nCentral Tendency: The middle line within the box represents the median value of the data. Unlike the mean, which can be influenced by extreme values, the median provides a robust measure of central tendency, especially for skewed data.\nSpread and Variability: The box itself shows the interquartile range (IQR), which represents the spread of the middle 50% of the data. A wider box means more variability in the data, while a narrower box indicates that the data is more tightly clustered around the median.\nOutliers: Boxplots are particularly useful for identifying outliersData points that are significantly different from the rest of the dataset, often lying far from the median or mean. — data points that fall far outside the typical range of values. These are shown as individual points beyond the “whiskers” (the lines extending from the box), making it easy to spot extreme values that could affect your analysis.\nComparison Between Groups: One of the greatest strengths of boxplots is their ability to compare distributions across different groups. By plotting multiple boxplots side by side (e.g., for different penguin species), you can easily see how a variable’s distribution changes between categories, helping to highlight differences or similarities between groups.\n\n\npenguins |&gt; \n  ggplot()+\n  geom_boxplot(aes(x=culmen_length_mm))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#continuous-and-categorical",
    "href": "07-data-insights.html#continuous-and-categorical",
    "title": "4  Data insights",
    "section": "\n4.7 Continuous and categorical",
    "text": "4.7 Continuous and categorical\nWhen you have a continuous variable (like culmen length) and want to explore how its distribution varies across a categorical variable (like species), visualizing the data is the best place to start. Simple figures allow us to easily see potential patterns and relationships between the variables.\nTo investigate how culmen length varies between different penguin species, we can create a simple boxplot that shows the distribution of culmen lengths for each species. This will help us quickly understand whether the different species tend to have different culmen lengths and whether any species has more variability or outliers than others.\n\npenguins %&gt;% \n    ggplot(aes(x = species,\n               y = body_mass_g,\n               fill = species))+\n    geom_boxplot(width = 0.2)+\n  coord_flip()\n\n\n\n\n\n\n\nBy subgrouping the data, such as looking at the distribution of culmen length by species, we can more easily identify outliers within each group. When we analyze all penguins together, outliers might be harder to spot or might seem more extreme. However, when we break the data down by species, we can see that what might have appeared as an outlier in the overall data may actually be a typical value within a specific group.\nThis shows us that outliers are context-specific. A value that is unusually large or small for one species might fall well within the normal range for another species. Subgrouping helps us understand that outliers are not absolute—they depend on the context of the data and the groups being compared. This insight is crucial for accurate analysis, as it helps us avoid labeling a data point as unusual without considering its appropriate context.\n\n4.7.1 Density plots\nHistograms and density plots are both useful tools for visualizing the distribution of a continuous variable, but they present the information in slightly different ways.\n\nHistograms divide the data into bins and count how many data points fall into each bin. This gives a blocky visual representation of the data’s distribution and is great for showing the frequency of data points within specific ranges. The choice of bin width can influence how smooth or rough the histogram looks, and it provides an easy way to understand how the data is spread out.\nDensity plots, on the other hand, provide a smoothed estimate of the data distribution. Instead of counting data points in bins, a density plot estimates the probability density of the variable, giving a continuous curve that helps visualize the shape of the data. The area under the curve of a density plot sums to 1, making it particularly useful when comparing distributions between groups, as it normalizes the data regardless of sample size.\n\nIn summary, histograms are useful for showing the exact count of data points in specific ranges, while density plots give a smoothed representation of the data’s distribution, helping to highlight the overall shape. Both are valuable tools, and the choice between them depends on the level of detail or smoothness you want to highlight.\nBelow we have an example of a density plot, more useful when comparing groups of uneven sample sizes:\n\npenguins %&gt;% \n  ggplot(aes(fill = species))+\n  geom_density(aes(x = culmen_length_mm),\n                   position = \"identity\",\n                   alpha = 0.6)\n\n\n\n\n\n\n\n\n4.7.2 GGridges\nThe package ggridges (Wilke (2021)) provides some excellent extra geoms to supplement ggplot. One if its most useful features is to to allow different groups to be mapped to the y axis, so that histograms are more easily viewed.\n\nlibrary(ggridges)\nggplot(penguins, aes(x = culmen_length_mm, \n                     y = species)) + \n  ggridges::geom_density_ridges()\n\n\n\n\n\n\n\nWhen a density plot shows double peaks (also known as bimodal distribution), it often suggests that the data may contain two distinct groups or underlying patterns that are not immediately visible. In such cases, subgrouping the data can help clarify these peaks. By breaking the data down into categories—such as species, gender, or location—we can explore whether the peaks correspond to meaningful subgroups within the dataset.\nFor example, in the Palmer Penguins dataset, a bimodal distribution of bill length might indicate that different species have distinct bill lengths, each contributing to one of the peaks. Subgrouping by species would allow us to visualize separate distributions for each species and better understand the source of the variation.\nSubgrouping helps us gain clarity by revealing if the apparent double peaks are caused by the presence of multiple groups, and it allows us to analyze these groups separately, leading to more accurate insights.\nLet’s see what happens when we account for sex:\n\npenguins |&gt; \n  drop_na() |&gt; \nggplot(aes(x = culmen_length_mm, \n                     y = species, fill = sex)) + \n    ggridges::geom_density_ridges()\n\n\n\n\n\n\n\nIf we still see double peaks after subgrouping the data, it suggests that there may be additional underlying factors or patterns within each subgroup that we haven’t accounted for yet. These could be caused by other variables such as age, environmental conditions, or even measurement error, depending on the context of the data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#normality",
    "href": "07-data-insights.html#normality",
    "title": "4  Data insights",
    "section": "\n4.8 Normality",
    "text": "4.8 Normality\nIf our data follows a normal distributionAlso known as the “Gaussian distribution,” it is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. , we can use just two key metrics — mean and standard deviation — to predict how the data is spread out and to calculate the probability of observing a specific value. The normal distribution is symmetric and bell-shaped, meaning most data points cluster around the mean, with fewer observations the farther we move from it.\n\n4.8.1 Why These Measures Matter:\n\nThe meanThe average of a dataset, calculated by summing all values and dividing by the number of observations. gives us a general idea of the “average” value, but the standard deviationA measure of the amount of variation or spread in a dataset, indicating how far data points are from the mean on average. tells us how reliable that mean is by showing how much the data varies around it.\nTogether, these two metrics help us understand the overall distribution of a continuous variable. For normally distributed data, roughly 68% of values lie within one standard deviation of the mean, which provides a good sense of the data’s spread.\n\n4.8.2 QQplots\nA QQ plot is a useful tool for visually assessing whether a dataset follows a particular distribution, such as the normal distribution. At first glance, QQ plots may look unfamiliar, but they are simple and powerful once you understand the concept.\nIn a QQ plot, your sample data is plotted on the y-axis, and the theoretical distribution (often a normal distribution) is plotted on the x-axis. If your data follows a normal distribution, the points in the QQ plot will align along a straight diagonal line. Any deviation from this line indicates that your data departs from the normal distribution, either through skewness, heavy tails, or other irregularities.\nHow It Works:\n\nData Points on the Y-Axis: These are the actual data values from your sample.\nTheoretical Distribution on the X-Axis: These represent what the data would look like if it perfectly followed a normal distribution.\n\nIf the data aligns well with the theoretical distribution, it forms a straight line. If not, the data will curve away from the line, signaling potential departures from normality.\nWatch this video to see QQ plots explained in action and how they help in assessing the distribution of your data!\n\nlibrary(qqplotr)\nggplot(penguins, aes(sample = culmen_length_mm))+\n    stat_qq_band() +\n    stat_qq_line() +\n    stat_qq_point() \n\n\n\n\n\n\n\n\n4.8.3 Shapiro test\nThe Shapiro-Wilk test is a commonly used statistical test to check whether a dataset follows a normal distribution. It gives a p-value that helps determine if the data significantly deviates from normality. While useful, the Shapiro test has its limitations, especially when it comes to sample size\nSensitivity to Sample Size: The Shapiro-Wilk test is sensitive to sample size. For large samples, even very small deviations from normality can result in significant p-values, leading to a rejection of normality even when the deviations are minor and practically insignificant.\n\nlibrary(rstatix)\n\npenguins %&gt;% \n  shapiro_test(culmen_length_mm)\n\n\n\n\nvariable\nstatistic\np\n\n\nculmen_length_mm\n0.9748548\n1.12e-05\n\n\n\n\n\nUnlike the Shapiro-Wilk test, a QQ plot provides a visual way to check for normality. It allows you to see exactly how the data deviates from a normal distribution. You can spot patterns such as skewness or heavy tails, which the Shapiro test cannot differentiate.\nQQ plots are also more flexible with sample size. They work well with both large and small datasets.\n\n4.8.4 Normality in subgroups\nWhen you analyze the entire dataset without subgrouping, you combine data from different groups, such as species, locations, or treatments. Each group may have its own unique distribution. For example, in the Palmer Penguins dataset, bill length and bill depth may differ significantly between penguin species. If you test for normality across the entire dataset, the mix of different distributions might lead to misleading results—either falsely showing non-normality or obscuring deviations that are present in specific groups.\nSubgrouping allows you to test for normality within each specific group and understand whether the deviations from normality are general or isolated to certain groups.\n\nlibrary(qqplotr)\nggplot(penguins, aes(sample = culmen_length_mm))+\n    stat_qq_band() +\n    stat_qq_line() +\n    stat_qq_point() +\n  facet_wrap(~species)\n\n\n\n\n\n\n\n\npenguins %&gt;% \n  group_by(species) |&gt; \n  shapiro_test(culmen_length_mm)\n\n\n\n\nspecies\nvariable\nstatistic\np\n\n\n\nAdelie\nculmen_length_mm\n0.9933618\n0.7166005\n\n\nChinstrap\nculmen_length_mm\n0.9752496\n0.1940926\n\n\nGentoo\nculmen_length_mm\n0.9727224\n0.0134914\n\n\n\n\n\n\nNow we can see that when splitting into species subgroups - that the qqplots are much better. The Shapiro wilk test indicates that the Gentoo population deviates from a normal distribution for bill length, but our qqplot indicates this is very minor and we can still safely use descriptive statistics of mean and standard deviation that rely on this assumption.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#summary-statistics",
    "href": "07-data-insights.html#summary-statistics",
    "title": "4  Data insights",
    "section": "\n4.9 Summary statistics",
    "text": "4.9 Summary statistics\n\n4.9.1 Calculating mean and standard deviation\nstandard deviation (or s) is a measure of how dispersed the data is in relation to the mean. Low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out. We can calculate these easily in R with group_by() and summarise().\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarise(mean = mean(culmen_length_mm),\n            sd = sd(culmen_length_mm))\n\n\n\n\nspecies\nmean\nsd\n\n\n\nAdelie\nNA\nNA\n\n\nChinstrap\n48.83382\n3.339256\n\n\nGentoo\nNA\nNA\n\n\n\n\n\n\n\n\nSolution\n\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarise(mean = mean(culmen_length_mm, na.rm = T),\n            sd = sd(culmen_length_mm, na.rm =T))\n\n\n\n\nspecies\nmean\nsd\n\n\n\nAdelie\n38.79139\n2.663405\n\n\nChinstrap\n48.83382\n3.339256\n\n\nGentoo\n47.50488\n3.081857\n\n\n\n\n\n\n\n\n4.9.2 Median and quartiles\nIf we were dealing with variables that did not fit a normal distribution, using summary statistics like max, min, median, and interquartile range (IQR) provides a robust way to describe the data’s spread and central tendency. Unlike the mean and standard deviation, which are sensitive to outliers and skewed data, the median and IQR give a more accurate representation of the distribution’s center and spread in non-normal data.\n\nMedian: This is the middle value and provides a better measure of central tendency than the mean when data is skewed. IQR (Interquartile Range): This measures the spread of the middle 50% of the data (between the 25th and 75th percentiles) and is less affected by extreme values.\nMin and Max: These values show the range of the data, highlighting any extreme values or outliers.\n\nThese summary statistics directly correspond to the boxplot: the median is the line inside the box, the IQR defines the box’s edges, and the min/max (after removing outliers) are represented by the ends of the whiskers.\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarise(median = median(culmen_length_mm, na.rm = T),\n            quantile = quantile(culmen_length_mm, c(0.25, 0.5, 0.75), na.rm=TRUE),\n            max = max(culmen_length_mm, na.rm = T),\n            min = min(culmen_length_mm, na.rm = T)\n  )\n\n\n\n\nspecies\nmedian\nquantile\nmax\nmin\n\n\n\nAdelie\n38.80\n36.750\n46.0\n32.1\n\n\nAdelie\n38.80\n38.800\n46.0\n32.1\n\n\nAdelie\n38.80\n40.750\n46.0\n32.1\n\n\nChinstrap\n49.55\n46.350\n58.0\n40.9\n\n\nChinstrap\n49.55\n49.550\n58.0\n40.9\n\n\nChinstrap\n49.55\n51.075\n58.0\n40.9\n\n\nGentoo\n47.30\n45.300\n59.6\n40.9\n\n\nGentoo\n47.30\n47.300\n59.6\n40.9\n\n\nGentoo\n47.30\n49.550\n59.6\n40.9\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you ran the command above you likely received a warning in your R console\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nThis is because summary() is supposed to only supply one row per group - by asking for the different quartiles from each group it forces it to print more rows. Try doing what R asks and see what happens\nSee here",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#collinearity",
    "href": "07-data-insights.html#collinearity",
    "title": "4  Data insights",
    "section": "\n4.10 Collinearity",
    "text": "4.10 Collinearity\nCollinearity occurs when two or more independent variables in a dataset are highly correlated with each other, meaning that they show a strong linear relationship. In simpler terms, it means that one variable can be almost perfectly predicted by another. Collinearity is particularly problematic in regression analysis, where you want each independent variable to provide unique information about the outcome (dependent variable).\nImagine you’re studying penguin bill characteristics in the Palmer Penguins dataset. If you include both culmen length and flipper length are highly correlated, then collinearity may occur. This would make it difficult to determine whether changes in culmen depth are due to culmen length or flipper length, or a combination of both, as they provide overlapping information.\n\n4.10.1 GGally\nThe R package GGally helps us understand the relationships between variables, by producing a pairwise matric grid made of:\n\nScatterplots: The scatterplots in a pairwise matrix (created by ggpairs()) allow you to visually inspect the relationship between pairs of continuous variables. If two variables show a strong linear relationship (points forming a clear diagonal line), it suggests collinearity between those variables.\nCorrelation Coefficients: In the pairwise matrix, you can also display correlation coefficients. When these values are close to 1 (positive correlation) or -1 (negative correlation), it suggests that the variables are highly correlated and might indicate collinearity.\n\n\nlibrary(GGally)\n\npenguins |&gt; \n  select(species, \n         island, \n         culmen_length_mm, \n         culmen_depth_mm, \n         flipper_length_mm, \n         body_mass_g, \n         sex) |&gt; \n  ggpairs()\n\n\n\n\n\n\n\nFrom these pairwise plots we can see that there is a correlation between flipper length, body mass and the culmen length and depth variables. This is useful information to know when considering the relationship between variables for cause and effect, and will be important when build statistical models.\n\n4.10.1.1 Applying Subgroups of Color in GGally (Using ggplot2)\nSince GGally builds on ggplot2, you can apply the same customisations that ggplot2 allows, including the use of color to represent different subgroups in your data. This is especially useful when exploring relationships between variables, as it helps highlight patterns or differences between categories (e.g., species, sex, or location).\n\npenguins |&gt; \n  ggpairs(columns = c(\n         \"island\", \n         \"culmen_length_mm\", \n         \"culmen_depth_mm\", \n         \"flipper_length_mm\", \n         \"body_mass_g\", \n         \"sex\"), \n         ggplot2::aes(colour = species))\n\n\n\n\n\n\n\nWhy is this useful? It means we can identify group-specific trends: Adding colour based on a categorical variable (like species) helps you quickly spot trends or patterns that may exist only within certain groups.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#correlation",
    "href": "07-data-insights.html#correlation",
    "title": "4  Data insights",
    "section": "\n4.11 Correlation",
    "text": "4.11 Correlation\nA common measure of association between two numerical variables is the correlation coefficient. The correlation metric is a numerical measure of the strength of an association\nThere are several measures of correlation including:\n\nPearson’s correlation coefficient : good for describing linear associations\nSpearman’s rank correlation coefficient: a rank ordered correlation - good for when the assumptions for Pearson’s correlation is not met.\n\nPearson’s correlation coefficient r is designed to measure the strength of a linear (straight line) association. Pearson’s takes a value between -1 and 1.\n\nA value of 0 means there is no linear association between the variables\nA value of 1 means there is a perfect positive association between the variables\nA value of -1 means there is a perfect negative association between the variables\n\nA perfect association is one where we can predict the value of one variable with complete accuracy, just by knowing the value of the other variable.\nWe can use the rstatix::cor_test() function in R to calculate Pearson’s correlation coefficient.\n\nlibrary(rstatix)\n\npenguins %&gt;% \n  cor_test(culmen_length_mm, \n           culmen_depth_mm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar1\nvar2\ncor\nstatistic\np\nconf.low\nconf.high\nmethod\n\n\nculmen_length_mm\nculmen_depth_mm\n-0.24\n-4.459093\n1.12e-05\n-0.3328072\n-0.1323004\nPearson\n\n\n\n\n\nThis tells us two features of the association. It’s sign and magnitude. The coefficient is negative, so as bill length increases, bill depth decreases. The value -0.24 indicates a weak negative relationship.\nBecause Pearson’s coefficient is designed to summarise the strength of a linear relationship, this can be misleading if the relationship is not linear e.g. curved or humped. This is why it’s always a good idea to plot the relationship first (see above).\nEven when the relationship is linear, it doesn’t tell us anything about the steepness of the association (see above). It only tells us how often a change in one variable can predict the change in the other not the value of that change.\n\n\n\n\n\n\n\n\nThis can be difficult to understand at first, so carefully consider the figure above.\n\nThe first row above shows differing levels of the strength of association. If we drew a perfect straight line between two variables, how closely do the data points fit around this line.\nThe second row shows a series of perfect linear relationships. We can accurately predict the value of one variable just by knowing the value of the other variable, but the steepness of the relationship in each example is very different. This is important because it means a perfect association can still have a small effect.\nThe third row shows a series of associations where there is clearly a relationship between the two variables, but it is also not linear so would be inappropriate for a Pearson’s correlation.\n\n\n4.11.1 Assumptions of Pearson’s correlation\n\nA linear relationship\nNormal distribution of the variance\n\nIn our case we have already looked at the relationship and the assumptions of normality and would conclude our data is basically fine for using Pearson’s correlation.\nBut what should we do if the relationship between our variables is non-linear or does not follow a normal distribution? Instead of using Pearson’s correlation coefficient we can calculate something called a rank correlation.\n\n4.11.2 Spearman’s rank correlation\nInstead of working with the raw values of our two variables we can use rank ordering instead. The idea is pretty simple if we start with the lowest vaule in a variable and order it as ‘1’, then assign labels ‘2’, ‘3’ etc. as we ascend in rank order. We can see a way that this could be applied manually with the function dense_rank from dplyr below:\n\nlibrary(rstatix)\n\npenguins %&gt;% \n  cor_test(culmen_length_mm, \n           culmen_depth_mm,\n           method = \"spearman\")\n\n\n\n\n\n\n\n\n\n\n\n\nvar1\nvar2\ncor\nstatistic\np\nmethod\n\n\nculmen_length_mm\nculmen_depth_mm\n-0.22\n8145268\n3.51e-05\nSpearman\n\n\n\n\n\nYou can see that we get a similar but not identical estimate of the strength of association. Generally non-parametric tests are less powerful, but have fewer assumptions about the structure of our data.\n\n4.11.3 Graphical summaries between numeric variables\nCorrelation coefficients are a quick and simple way to attach a metric to the level of association between two variables. They are limited however in that a single number can never capture the every aspect of their relationship. This is why we visualise our data, and the best way to do that for two continuous variables is to make a scatterplot\n\nscatterplot &lt;- ggplot(penguins, aes(x= culmen_length_mm, \n                                    y= culmen_depth_mm)) +\n  geom_point()\n\nscatterplot\n\n\n\n\n\n\n\n\n4.11.3.1 Marginal plots\nMarginal plots are useful when you want to explore the distribution of individual variables alongside a scatterplot of the relationship between two variables. They add histograms, density plots, or boxplots to the margins of a scatterplot, providing extra information about the distribution of each variable separately. So we would could add any of the plots we used previously to look at variance in each of our variables culmen_length_mm and culmen_depth_mm\n\n4.11.3.2 How to Create Marginal Plots: Using patchwork in R\npatchwork is an R package that makes it easy to combine multiple ggplots into a single plot layout, including creating layouts for marginal plots. While ggplot2 doesn’t natively support marginal plots, patchwork allows you to combine the scatterplot with additional histograms or density plots at the top and side, creating a custom marginal plot.\n\nlibrary(patchwork) # package calls should be placed at the TOP of your script\n\nbill_depth_marginal &lt;- penguins %&gt;% \n  ggplot()+\n  geom_density(aes(x=culmen_depth_mm), fill=\"darkgrey\")+\n  theme_void()+\n  coord_flip() # this graph needs to be rotated\n\nbill_length_marginal &lt;- penguins %&gt;% \n  ggplot()+\n  geom_density(aes(x=culmen_length_mm), fill=\"darkgrey\")+\n  theme_void()\n\nlayout &lt;- \"\nAA#\nBBC\nBBC\"\n\n# layout is easiest to organise using a text distribution, where ABC equal the three plots in order, and the grid is how much space they take up. We could easily make the main plot bigger and marginals smaller with\n\nscatterplot &lt;- scatterplot +\n  geom_smooth(method = \"lm\",\n              se = FALSE)\n\nbill_length_marginal+\n  scatterplot+\n  bill_depth_marginal+ \n  # order of plots is important\n  plot_layout(design=layout) \n\n\n\n\n\n\n# uses the layout argument defined above to arrange the size and position of plots\n\nFrom this plot we would conclude that there is weak \npositive\nnegative relationship between culmen length and culmen depth.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#important-variables",
    "href": "07-data-insights.html#important-variables",
    "title": "4  Data insights",
    "section": "\n4.12 Important variables",
    "text": "4.12 Important variables\nWhen analyzing the correlation between two variables, it’s essential to consider important categories or grouping variables (like species, gender, location, or time) to avoid misleading results and oversimplified interpretations. Ignoring relevant categories can result in omitted variable bias, which occurs when a key variable is left out of the analysis and its influence distorts the observed relationship between the variables of interest.\nThink about important categories or subgroups that should be included when considering the relationship between culment length and depth?\n\n\nSolution\n\n\ncolours &lt;- c(\"cyan\",\n             \"darkorange\",\n             \"purple\")\n\nscatterplot_2 &lt;- ggplot(penguins, aes(x= culmen_length_mm, \n                     y= culmen_depth_mm,\n                     colour=species)) +\n    geom_point()+\n  geom_smooth(method=\"lm\",\n              se=FALSE)+\n  scale_colour_manual(values=colours)+\n  theme_classic()+\n  theme(legend.position=\"none\")+\n    labs(x=\"Bill length (mm)\",\n         y=\"Bill depth (mm)\")\n\nbill_depth_marginal_2 &lt;- penguins %&gt;% \n  ggplot()+\n  geom_density(aes(x=culmen_depth_mm,\n                   fill=species),\n               alpha=0.5)+\n  scale_fill_manual(values=colours)+\n  theme_void()+\n  coord_flip() # this graph needs to be rotated\n\nbill_length_marginal_2 &lt;- penguins %&gt;% \n  ggplot()+\n  geom_density(aes(x=culmen_length_mm,\n                   fill=species),\n               alpha=0.5)+\n  scale_fill_manual(values=colours)+\n  theme_void()+\n  theme(legend.position=\"none\")\n\nlayout2 &lt;- \"\nAAA#\nBBBC\nBBBC\nBBBC\"\n\nbill_length_marginal_2+scatterplot_2+bill_depth_marginal_2+ # order of plots is important\n  plot_layout(design=layout2) # uses the layout argument defined above to arrange the size and position of plots\n\n\n\n\n\n\n\nWe now clearly see a striking reversal of our previous trend, that in fact within each species of penguin there is an overall positive association between bill length and depth.\nThis is called Simpson’s Paradox it is a classic example of omitted variable bias, where a trend observed in different groups reverses or disappears when the data is combined. This happens when a key variable (often categorical) is not included in the analysis, leading to misleading conclusions.\nThis should prompt us to re-evaluate our correlation metrics:\n\npenguins %&gt;% \n  group_by(species) %&gt;% \n  cor_test(culmen_length_mm, \n           culmen_depth_mm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nvar1\nvar2\ncor\nstatistic\np\nconf.low\nconf.high\nmethod\n\n\n\nAdelie\nculmen_length_mm\nculmen_depth_mm\n0.39\n5.193285\n7e-07\n0.2472226\n0.5187796\nPearson\n\n\nChinstrap\nculmen_length_mm\nculmen_depth_mm\n0.65\n7.014647\n0e+00\n0.4917326\n0.7717134\nPearson\n\n\nGentoo\nculmen_length_mm\nculmen_depth_mm\n0.64\n9.244703\n0e+00\n0.5262952\n0.7365271\nPearson\n\n\n\n\n\n\nWe now see that all of our correlation coefficients are stronger and positive within each species.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#summary",
    "href": "07-data-insights.html#summary",
    "title": "4  Data insights",
    "section": "\n4.13 Summary",
    "text": "4.13 Summary\nWhen analysing data, it’s important to start with simple visualizations like scatterplots, boxplots, or histograms to uncover basic patterns and distributions. Before continuing with simple visualisations of relationships and groupings. These tools help provide quick insights into your data, such as distribution, trends, or outliers.\nHowever, it’s equally important to think carefully about the context of your data. This includes considering grouping variables (like species, sex, or location) to avoid pitfalls like omitted variable bias or Simpson’s Paradox, where important patterns may be hidden or misrepresented if key variables are ignored.\nBy incorporating relevant categories and ensuring you’re accounting for potential confounding variables, you make sure your analysis is more accurate, insightful, and meaningful. Always take a moment to reflect on the broader context before drawing conclusions from simple data summaries",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#glossary",
    "href": "07-data-insights.html#glossary",
    "title": "4  Data insights",
    "section": "\n4.14 Glossary",
    "text": "4.14 Glossary\n\n\n\n\nterm\ndefinition\n\n\n\nbias\nA systematic error or distortion in data collection or analysis that leads to inaccurate conclusions or results.\n\n\nboxplot\nA compact visual summary of a datasets distribution, showing the median, interquartile range (IQR), and outliers.\n\n\ncentral tendency\nA statistical measure that identifies the center or typical value in a dataset, often represented by the mean, median, or mode.\n\n\nconfounding variable\nA variable that affects both the independent and dependent variables, potentially distorting the observed relationship between them.\n\n\ncorrelation coefficient\nA numerical value ranging from -1 to 1 that measures the strength and direction of the linear relationship between two variables.\n\n\ndensity plots\n\n\n\nhistogram\nA graphical representation of the distribution of a dataset by grouping data into bins and showing the frequency of observations within each bin.\n\n\nmean\nThe average of a dataset, calculated by summing all values and dividing by the number of observations.\n\n\nnormal distribution\nAlso known as the \"Gaussian distribution,\" it is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean.\n\n\noutliers\nData points that are significantly different from the rest of the dataset, often lying far from the median or mean.\n\n\nscatterplots\n\n\n\nstandard deviation\nA measure of the amount of variation or spread in a dataset, indicating how far data points are from the mean on average.\n\n\nvariation\nA measure of how spread out or dispersed the data points are in relation to each other or the central value.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "07-data-insights.html#reading",
    "href": "07-data-insights.html#reading",
    "title": "4  Data insights",
    "section": "\n4.15 Reading",
    "text": "4.15 Reading\n\nA protocol for data exploration to avoid common statistical problems",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data insights</span>"
    ]
  },
  {
    "objectID": "08-inferential-statistics.html",
    "href": "08-inferential-statistics.html",
    "title": "5  Inferential statistics",
    "section": "",
    "text": "5.1 Darwin’s maize data\nLoss of genetic diversity is an important issue in the conservation of species. Declines in population size due to over exploitation, habitat fragmentation lead to loss of genetic diversity. Even populations restored to viable numbers through conservation efforts may suffer from continued loss of population fitness because of inbreeding depression.\nCharles Darwin even wrote a book on the subject “The Effects of Cross and Self-Fertilisation in the Vegetable Kingdom”. In this he describes how he produced seeds of maize (Zea mays) that were fertilised with pollen from the same individual or from a different plant. The height of the seedlings that were produced from these were then measured as a proxy for their evolutionary fitness.\nDarwin wanted to know whether inbreeding reduced the fitness of the selfed plants - this was his hypothesis. The data we are going to use today is from Darwin’s original dataset.\nDownload Maize data as csv\nlibrary(tidyverse)\nlibrary(here)\n\ndarwin &lt;- read_csv(here(\"data\", \"darwin.csv\"))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferential statistics</span>"
    ]
  },
  {
    "objectID": "08-inferential-statistics.html#darwins-maize-data",
    "href": "08-inferential-statistics.html#darwins-maize-data",
    "title": "5  Inferential statistics",
    "section": "",
    "text": "Important\n\n\n\nSet up a new project\nHave you got separate subfolders set up within your project?\nYou should set up a script to put your work into - use this to write instructions and store comments.\nUse the File &gt; New Script menu item and select an R Script.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferential statistics</span>"
    ]
  },
  {
    "objectID": "08-inferential-statistics.html#activity-1-carry-out-some-basic-exploratory-data-analysis",
    "href": "08-inferential-statistics.html#activity-1-carry-out-some-basic-exploratory-data-analysis",
    "title": "5  Inferential statistics",
    "section": "\n5.2 Activity 1: Carry out some basic exploratory data analysis",
    "text": "5.2 Activity 1: Carry out some basic exploratory data analysis\nThe first thing we should know by now is to always start by exploring our data. If you want to stretch yourself, see if you can perform a basic data check without prompts.\n\n\nSolution\n\n\n# check the structure of the data\nglimpse(darwin)\n\n# check data is in a tidy format\nhead(darwin)\n\n# check variable names\ncolnames(darwin)\n\n\n# clean up column names\n\ndarwin &lt;- janitor::clean_names(darwin)\n\n# check for duplication\ndarwin |&gt; \n  duplicated() |&gt; \n  sum()\n\n# check for typos - by looking at impossible values\ndarwin |&gt; \n  summarise(min=min(height, na.rm=TRUE), \n            max=max(height, na.rm=TRUE))\n\n# check for typos by looking at distinct characters/values\n\ndarwin |&gt; \n  distinct(pair)\n\ndarwin |&gt; \n  distinct(type)\n\n# missing values\ndarwin |&gt; \n  is.na() |&gt; \n  sum()\n\n# quick summary\n\nsummary(darwin)\n\n\n\n5.2.1 Visualisation\nNow seems like a good time for our first data visualisation.\n\ndarwin |&gt; \n  ggplot(aes(x=type,\n         y=height))+\n  geom_point()\n\n\n\n\n\n\n# you could also substitute (or combine) other geoms including\n# geom_boxplot()\n# geom_violin()\n# geom_histogram()\n# Why not have a go and see what you can make?\n\nThe graph clearly shows that the average height of the ‘crossed’ plants is greater than that of the ‘selfed’ plants. But we need to investigate further in order to determine whether the signal (any apparent differences in mean values) is greater than the level of noise (variance within the different groups).\nThe variance appears to be roughly similar between the two groups - though by making a graph we can now clearly see that in the crossed group, there is a potential outlier with a value of 12.\n\n5.2.2 Comparing groups\nAs we have seen previously we can use various tidy functions to determine the mean and standard deviations of our groups.\n\ndarwin |&gt; \n  group_by(type) |&gt; \n  summarise(mean=mean(height),\n            sd=sd(height)\n            )\n\n\n\n\ntype\nmean\nsd\n\n\n\nCross\n20.19167\n3.616945\n\n\nSelf\n17.57500\n2.051676\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou should (re)familiarise yourself with how (and why) we calculate standard deviation.\n\n\nSummary statistics like these could be presented as figures or tables. We normally reserve tables for very simple sets of numbers, and this instance we could present both.\n\n# make a new object\ndarwin_summary &lt;-darwin |&gt; \n  group_by(type) |&gt; \n  summarise(mean=mean(height),\n            sd=sd(height))\n\n# make a summary plot\ndarwin_summary |&gt; \n  ggplot(aes(x=type,\n             y=mean))+\n  geom_pointrange(aes(ymin=mean-sd, ymax=mean+sd))+\n  theme_bw()\n\n\n\n\n\n\n# put this at top of script\nlibrary(kableExtra)\n\n# use kable extra functions to make a nice table (could be replaced with kable() if needed)\ndarwin_summary |&gt; \n    kbl(caption=\"Summary statistics of crossed and selfed maize plants\") |&gt; \n  kable_styling(bootstrap_options = \"striped\", full_width = T, position = \"left\")\n\n\nSummary statistics of crossed and selfed maize plants\n\ntype\nmean\nsd\n\n\n\nCross\n20.19167\n3.616945\n\n\nSelf\n17.57500\n2.051676\n\n\n\n\n\nDescriptive statistics and careful data checking are often skipped steps in the rush to answer the big questions. However, description is an essential part of early phase analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferential statistics</span>"
    ]
  },
  {
    "objectID": "08-inferential-statistics.html#estimation",
    "href": "08-inferential-statistics.html#estimation",
    "title": "5  Inferential statistics",
    "section": "\n5.3 Estimation",
    "text": "5.3 Estimation\nIn the section above we concentrated on description. But the hypothesis Darwin aimed to test was whether ‘inbreeding reduced the fitness of the selfed plants’. To do this we will use the height of the plants as a proxy for fitness and explicitly address whether there is a difference in the mean heights of the plants between these two groups.\nOur goal is to:\n\nEstimate the mean heights of the plants in these two groups\nEstimate the mean difference in heights between these two groups\nQuantify our confidence in these differences\n\n\n5.3.1 Differences between groups\nDarwin’s data used match pairs - each pair shared one parent. So that in pair 1 the same parent plant was either ‘selfed’ or ‘crossed’ to produce offspring. This is a powerful approach to experimental design, as it means that we can look at the differences in heights across each of the 15 pairs of plants - rather than having to infer differences from two randomly derived groups.\nIn order to calculate the differences in height between each pair we need to do some data wrangling with tidyr::pivot_wider() {Appendix C} and calculations with mutate.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferential statistics</span>"
    ]
  },
  {
    "objectID": "08-inferential-statistics.html#activity-2-differences",
    "href": "08-inferential-statistics.html#activity-2-differences",
    "title": "5  Inferential statistics",
    "section": "\n5.4 Activity 2: Differences",
    "text": "5.4 Activity 2: Differences\nCreate a new column called difference with the height of the selfed plant in each pair subtracted from the crossed plant.\n\n\nSolution\n\n\n# pivot data to wide format then subtract Selfed plant heights from Crossed plant heights\n\ndarwin_wide &lt;- darwin |&gt; \n  pivot_wider(names_from = type, \n              values_from = height) |&gt; \n  mutate(difference = Cross - Self)\n\n\nWe now have the difference in height for each pair, we can use this to calculate the mean difference in heights between paired plants, and the amount of variance (as standard deviation)\n\ndifference_summary &lt;- darwin_wide |&gt; \n  summarise(mean=mean(difference),\n            sd=sd(difference),\n            n=n())\n\ndifference_summary\n\n\n\n\nmean\nsd\nn\n\n\n2.616667\n4.718047\n15\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhat we have just calculated is the average difference in height between these groups of plants and the standard deviation of the difference Moving forward we will be working a lot with estimating our confidence in differences between groups\n\n\n\n5.4.1 Standard error of the difference\nRemember standard deviation is a descriptive statistic - it measures the variance within our dataset - e.g. how closely do datapoints fit to the mean. However for inferential statistics we are more interested in our confidence in our estimation of the mean. This is where standard error comes in.\nWe can think of error as a standard deviation of the mean. The mean we have calculated is an estimate based on one sample of data. We would expect that if we sampled another 30 plants these would have a different sample mean. Standard error describes the variability we would expect among sample means if we repeated this experiment many times. So we can think of it as a measure of the confidence we have in our estimate of a true population mean.\n\\[\nSE = \\frac{s}{\\sqrt(n)}\n\\]\nAs sample size increases the standard error should reduce - reflecting an increasing confidence in our estimate.\nHere is a great explainer video on Standard Error\nAnd try this Shiny App on Sampling\nWe can calculate the standard error for our sample by applying this equation to our difference_summary object, can you complete this?\n\n\nSolution\n\n\ndifference_summary |&gt; \n  mutate(se= sd/sqrt(n))\n\n\n\n\nmean\nsd\nn\nse\n\n\n2.616667\n4.718047\n15\n1.218195\n\n\n\n\n\n\nOur estimate of the mean is not really very useful without an accompanying measuring of uncertainty like the standard error, in fact estimates of averages or differences should always be accompanied by their measure of uncertainty.\n\n5.4.2 Activity 3: Communicate\nWith the information above, how would you present a short sentence describing the average different in height?\n\n\nsolution\n\n… the average difference in height was 2.62 ± 1.22 inches (mean ± SE).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferential statistics</span>"
    ]
  },
  {
    "objectID": "08-inferential-statistics.html#uncertainty",
    "href": "08-inferential-statistics.html#uncertainty",
    "title": "5  Inferential statistics",
    "section": "\n5.5 Uncertainty",
    "text": "5.5 Uncertainty\nWith a mean and standard error of the difference in heights between inbred and crossed plants - how do we work out how much confidence we have in their being a difference between our population means?\nStandard error is a measure of uncertainty, the larger the standard error the more noise around our data and the more uncertainty we have. The smaller the standard error the more confidence we can have that our difference in means is real.\n\nNull hypothesis - there is no difference in the mean height of self vs crossed plants\nAlternate hypothesis - inbreeding reduces the fitness of the selfed plants, observed as selfed plants on average being smaller than crossed plants\n\n\n\n\n\n\n\nTip\n\n\n\nA statistical way of thinking about our inferences is in terms of confidence around keeping or rejecting the null hypothesis. The (alternate) hypothesis is simply one that contradicts the null.\nWhen we decide whether we have enough confidence that a difference is real (e.g. we could reject the null hypothesis), we cannot ever be 100% certain that this isn’t a false positive (also known as a Type I error). More on this later\n\n\n\n5.5.1 Normal distribution\nThe normal distribution is a way to describe data that forms a bell-shaped curve when plotted on a graph. Most of the data points are clustered around the middle, with fewer data points as you move farther from the center in either direction. The middle point is the average (or mean), and it represents the most common value in the data set.\nWhat is the normal distribution\nNow, standard deviation tells you how spread out the data points are from the mean. A small standard deviation means the data points are close to the mean, so the curve will be tall and narrow. A large standard deviation means the data points are more spread out, making the curve wider and flatter.\nIn short, the standard deviation measures how much variation there is in the data from the average value.\nLarge standard deviations produce wide bell curves, with short peaks. Small standard deviations produce narrow bell curves with tall peaks.\nThe bell curve occurs frequently in nature, most circumstances where we can think of a continuous measure coming from a population e.g. human mass, penguin flipper lengths or plant heights.\nAs a probability distribution, the area within the curve sums to the whole population (e.g. the probability that the curve contains every possible measurement is 1). Known proportions of the curve lie within certain distances from the centre e.g. 67.8% of observations should have values within one standard deviation of the mean. 95% of observations should have values within two standard deviations of the mean. This idealised normal distribution is presented below:\n\n\n\n\n\n\n\n\nHow do we convert this information into how likely we are to observe a difference of 2.62 inches in plant heights if the ‘true’ difference between crosses and selfed plants is zero?\nThe central limit theorem states that if you have a population with mean and standard deviation, and take sufficiently large random samples from the population, then the distribution of the sample means will be approximately normally distributed. Standard error then is our measure of variability around our sample mean, and we will assume that we can apply a normal distribution to our ability to estimate the mean.\nSo if we now center our bell curve on the estimate of the mean (2.62), then just over two thirds of the area under the curve is ± 1.22 inches. 95% of it will be within ± 2 standard errors, and 99.8% will be within ± 3 standard errors.\n\n\n\n\n\n\n\n\nTaking a look at this figure we can ask ourselves where is zero on our normal distribution? One way to think about this is, if the true difference between our plant groups is zero, how surprising is it that we estimated a difference between the groups of 2.62 inches?\nIf zero was close to the center of the bell curve, then our observed mean would not be surprising at all (if the null hypothesis is true). However in this case it is not in the middle of the bell. It falls in the left-hand tail, and it is &gt; than two standard deviations from our estimated mean.\nWe can describe this in two ways:\n\nWe estimate that if we ran this experiment 100 times then &gt;95 of our experiments would estimate a mean difference between our plant groups that is &gt; 0.\nThis is also usually taken as the minimum threshold needed to reject a null hypothesis. We can think of the probability of estimating this mean difference, if our true mean difference was zero, as p &lt; 0.05.\n\nYou will probably be very used to a threshold for null hypothesis rejection of \\(\\alpha\\) = 0.05, but this only the very lowest level of confidence at which we can pass a statistical test. If we increase the severity of our test so that the minimum we require is \\(\\alpha\\) = 0.001 or 99% confidence, we can see that we no longer believe we have enough confidence to reject the null hypothesis (0 is within 3 s.d. of our estimated mean).\n\n5.5.2 Confidence Intervals\nBecause ± 2 standard errors covers the central 95% of the area under the normal curve, we refer to this as our 95% confidence interval. We can calculate confidence intervals for any level, but commonly we refer to standard error (68% CI), 95% and 99%.\nWe can work out the 95% confidence interval range of our estimated mean as follows:\n\nlowerCI &lt;- 2.62-(2*1.22)\n\nupperCI &lt;- 2.62+(2*1.22)\n\nlowerCI\nupperCI\n\n[1] 0.18\n[1] 5.06\n\n\nA common mistake it to state that we are 95% confident that the ‘true’ mean lies within our interval. But technically it refers to the fact that if we kept running this experiment again and again the intervals we calculate would capture the true mean in 95% of experiments. So really we are saying that we are confident we would capture the true mean in 95% of our experiments.\nHow might we write this up?\n\n\n\n\n\n\nTip\n\n\n\nThe maize plants that have been cross pollinated were taller on average than the self-pollinated plants, with a mean difference in height of 2.62 [0.18, 5.06] inches (mean [95% CI]).\n\n\n\nNote that because it is possible to generate multiple types of average and confidence interval, we clearly state them in our write up. The same would be true if you were presenting the standard error (± S.E.) or the standard deviation (± S.D.) or a median and interquartile range (median [± IQR]).\n\nThe above is a good example of a simple but clear write-up because it clearly describes the direction of the difference, the amount and the error in our estimate.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferential statistics</span>"
    ]
  },
  {
    "objectID": "08-inferential-statistics.html#summary",
    "href": "08-inferential-statistics.html#summary",
    "title": "5  Inferential statistics",
    "section": "\n5.6 Summary",
    "text": "5.6 Summary\nStatistics is all about trying to interpret whether the signal (of a difference or trend) is stronger than the amount of noise (variability). In a sample the standard deviation is a strong choice for estimating this within a dataset. The standard deviation of the sampling distribution of the mean is known as the standard error. The standard error (of the mean) is a measure of the precision we have in our estimate of the mean. Thanks to the central limit theorem and normal distribution we can use the variability in our estimate to calculate confidence intervals, and decide whether our signal of an effect is strong enough to reject a null hypothesis (of no effect or no difference).\nNext we will start working with linear models - this approach allows us to estimate means and intervals in a more sophisticated (and automated) way.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Inferential statistics</span>"
    ]
  },
  {
    "objectID": "09-linear-models.html",
    "href": "09-linear-models.html",
    "title": "6  Linear models",
    "section": "",
    "text": "6.0.1 Packages\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(emmeans)\nlibrary(performance)\nlibrary(broom.helpers)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "09-linear-models.html#a-linear-model-analysis-for-calculating-means",
    "href": "09-linear-models.html#a-linear-model-analysis-for-calculating-means",
    "title": "6  Linear models",
    "section": "\n6.1 A linear model analysis for calculating means",
    "text": "6.1 A linear model analysis for calculating means\nR has a general function lm() for fitting linear models, this is part of base R (does not require the tidyverse packages). We will run through a few different iterations of the linear model increasing in complexity. We will often want to fit several models to our data, so a common way to work is to fit a model and assign it to a named R object, so that we can extract data from when we need it.\nIn the example below I have called the model lsmodel0, short for “least-squares model 0”, this is because the linear-model uses a technique called least squares.\n\nlsmodel0 &lt;- lm(formula = height ~ 1, data = darwin)\n\n\n\n\n\n\n\nTip\n\n\n\nYou can pipe into the lm() function, but when we use functions that are “outside” of the tidyverse family we need to put a _ where the data should go (as it is usually not the first argument).\n\nlsmodel0 &lt;- darwin |&gt; \n             lm(height ~ 1, data= _)\n\n\n\nThe first argument of the lm() function is formula (we won’t write this out in full in the future) - and this specifies we want to analyse a response variable (height) as a function of an explanatory variable using the tilde symbol (~).\nThe simplest possible model ignores any explanatory variables, instead the 1 indicates we just want to estimate an intercept.\nWithout explanatory variables this means the formula will just estimate the overall mean height of all the plants in the dataset.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "09-linear-models.html#summaries-for-models",
    "href": "09-linear-models.html#summaries-for-models",
    "title": "6  Linear models",
    "section": "\n6.2 Summaries for models",
    "text": "6.2 Summaries for models\nWhen you have made a linear model, we can investigate a summary of the model using the base R function summary(). There is also a tidyverse option provided by the package broom(Robinson et al. (2024)).\n\n6.2.1 Broom\nbroom summarizes key information about models in tidy tibble()s. broom provides three verbs to make it convenient to interact with model objects:\n\nbroom::tidy() summarizes information about model components\nbroom::glance() reports information about the entire model\nbroom::augment() adds informations about individual observations to a dataset and it can be used to model predictions onto a new dataset.\n\n6.2.2 Model summary\n\n\nBase R\nBroom\n\n\n\n\nsummary(lsmodel0)\n\n\nCall:\nlm(formula = height ~ 1, data = darwin)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.8833 -1.3521 -0.0083  2.4917  4.6167 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  18.8833     0.5808   32.52   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.181 on 29 degrees of freedom\n\n\n\n\n\nbroom::tidy(lsmodel0)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n(Intercept)\n18.88333\n0.5807599\n32.51487\n0\n\n\n\n\n\n\n\n\nThe output above is called the table of coefficients. The 18.9 is the estimate of the model coefficient (in this case it is the overall mean), together with its standard error (SEM). The first row in any R model output is always labelled the ‘Intercept’ and the challenge is usually to workout what that represents. In this case we can prove that this is the same as the overall mean as follows:\n\nmean(darwin$height)\n\n[1] 18.88333\n\n\nThis simple model allows us to understand what the lm() function does.\n\n6.2.3 Compare means\nWhat we really want is a linear model that analyses the difference in average plant height (type) as a function of pollination type. We can use the lm() function to fit this as a linear model as follows:\n\n\nBase R\nBroom\n\n\n\n\nlsmodel1 &lt;- lm(height ~ type, data=darwin)\n\n# note that the following is identical\n\n# lsmodel1 &lt;- lm(height ~ 1 + type, data=darwin)\n\nsummary(lsmodel1)\n\n\nCall:\nlm(formula = height ~ type, data = darwin)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.1917 -1.0729  0.8042  1.9021  3.3083 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.1917     0.7592  26.596   &lt;2e-16 ***\ntypeSelf     -2.6167     1.0737  -2.437   0.0214 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.94 on 28 degrees of freedom\nMultiple R-squared:  0.175, Adjusted R-squared:  0.1455 \nF-statistic:  5.94 on 1 and 28 DF,  p-value: 0.02141\n\n\n\n\n\nlsmodel1 &lt;- lm(height ~ type, data=darwin)\n\nbroom::tidy(lsmodel1)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n20.191667\n0.7592028\n26.595880\n0.0000000\n\n\ntypeSelf\n-2.616667\n1.0736749\n-2.437113\n0.0214145\n\n\n\n\n\n\n\n\n\nNow the model formula contains the pollination type in addition to an intercept.\n\n6.2.4 Coefficients\nThis is where the model results are reported.\n\nIntercept: This is the predicted height when the plant type is “Cross” fertilised, which is the reference category). The estimate is 20.1917, which means the average height for plants that are not self-fertilised is about 20.19 inches.\ntypeSelf: This coefficient tells us how the height of “Self” fertilized plants differs from the reference group (“Cross” fertilized plants). The estimate is -2.6167, meaning “Self” fertilized plants are, on average, 2.62 units shorter than “Cross” fertilized plants.\n\n\n\n\n\n\n\nNote\n\n\n\nYou can confirm this for yourself:\n\ndarwin |&gt; \n  group_by(type) |&gt; \n  summarise(mean=mean(height))\n\n\n\n\ntype\nmean\n\n\n\nCross\n20.19167\n\n\nSelf\n17.57500\n\n\n\n\n\n\n\n\n\nThe Std. Error for each coefficient measures the uncertainty in the estimate. Smaller values indicate more precise estimates. For example, the standard error of 1.0737 for the “typeSelf” coefficient suggests there’s some variability in how much shorter the “Self” fertilized plants are, but it’s reasonably precise.\nThe t value is the ratio of the estimate to its standard error (\\(\\frac{Mean}{Std. Error}\\)). The larger the t-value (either positive or negative), the more evidence there is that the estimate is different from zero.\nPr(&gt;|t|) gives the p-value, which tells us the probability of observing a result as extreme as this, assuming that there is no real effect (i.e., the null hypothesis is true). In this case, a p-value of 0.0214 means there’s about a 2% chance that the difference in height between “Self” and “Cross” fertilized plants is due to random chance. Since this is below the typical cutoff of 0.05, it suggests that the difference is statistically significant.\nSignificance codes: These symbols next to the p-value indicate how strong the evidence is. In this case, one star (*) indicates a p-value below 0.05, meaning the effect is considered statistically significant.\n\n\n\n\n\nAnnotation of the summary function output\n\n\n\n\n\nThe t-statistic\n\nThe t-statistic is closely related to the z-statistic but is used in significance testing when the sample size is small or the population standard deviation is unknown. As we almost never know the “population” standard deviation, it is always safer to use the t-distribution for significance tests.\n\n6.2.5 The t-Distribution:\nThe t-distribution is similar to the normal distribution but has more spread (wider tails), especially with smaller sample sizes. As the sample size increases, the t-distribution approaches the normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n6.2.6 A simple write up\nFrom this summary model we can make conclusions about the effect of inbreeding and produce a simple write up:\n“The maize plants that have been cross pollinated had an average height (mean ±S.E.) of 20.19 (± 0.76) inches and were taller on average than the self-pollinated plants, with a mean difference in height of 2.62 (±1.07) inches (t(28) = -2.44, p = 0.02)”",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "09-linear-models.html#confidence-intervals",
    "href": "09-linear-models.html#confidence-intervals",
    "title": "6  Linear models",
    "section": "\n6.3 Confidence intervals",
    "text": "6.3 Confidence intervals\nA confidence interval is a range of values that we expect to contain the true population parameter (like a mean or difference in means) with a certain level of confidence. In the case of a 95% confidence interval, this means we are 95% confident that the true parameter lies within the interval.\nThe confidence interval is closely tied to the standard error (SE), which measures how much variability we expect in the sample estimates due to sampling randomness. The standard error depends on the sample size and the variability in the data, and it’s computed as:\n\\[\nStandard~Error(SE)= \\frac{SD}{\\sqrt(n)}\n\\] For a 95% confidence interval, we use the t-distribution to determine how many standard errors away from the sample estimate we should go to capture the true population parameter with 95% confidence. In most cases:\n\\[\nConfidence~Interval=Mean(\\pm(criticial~t) \\times SE)\n\\] Where:\n\ncritical t = is the critical value from the t-distribution that corresponds to a 95% confidence level\nStandard Error - the precision of the sample estimate\n\n\n6.3.1 Confidence intervals and p-values\nA p-value and a confidence interval are inversely related, but they provide information in different ways:\n\nA p-value tells you how likely it is to observe your data (or something more extreme) if the null hypothesis is true. It’s a single number that quantifies the evidence against the null hypothesis. If the p-value is less than the significance level (typically 0.05), you reject the null hypothesis.\nA confidence interval provides a range of plausible values for the true parameter. If the confidence interval does not include 0 (or another hypothesized value, such as a difference of 0 between two groups), it suggests the parameter is significantly different from 0 at the corresponding confidence level (usually 95%).\n\nHere’s how they are inversely related:\n\np-value &lt; 0.05: If the p-value is below 0.05, this typically means the null hypothesis (no effect or no difference) is rejected, suggesting a statistically significant result. In this case, a 95% confidence interval for the effect will not include 0, indicating the result is statistically significant.\np-value &gt; 0.05: If the p-value is greater than 0.05, this means we do not reject the null hypothesis. In this scenario, the 95% confidence interval will likely include 0, indicating that the true effect could be zero and the result is not statistically significant.\n\n6.3.2 Confidence intervals (CI) in R\nWith a wrapper function around our model we can generate accurate 95% confidence intervals from the SE and calculated t-distribution:\n\n\nBase R\nBroom\n\n\n\n\nconfint(lsmodel1)\n\n               2.5 %     97.5 %\n(Intercept) 18.63651 21.7468231\ntypeSelf    -4.81599 -0.4173433\n\n\n\n\n\nbroom::tidy(lsmodel1, conf.int=T)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n20.191667\n0.7592028\n26.595880\n0.0000000\n18.63651\n21.7468231\n\n\ntypeSelf\n-2.616667\n1.0736749\n-2.437113\n0.0214145\n-4.81599\n-0.4173433\n\n\n\n\n\n\n\n\n\nBecause this follows the same layout as the table of coefficients, the output intercept row gives a 95% CI for the height of the crossed plants and the second row gives a 95% interval for the difference in height between crossed and selfed plants. The lower and upper bounds are the 2.5% and 97.5% of a t-distribution.\nIt is this difference in height in which we are specifically interested.\n\n6.3.3 Answering the question\nDarwin’s original hypothesis was that self-pollination would reduce fitness (using height as a proxy for this). The null hypothesis is that there is no effect of pollination type, and therefore no difference in the average heights.\nWe must ask ourselves if our experiment is consistent with this null hypothesis or can we reject it? If we choose to reject the null hypothesis, with what level of confidence can we do so?\nTo do this, we can simply determine whether or not the predicted value of our null hypothesis (a difference of zero) lies inside the 95% CI for the difference of the mean.\nIf our confidence intervals contain zero (or no difference), then we cannot establish a difference between our sample difference in height (-2.62 inches) from the null prediction of zero difference, given the level of variability (noise) in our data.\nIn this case we can see that the upper and lower bounds of the confidence intervals do not contain zero. The difference in height is consistent with Darwin’s alternate hypothesis of inbreeding depression.\n\n6.3.4 Plausible range of significant difference\n\nA 95% CI provides a range of values where the true effect (like a difference between two group means) is likely to fall. Our analysis suggests that within the 95% confidence interval the true (population) difference could be between 0.42 and 4.8 inches in height difference between crossed and selfed plants.\n\nMinimum effect size: the value closest to zero gives a sense of the minimum effect size that is plausible at 95% confidence. This is important because it shows the smallest effect you might expect. Here the minimum effect size is 0.42 inches. Or that the cost of inbreeding could be as little as a 0.42 inch height difference.\n\nThe GGally package has a handy ggcoef_model() function, that produces a graph of the estimated mean difference with an approx 95% CI. As we can see we are able to reject the null hypothesis at a 95% confidence level.\n\n# Generate a coefficient plot for the linear regression model using ggcoef_model\nGGally::ggcoef_model(lsmodel1,\n                     show_p_values = FALSE,\n                     conf.level = 0.95)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSet the confidence levels to 99%, do you think the difference between treatments is still statistically significant at an of 0.01?\n\n\nIf we increase the level of confidence (from 95% to 99%, roughly 2 SE to 3 SE), then we may find that we cannot reject the null hypothesis at a higher threshold of confidence (p &lt; 0.01). Try altering the conf.level argument above for yourself to see this in action.\nWe can also include this argument in the tidy() function if we wish to:\n\nbroom::tidy(lsmodel1, conf.int=T, conf.level=0.99)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n20.191667\n0.7592028\n26.595880\n0.0000000\n18.093790\n22.2895433\n\n\ntypeSelf\n-2.616667\n1.0736749\n-2.437113\n0.0214145\n-5.583512\n0.3501789",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "09-linear-models.html#estimating-means",
    "href": "09-linear-models.html#estimating-means",
    "title": "6  Linear models",
    "section": "\n6.4 Estimating means",
    "text": "6.4 Estimating means\nOne limitation of the table of coefficients output is that it doesn’t provide the mean and standard error of the other treatment level (only the difference between them). If we wish to calculate the “other” mean and SE then we can get R to do this.\n\n6.4.1 Changing the intercept\nOne way to do this is to change the levels of the type variable as a factor:\n\n# Perform linear regression on darwin data with Self as the intercept\ndarwin |&gt; \n  # Convert 'type' column to a factor\n  mutate(type = factor(type)) |&gt;\n  # Relevel 'type' column to specify the order of factor levels\n  mutate(type = fct_relevel(type, c(\"Self\", \"Cross\"))) |&gt;\n  # Fit linear regression model with 'height' as the response variable and 'type' as the predictor\n  lm(height ~ type, data = _) |&gt;\n  # Tidy the model summary\n  broom::tidy()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n17.575000\n0.7592028\n23.149282\n0.0000000\n\n\ntypeCross\n2.616667\n1.0736749\n2.437113\n0.0214145\n\n\n\n\n\n\nAfter releveling, the self treatment is now taken as the intercept, and we get the estimate for it’s mean and standard error\n\n6.4.2 Emmeans\nWe could also use the package emmeans and its function emmeans() to do a similar thing\n\n# Calculate estimated marginal means (EMMs) using emmeans package\nmeans &lt;- emmeans::emmeans(lsmodel1, specs = ~ type)\n\nmeans\n\n type  emmean    SE df lower.CL upper.CL\n Cross   20.2 0.759 28     18.6     21.7\n Self    17.6 0.759 28     16.0     19.1\n\nConfidence level used: 0.95 \n\n\nThe advantage of emmeans is that it provides the mean, standard error and 95% confidence interval estimates of all levels from the model at once (e.g. it relevels the model multiple times behind the scenes).\nemmeans also gives us a handy summary to include in data visuals that combine raw data and statistical inferences. These are standard ggplot() outputs so can be customised as much as you want.\n\n# Convert the 'means' object to a tibble\nmeans |&gt;\n  as_tibble() |&gt;\n  # Create a plot using ggplot\n  ggplot(aes(x = type, y = emmean)) +\n  # Add point estimates with error bars\n  geom_pointrange(aes(ymin = lower.CL, ymax = upper.CL))\n\n\n\n\n\n\n\nNotice that no matter how we calculate the estimated SE (and therefore the 95% CI) of both treatments is the same. This is because as mentioned earlier the variance is a pooled estimate, e.g. variance is not being calculate separately for each group. The only difference you should see in SE across treatments will be if there is a difference in sample size between groups.\n\n\n\n\n\n\nTip\n\n\n\nNotice how the Confidence Intervals of the estimated means strongly overlap, there is a difference between the two SEMs and the SED we have calculated. So overlapping error bars cannot be used to infer significance.\n\n\nBecause of this pooled variance, there is an assumption that variance is equal across the groups, this and other assumptions of the linear model should be checked. We cannot trust our results if the assumptions of the model are not adequately met.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "09-linear-models.html#summary",
    "href": "09-linear-models.html#summary",
    "title": "6  Linear models",
    "section": "\n6.5 Summary",
    "text": "6.5 Summary\nSo remember a linear model sets one factor level as the ‘intercept’ estimates its mean, then draws a line from the first treatment to the second treatment, the slope of the line is the difference in means between the two treatments.\nThe difference in means is always accompanied by a standard error of the difference (SED), and this can be used to calculate a 95% confidence interval. If this confidence interval does not contain the intercept value, we can reject the null hypothesis that there is ‘no effect’.\nLinear models make a variety of assumptions, including that the noise (residual differences) are approximately normally distributed, with roughly equal (homogenous) variance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "09-linear-models.html#write-up",
    "href": "09-linear-models.html#write-up",
    "title": "6  Linear models",
    "section": "\n6.6 Write-up",
    "text": "6.6 Write-up\nCan you write an Analysis section? Add calculated 95% confidence intervals on top of the summary we produced earlier\n\n\nSolution\n\nThe maize plants that have been cross pollinated had an average height of 20.19 inches [18.63 - 21.74] and were taller on average than the self-pollinated plants, with a mean difference in height of 2.62 [0.42, 4.82] inches (mean [95% CI]) (t(28) = -2.44, p = 0.02).\n\n# Convert the 'means' object to a tibble\nmeans |&gt;\n  as_tibble() |&gt;\n  # Create a plot using ggplot\n  ggplot(aes(x = type, y = emmean, fill = type)) +\n    # Add raw data\n  geom_jitter(data = darwin,\n              aes(x = type,\n                  y = height),\n              width = 0.1,\n              pch = 21,\n              alpha = 0.4) +\n  # Add point estimates with 95% confidence error bars\n  geom_pointrange(aes(ymin = lower.CL, \n                      ymax = upper.CL),\n                  pch = 21) +\n  theme_classic()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis will be different to your previous manual calculations on two counts. One, we are using a t-distribution for our confidence intervals. Two this example is a two-sample t-test, our previous example was closer to a paired t-test we will see how to implement a linear model with a paired design in subsequent chapters.\n\n\n\n\n\n\nRobinson, D., Hayes, A., & Couch, S. (2024). Broom: Convert statistical objects into tidy tibbles. https://broom.tidymodels.org/",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "r-basics.html",
    "href": "r-basics.html",
    "title": "Appendix A — R Basics",
    "section": "",
    "text": "A.1 Using Posit cloud\nAll of our sessions will run on cloud-based software. All you have to do is make a free account, and join our Workspace.\nOnce you are signed up - you will see that there are two spaces:\nMake sure you are working in the classroom workspace - so that I can distribute project work and ‘visit’ your projects if needed.\nPosit cloud works in exactly the same way as RStudio, but means you don’t have to download any software. You can access the hosted cloud server and your projects through any browser connection (Chrome works best), from any computer.\nHere is a good reference guide to Posit cloud",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#using-posit-cloud",
    "href": "r-basics.html#using-posit-cloud",
    "title": "Appendix A — R Basics",
    "section": "",
    "text": "Your workspace - for personal use (20hrs/month)\nOur shared classroom - educational licence (no limit)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#getting-to-know-rstudio",
    "href": "r-basics.html#getting-to-know-rstudio",
    "title": "Appendix A — R Basics",
    "section": "\nA.2 Getting to know RStudio",
    "text": "A.2 Getting to know RStudio\nR Studio has a console that you can try out code in (appearing as the bottom left window), there is a script editor (top left), a window showing functions and objects you have created in the “Environment” tab (top right window in the figure), and a window that shows plots, files packages, and help documentation (bottom right).\n\n\n\n\nRStudio interface\n\n\n\nYou will learn more about how to use the features included in R Studio throughout this course, however, I highly recommend watching RStudio Essentials 1 at some point.\nThe video lasts ~30 minutes and gives a tour of the main parts of R Studio.\n\nA.2.1 Consoles vs. scripts\n\nThe script window is the place to enter and run code so that it is easily edited and saved for future use. Usually the Script Window is shown at the top left in RStudio. If this window is not shown, it will be visible if you open a previously saved R script, or if you create a new R Script. You create new R Script by clicking on File &gt; New File &gt; R Script in the RStudio menu bar.\nTo execute your code in the R script, you can either highlight the code and click on Run, or you can highlight the code and press CTRL + Enter on your keyboard.\nThe console: you can enter code directly in the Console Window and click Enter. The commands that you run will be shown in the History Window on the top right of RStudio. Though it is much more difficult to keep track of your work this way.\n\nA.2.2 Environment\nThe Environment tab (top right) allows you to see what objects are in the workspace. If you create variables or data frames, you have a visual listing of everything in the current workspace. When you start a new project this should be completely empty.\n\nA.2.3 Plots, files, packages, help\n\nPlots - The Plots panel, shows all your plots. There are buttons for opening the plot in a separate window and exporting the plot as a pdf or jpeg (though you can also do this with code.)\nFiles - The files panel gives you access to the file directory on your hard drive.\nPackages - Shows a list of all the R packages installed on your harddrive and indicates whether or not they are currently loaded. Packages that are loaded in the current session are checked while those that are installed but not yet loaded are unchecked. We will discuss packages more later.\nHelp - Help menu for R functions. You can either type the name of a function in the search window, or use the code to search for a function with the name\n\n\n\n\n\nRStudio interface labelled\n\n\n\n\nA.2.4 Make RStudio your own\nYou can personalise the RStudio GUI as much as you like.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#get-help",
    "href": "r-basics.html#get-help",
    "title": "Appendix A — R Basics",
    "section": "\nA.3 Get Help!",
    "text": "A.3 Get Help!\nThere are a lot of sources of information about using R out there. Here are a few helpful places to get help when you have an issue, or just to learn more\n\nThe R help system itself - type help() and put the name of the package or function you are querying inside the brackets\nVignettes - type browseVignettes() into the console and hit Enter, a list of available vignettes for all the packages we have will be displayed\nCheat Sheets - available at RStudio.com. Most common packages have an associate cheat sheet covering the basics of how to use them. Download/bookmark ones we will use commonly such as ggplot2, data transformation with dplyr, Data tidying with tidyr & Data import.\nGoogle - I use Google constantly, because I continually forget how to do even basic tasks. If I want to remind myself how to round a number, I might type something like R round number - if I am using a particular package I should include that in the search term as well\nAsk for help - If you are stuck, getting an error message, can’t think what to do next, then ask someone. It could be me, it could be a classmate. When you do this it is very important that you show the code, include the error message. “This doesn’t work” is not helpful. “Here is my code, this is the data I am using, I want it to do X, and here’s the problem I get.”\n\n\n\n\nIt may be daunting to send your code to someone for help.\n\n\nIt is natural and common to feel apprehensive, or to think that your code is really bad. I still feel the same! But we learn when we share our mistakes, and eventually you will find it funny when you look back on your early mistakes, or laugh about the mistakes you still occasionally make!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#r",
    "href": "r-basics.html#r",
    "title": "Appendix A — R Basics",
    "section": "\nA.4 R",
    "text": "A.4 R\nGo to Posit cloud and enter the Project labelled Day One - this will clone the project and provide you with your own project workspace.\nFollow the instructions below to get used to the R command line, and how R works as a language.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#your-first-r-command",
    "href": "r-basics.html#your-first-r-command",
    "title": "Appendix A — R Basics",
    "section": "\nA.5 Your first R command",
    "text": "A.5 Your first R command\nIn the RStudio pane, navigate to the console (bottom left) and type or copy the below it should appear at the &gt;\nHit Enter on your keyboard.\n\n10 + 20\n\n\nWhat answer did you get?\n\n\n\nSolution\n\n\n30\n\n\nThe first line shows the request you made to R, the next line is R’s response\nYou didn’t type the &gt; symbol: that’s just the R command prompt and isn’t part of the actual command.\nWhen a complete expression is entered at the prompt, it is evaluated and the result of the evaluated expression is returned. The result may be auto-printed.\n\nprint(10 + 20) ## explicit printing\n\n10 + 20 ## autoprinting\n\nUsually, with interactive work, we do not explicitly print objects with the print function; it is much easier to auto-print them by typing the name of the object and hitting return/enter. However, when writing scripts, functions, or more extended programs, there is sometimes a need to explicitly print objects.\nWhen an R vector is printed, you will notice that an index for the vector is printed in square brackets [] on the side. For example, see this integer sequence\n\n1:30\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30\n\n\nThe numbers in the square brackets are not part of the vector itself; they are merely part of the printed output.\n\nNote that the : operator is used to create integer sequences\n\n\nA.5.1 Operators\nThere are a few different types of operators to consider in R\n\nA.5.1.1 Assignment Operator\n\n\nOperator\nDescription\n\n\n&lt;-\nused to assign values to variables\n\n\nA.5.1.2 Arithmetic Operators\n\n\nOperator\nDescription\n\n\n\n+\naddition\n\n\n-\nsubtraction\n\n\n*\nmultiplication\n\n\n/\ndivision\n\n\n^\nexponentiation\n\n\n\nA.5.1.3 Relational Operators\n\n\nOperator\nDescription\n\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to\n\n\n\nA.5.1.4 Logical Operators\n\n\nOperator\nDescription\n\n\n\n!\nnot\n\n\n&\nAND\n\n\n⎮\nOR\n\n\n\nA.5.1.5 Membership Operators\n\n\nOperator\nDescription\n\n\n%in%\nused to check if an element is in a vector or list\n\n\nA.5.2 Typos\n\n\n\nBefore we go on to talk about other types of calculations that we can do with R, there’s a few other things I want to point out. The first thing is that, while R is good software, it’s still software. It’s pretty stupid, and because it’s stupid it can’t handle typos. It takes it on faith that you meant to type exactly what you did type.\n\n\n\nSuppose you forget to hit the shift key when trying to type +, and as a result your command ended up being 10 = 20 rather than 10 + 20. Try it for yourself and replicate this error message:\n\n10 = 20\n\n\n\nWhat answer did you get?\n\n\nError in 10 = 20 : invalid (do_set) left-hand side to assignment\n\n\nWhat’s going on: R tries to interpret 10 = 20 as a command, but it doesn’t make sense, so it gives you an error message.\nWhen a person sees this, they might realize it’s a typo because the + and = keys are right next to each other on the keyboard. But R doesn’t have that insight, so it just gets confused.\nWhat’s even trickier is that some typos won’t create errors because they accidentally form valid R commands. For example, if I meant to type 10 + 20 but mistakenly pressed a neighboring key, I’d end up with 10 - 20. Now, R can’t read your mind to know you wanted to add, not subtract, so something different happens:\n\n10 - 20\n\n[1] -10\n\n\nIn this case, R produces the right answer, but to the the wrong question.\n\nA.5.3 More simple arithmetic\nOne of the best ways to get familiar with R is to experiment with it. The good news is that it’s quite hard to mess things up, so don’t stress too much. Just type whatever you like into the console and see what happens.\nNow, if your console’s last line looks like this:\n&gt; 10+\n+ \nAnd there’s a blinking cursor next to that plus sign, it means R is patiently waiting for you to complete your command. It believes you’re still typing, so it hasn’t tried to run anything yet. This plus sign is a bit different from the usual prompt (the &gt; symbol). It’s there to nudge you that R is ready to “add” what you’re typing now to what you typed before. For example, type 20 and hit enter, and then R will complete the command like this:\n&gt; 10 +\n+ 20\n[1] 30\nAlternatively hit the escape key, and R will forget what you were trying to do and return to a blank line.\n\nA.5.4 Try some simple maths\n\n1+7\n\n\n13-10\n\n\n4*6\n\n\n12/3\n\nRaise a number to the power of another\n\n5^4\n\nMultiplying a number \\(x\\) by itself \\(n\\) times is called “raising \\(x\\) to the \\(n\\)-th power”. Mathematically, this is written as \\(x^n\\). Some values of \\(n\\) have special names: in particular \\(x^2\\) is called \\(x\\)-squared, and \\(x^3\\) is called \\(x\\)-cubed. So, the 4th power of 5 is calculated like this:\n\\[5^4 = 5 \\times 5 \\times 5 \\times 5 \\]\n\nA.5.5 Perform some combos\nR follows the standard order of operations (BODMAS/BIDMAS), which means it first calculates within brackets, then deals with exponents, followed by division and multiplication, and finally addition and subtraction.\nLet’s look at two examples to see how the order of operations affects the results:\n\n3^2-5/2\n\n\n(3^2-5)/2\n\nSimilarly if we want to raise a number to a fraction, we need to surround the fraction with parentheses ()\n\n16^1/2\n\n\n16^(1/2)\n\nThe first one calculates 16 raised to the power of 1, then divided this answer by two. The second one raises 16 to the power of a half. A big difference in the output.\n\n\n\nWhile the cursor is in the console, you can press the up arrow to see all your previous commands.\n\n\nYou can run them again, or edit them. Later on we will look at scripts, as an essential way to re-use, store and edit commands.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#true-or-false-data",
    "href": "r-basics.html#true-or-false-data",
    "title": "Appendix A — R Basics",
    "section": "\nA.6 “TRUE or FALSE” data",
    "text": "A.6 “TRUE or FALSE” data\nTime to make a sidebar onto another kind of data. Many concepts in programming rely on the idea of a logical value. A logical value is an assertion about whether something is true or false. This is implemented in R in a pretty straightforward way. There are two logical values, namely TRUE and FALSE. Despite the simplicity, logical values are very useful things. Let’s see how they work.\n\nA.6.1 Assessing mathematical truths\nTime to explore a different kind of data. In programming, many concepts rely on logical values. A logical value is a statement about whether something is true or false. In R, this is pretty straightforward. There are two logical values: TRUE and FALSE. Despite their simplicity, these logical values are incredibly useful. Let’s dive into how they work.\nIn R, basic mathematics is solid, and there’s no room for manipulation. When you ask R to calculate 2 + 2, it always provides the same answer,\n\n2 + 2\n\n[1] 4\n\n\nup to this point, R has been performing calculations without explicitly asserting whether \\(2 + 2 = 4\\) is a true statement. If I want R to make an explicit judgment, I can use a command like this:\n\n2 + 2 == 4\n\n\n\nSolution\n\n\nTRUE\n\n\nWhat I’ve done here is use the equality operator, ==, to force R to make a “true or false” judgement.\n\n\n\nThis is a very different operator to the assignment operator = you saw previously.\n\n\nA common typo that people make when trying to write logical commands in R (or other languages, since the “= versus ==” distinction is important in most programming languages) is to accidentally type = when you really mean ==.\n\n\n\nOkay, let’s see what R thinks of 2 +2 ==5:\n\n2+2 == 5\n\n[1] FALSE\n\n\nNow, let’s see what happens when I attempt to make R believe that two plus two equals five by using an assignment statement like 2 + 2 = 5 or 2 + 2 &lt;- 5. Here’s the outcome:\n\n2 + 2 = 5\n\nError in 2 + 2 = 5 : target of assignment expands to non-language object\nIndeed, R isn’t too fond of this idea. It quickly realizes that 2 + 2 is not a variable (that’s what the “non-language object” part is saying), and it refuses to let you “reassign” it. While R can be quite flexible and allows you to do some remarkable things to redefine parts of itself, there are fundamental truths it simply won’t budge on. It won’t tamper with the laws of addition, and it won’t redefine the number 2.\nThat’s probably for the best.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#storing-outputs",
    "href": "r-basics.html#storing-outputs",
    "title": "Appendix A — R Basics",
    "section": "\nA.7 Storing outputs",
    "text": "A.7 Storing outputs\nWhen dealing with more complex questions, it’s often helpful to store our answers and use them in later steps. Fortunately, this is quite easy to do in R. We can assign the results to a name with the assignment operator:\n\na &lt;- 1+2\n\nThis literally means please assign the value of 1+2 to the name a. We use the assignment operator &lt;- to make this assignment.\n\n\n\nNote the shortcut key for &lt;- is Alt + - (Windows) or Option + - (Mac)\n\n\n\nBy performing this action, you’ll achieve two things:\nYou will notice in the top right-hand pane within the Environment tab that there is now an object labeled a with a value of 3.\n\n\n\n\nobject a is now visible withe a value of 3 in the Environment Pane\n\n\n\n\nYou can check what the variable a contains by typing it into your Console and pressing Enter.\nKeep in mind that you won’t see the result of your operations until you type the object into the R console and press Enter.\n\n\na  ## autoprinting\n\nprint(a) ## explicit printing\n\n\n\nWhat output do you get when you type a into your console?\n\n\n3\n\n\nYou can now call this object at any time during your R session and perform calculations with it.\n\n2 * a\n\n\n\nSolution\n\n\n6\n\n\nWhat happens if we assign a value to a named object that already exists in our R environment??? for example\n\na &lt;- 10\na\n\nThe value of a is now 10.\nYou should see that the previous assignment is lost, gone forever and has been replaced by the new value.\nWe can assign lots of things to objects, and use them in calculations to build more objects.\n\nb &lt;- 5\nc &lt;- a + b\n\n\n\n\nRemember: If you now change the value of b, the value of c does not change.\n\n\nObjects are totally independent from each other once they are made.\n\n\nOverwriting objects with new values means the old value is lost.\n\n\n\n\nb &lt;- 7\nb\nc\n\n\nWhat is the value of c?\n\n\n\nWhat is the value of c ?\n\n\n[1] 15\n\nWhen c was created it was a product of a and b having values of 10 and 15 respectively. If we re-ran the command c &lt;- a + b after changing the value of b then we would get a value of 17.\n\nLook at the environment tab again - you should see it’s starting to fill up now!\n\n\n\nRStudio will by default save the objects in its memory when you close a session.\n\n\nThese will then be there the next time you logon. It might seem nice to be able to close things down and pick up where you left off, but its actually quite dangerous. It’s messy, and can cause lots of problems when we work with scripts later, so don’t do this!\n\n\nTo stop RStudio from saving objects by default go to Tools &gt; Project Options option and change “Save workspace to .RData on exit” to “No” or “Never”.\n\n\nInstead we are going to learn how to use scripts to quickly re-run analyses we have been working on.\n\n\n\n\nA.7.1 Choosing names\n\nUse informative variable names. As a general rule, using meaningful names like orange and apple is preferred over arbitrary ones like variable1 and variable2. Otherwise it’s very hard to remember what the contents of different variables actually are.\nUse short variable names. Typing is a pain and no-one likes doing it. So we much prefer to use a name like apple over a name like pink_lady_apple.\nUse one of the conventional naming styles for multi-word variable names. R only lets you use certain things as legal names. Legal names must start with a letter not a number, which can then be followed by a sequence of letters, numbers, ., or _. R does not like using spaces. Upper and lower case names are allowed, but R is case sensitive so Apple and apple are different.\nMy favourite naming convention is snake_case short, lower case only, spaces between words are separated with a _. It’s easy to read and easy to remember.\n\n\n\n\n\ncourtesy of Allison Horst",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#r-objects",
    "href": "r-basics.html#r-objects",
    "title": "Appendix A — R Basics",
    "section": "\nA.8 R objects",
    "text": "A.8 R objects\nIn R, there are five fundamental or “atomic” classes of objects:\n\nCharacter: These represent text or character strings.\nNumeric (num) or Double (dbl): These are used for real numbers (e.g., decimal numbers).\nInteger: Used for whole numbers.\nComplex: For complex numbers.\nLogical: Represented as True or False, these are used for logical values.\n\nThe most basic type of R object is a vector. You can create empty vectors using the vector() function. The primary rule regarding vectors in R is that a vector can only contain objects of the same class.\nHowever, as with any good rule, there’s an exception, which is the “list.” Lists are represented as vectors but can hold objects of different classes, which is why they’re often used.\n\nA.8.1 Numbers\nIn R, both “dbl” and “num” refer to numeric data types, but there is a subtle difference between them:\n\ndbl (“double”): This refers to double-precision floating-point numbers, which are capable of storing real numbers with high precision. Double-precision numbers have more decimal places of accuracy and can represent a wider range of values without loss of precision. When you perform arithmetic operations, R typically returns results as “dbl” values by default.\nnum (“numeric”): “Num” is a more general term that includes not only double-precision floating-point numbers but also integer values. In R, integers are a subtype of numeric data. Numeric data can include both integers and double-precision floating-point numbers, depending on the specific data and how it is represented.\n\nSo, “dbl” specifically denotes double-precision floating-point numbers, while “num” encompasses a broader range of numeric data, including both integers and double-precision numbers. In most cases, you can use “num” to work with numeric data in a more general sense, while “dbl” focuses on the higher-precision representation of real numbers.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#attributes",
    "href": "r-basics.html#attributes",
    "title": "Appendix A — R Basics",
    "section": "\nA.9 Attributes",
    "text": "A.9 Attributes\nR objects can come with attributes, which are essentially metadata for the object. These metadata are handy because they help describe the object. For instance, in a data frame, column names serve as attributes, clarifying the data contained in each column. Here are a few examples of R object attributes:\n\nnames() and dimnames()\ndimensions (e.g., for matrices and arrays) dim()\nclass() (e.g., integer, numeric)\nlength()\nOther user-defined attributes or metadata\n\nYou can access the attributes of an object, if it has any, by using the attributes() function. If an R object doesn’t have any attributes, the attributes() function will return NULL.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#vectors",
    "href": "r-basics.html#vectors",
    "title": "Appendix A — R Basics",
    "section": "\nA.10 Vectors",
    "text": "A.10 Vectors\nWe have been working with R objects containing a single element of data (the technical term is scalar), but we will more commonly work with vectors. A vector is a sequence of elements, all of the same data type. These could be logical, numerical, character etc.\n\nnumeric_vector &lt;- c(1,2,3)\n\ncharacter_vector &lt;- c(\"fruits\", \"vegetables\", \"seeds\")\n\nlogical_vector &lt;- c(TRUE, TRUE, FALSE)\n\ninteger_vector &lt;- 1:10\n\n\nA.10.1 Coercion\nIn R, when different classes of objects are mixed together in a vector, coercion occurs to ensure that every element in the vector belongs to the same class. Coercion is the process of converting objects to a common class to make the combination reasonable. Let’s see the effects of implicit coercion in the provided examples:\n\ny &lt;- c(2.3, \"a\") # Here, we're mixing a numeric value (1.7) with a character value (\"a\"). To make them compatible, R coerces both elements into character values. So, y becomes a character vector.\n\ny &lt;- c(TRUE, 2) # In this case, we're combining a logical value (TRUE) with a numeric value (2). R coerces the logical value into 1, so y becomes a numeric vector.\n\ny &lt;- c(\"a\", TRUE) # We're mixing a character value (\"a\") with a logical value (TRUE). In this scenario, R coerces the logical value into a character value, resulting in y becoming a character vector.\n\nSo, the outcome depends on how R can reasonably represent all the objects in the vector. It aims to create a vector of the most inclusive class to accommodate the mixed objects. Keep in mind that this coercion can lead to unexpected results, so it’s essential to be aware of the implicit type conversion when mixing different data types in R.\nObjects can also be explicitly coerced from one class to another using the as.* functions, if available.\n\nA.10.2 Task\nCreate the following vector and check its class, then note what happens when you attempt to coerce to numeric, logical and character\n\nx &lt;- 0:5\n\n\n\nSolution\n\n\nas.numeric(x)\n\nas.logical(x)\n\nas.character(x)\n\n[1] 0 1 2 3 4 5\n[1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n[1] \"0\" \"1\" \"2\" \"3\" \"4\" \"5\"\n\n\nr unhide()\nSometimes, R can’t figure out how to coerce an object and this can result in NAs being produced\n\nA.10.3 Subsetting vectors\nWith numerical indexing, you enter a vector of integers corresponding to the values in the vector you want to access in the form a[index], where a is the vector, and index is a vector of index values. For example, let’s use numerical indexing to get values from our character_vector\n\ncharacter_vector[2]\n# [1] \"vegetables\"\n\n\ncharacter_vector[1:2]\n# [1] \"fruits\"     \"vegetables\"\n\ncharacter_vector[c(1,3)]\n# [1] \"fruits\" \"seeds\" \n\nWe can also use logical indexing\n\nnumeric_vector &lt;=2\n# [1]  TRUE  TRUE FALSE\n\ncharacter_vector == \"fruits\"\n#[1]  TRUE FALSE FALSE\n\n\nA.10.4 Operations on vectors\nWe can run the same basic operations on vectors as we did on scalars\n\nx &lt;- c(1,2,3)\ny &lt;- c(2,3,4)\n\n# Operations will happen between vectors\nx*y\n\n[1]  2  6 12\n\n\nA very super-wickedly, important, concept: R likes to operate on vectors of the same length, so if it encounters two vectors of different lengths in a binary operation, it merely replicates (recycles) the smaller vector until it is the same length as the longest vector, then it does the operation.\n\nx &lt;- c(1,2,3)\ny &lt;- c(1,2)\n\n# Operations will happen between vectors\nx*y\n\n[1] 1 4 3\n\nWarning: longer object length is not a multiple of shorter object length[1] 1 4 3\n\n\nA.11 Matrices\nMatrices can be thought of as vectors with an added dimension attribute. This dimension attribute is a two-element integer vector specifying the number of rows and columns, which defines the shape and structure of the matrix.\n\n\n\nData frames are also two-dimensional but can store columns of different data types - matrices are simpler as they consist of elements of the same data type.\n\n\n\nMatrices are constructed “columns-first” so entries start in the “upper left” and and run down columns.\n\nm &lt;- matrix(1:6, nrow = 2, ncol = 3) \nm\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\nattributes(m)\n\n$dim\n[1] 2 3\n\n\nWe can create matrices in several ways:\n\nAdding a dim() to existing vectors\nColumn/row-binding vectors with cbind() and rbind()\n\n\nm &lt;- 1:6\n\ndim(m) &lt;- c(2,3)\n\nm\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\na &lt;- 1:2\nb &lt;- 3:4\nc &lt;- 5:6\n\nm &lt;- cbind(a,b,c)\nm\n\n     a b c\n[1,] 1 3 5\n[2,] 2 4 6\n\n\nYou will see how in this last operation column names were added to the matrix, we can add, change or remove column and rownames on a matrix with colnames() and rownames()\n\nrownames(m) &lt;- c(\"y\",\"z\")\nm\n\n  a b c\ny 1 3 5\nz 2 4 6\n\n\n\nA.12 Lists\nLists are a versatile and fundamental data type in R. They set themselves apart from regular vectors by allowing you to store elements of different classes within the same list. This flexibility is what makes lists so powerful for various data structures and data manipulation tasks.\nYou can create lists explicitly using the list() function, which can take an arbitrary number of arguments. Lists, when combined with functions like the “apply” family, enable you to perform complex and versatile data manipulations and analyses in R. Lists are often used to represent heterogeneous data structures, such as datasets where different columns can have different data types and structures.\n\nl &lt;- list(1, \"apple\", TRUE )\nl\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"apple\"\n\n[[3]]\n[1] TRUE\n\n\nWe can also create empty lists of set lengths with the vector() function, this can be useful for preallocating memory for iterations - as we will see later\n\nl &lt;- vector(\"list\", length = 3)\nl\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n\nLists can also have names\n\nnames(l) &lt;- c(\"apple\",\"orange\",\"pear\")\n\n\nA.13 Dataframes\nData frames are essential for storing tabular data in R and find extensive use in various statistical modeling and data analysis applications. They offer a structured way to manage and work with data in R, and packages like dplyr, developed by Hadley Wickham, provide optimized functions for efficient data manipulation with data frames.\nHere are some key characteristics and advantages of data frames:\n\nTabular Structure: Data frames are a type of list, where each element in the list represents a column. The number of rows in each column is the same, and this tabular structure makes them suitable for working with datasets.\nMixed Data Types: Unlike matrices, data frames can contain columns with different classes of objects. This flexibility allows you to handle real-world datasets that often include variables of different data types.\nColumn and Row Names: Data frames include column names, which describe the variables or predictors. Additionally, they have a special attribute called “row.names” that provides information about each row in the data frame.\nCreation and Conversion: Data frames can be created in various ways, such as reading data from files using functions like read.table() and read.csv(). You can also create data frames explicitly with data.frame().\nWorking with Data: Data frames are especially useful when working with datasets that require data cleaning, transformation, or merging. They provide a high level of data organization, and many R packages are designed to work seamlessly with data frames.\ndplyr: The dplyr package is optimized for efficient data manipulation with data frames. It offers a set of functions to perform data operations quickly and intuitively.\n\nData frames are a fundamental structure for managing tabular data in R. They excel in handling datasets with mixed data types and are essential for various data analysis and modeling tasks.\nTo create a dataframe from vectors we use the data.frame() function\n\nsurvey &lt;- data.frame(\"index\" = c(1, 2, 3, 4, 5),\n                     \"sex\" = c(\"m\", \"m\", \"m\", \"f\", \"f\"),\n                     \"age\" = c(99, 46, 23, 54, 23))\n\nThere is one key argument to data.frame() and similar functions called stringsAsFactors. By default, the data.frame() function will automatically convert any string columns to a specific type of object called a factor in R. A factor is a nominal variable that has a well-specified possible set of values that it can take on. For example, one can create a factor sex that can only take on the values “male” and “female”.\n\n\n\nSince R ver 4.0 release, stringsAsFactors is set FALSE by default!\n\n\n\nHowever, as I’m sure you’ll discover, having R automatically convert your string data to factors can lead to lots of strange results. For example: if you have a factor of sex data, but then you want to add a new value called other, R will yell at you and return an error. I hate, hate, HATE when this happens. While there are very, very rare cases when I find factors useful, I almost always don’t want or need them. For this reason, I avoid them at all costs.\nTo tell R to not convert your string columns to factors, you need to include the argument stringsAsFactors = FALSE when using functions such as data.frame()\n\nstr(survey)\n\n'data.frame':   5 obs. of  3 variables:\n $ index: num  1 2 3 4 5\n $ sex  : chr  \"m\" \"m\" \"m\" \"f\" ...\n $ age  : num  99 46 23 54 23\n\n\nTo access a specific column in a dataframe by name, you use the $ operator in the form df$name where df is the name of the dataframe, and name is the name of the column you are interested in. This operation will then return the column you want as a vector.\n\nsurvey$sex\n\n[1] \"m\" \"m\" \"m\" \"f\" \"f\"\n\n\nBecause the $ operator returns a vector, you can easily calculate descriptive statistics on columns of a dataframe by applying your favorite vector function (like mean()).\n\nmean(survey$age)\n\n[1] 49\n\n\nWe can also use the $ to add new vectors to a dataframe\n\nsurvey$follow_up &lt;- c(T,F,T,F,F)\nsurvey\n\n\n\n\nindex\nsex\nage\nfollow_up\n\n\n\n1\nm\n99\nTRUE\n\n\n2\nm\n46\nFALSE\n\n\n3\nm\n23\nTRUE\n\n\n4\nf\n54\nFALSE\n\n\n5\nf\n23\nFALSE\n\n\n\n\n\n\nChanging column names is easy with a combination of names() and indexing\n\nnames(survey)[1] &lt;- \"ID\"\n\nsurvey\n\n\n\n\nID\nsex\nage\nfollow_up\n\n\n\n1\nm\n99\nTRUE\n\n\n2\nm\n46\nFALSE\n\n\n3\nm\n23\nTRUE\n\n\n4\nf\n54\nFALSE\n\n\n5\nf\n23\nFALSE\n\n\n\n\n\n\n\nA.13.1 Slice dataframes\nMatrices and dataframes can be sliced with [,]\n# Return row 1\ndf[1, ]\n\n\n# Return column 5 as vector\ndf[, 5]\n\n# Return column as data.frame\ndf[5]\n\n# Rows 1:5 and column 2\ndf[1:5, 2]\n\n# Single element\ndf[[1,2]]\n\nOr slice with subset\n\nsurvey_slice &lt;- subset(x = survey,\n      subset = age &lt; 50 &\n               sex == \"m\")\n\nsurvey_slice\n\n\n\n\n\nID\nsex\nage\nfollow_up\n\n\n\n2\n2\nm\n46\nFALSE\n\n\n3\n3\nm\n23\nTRUE\n\n\n\n\n\n\n\nA.13.2 Tibbles\n“Tibbles” are a new modern data frame. It keeps many important features of the original data frame\n\nA tibble never changes the input type.\nA tibble can have columns that are lists.\n\nA tibble can have non-standard variable names.\n\ncan start with a number or contain spaces. -to use this refer to these in a backtick.\n\n\nTibbles only print the first 10 rows and all the columns that fit on a screen. - Each column displays its data type\n\nThe way we make tibbles is very similar to making dataframes\n\nsurvey_tibble &lt;- tibble(\"index\" = c(1, 2, 3, 4, 5),\n                     \"sex\" = c(\"m\", \"m\", \"m\", \"f\", \"f\"),\n                     \"age\" = c(99, 46, 23, 54, 23))\n\n\n# Some R functions for looking at tibbles and dataframes\n\nhead(survey_tibble, n=2)\ntail(survey_tibble, n=1)\nnrow(survey_tibble)\nncol(survey_tibble)\ncolnames(survey_tibble)\nview(survey_tibble)\nglimpse(survey_tibble)\nstr(survey_tibble)\n\n\nA.13.3 Brackets with tibbles\nThe behaviour of single [] indexing with tibbles is slightly different.\nIn a dataframe [,1] extracts a single column as a vector, but with a tibble this conversion does not occur. Instead it returns as a tibble with a single column, not a vector.\nTo extract a vector we must use:\n\n# pull function\npull(survey_tibble, sex)\n\n# double brackets\nsurvey_tibble[[2]]\n\nhttps://tibble.tidyverse.org/\nhttps://cran.r-project.org/web/packages/tibble/vignettes/tibble.html\n\nA.14 Matrix, dataframe, tibble functions\nImportant functions for understanding matrices and dataframes.\n\n\n\n\n\n\nFunction\nDescription\n\n\n\nhead(x), tail(x)\nPrint the first few rows (or last few rows).\n\n\nView(x)\nOpen the entire object in a new window.\n\n\nnrow(x), ncol(x), dim(x)\nCount the number of rows and columns.\n\n\nrownames(), colnames(), names()\nShow the row (or column) names.\n\n\nstr(x), summary(x)\nShow the structure of the dataframe (i.e., dimensions and classes) and summary statistics.\n\n\n\nA.15 Functions\nFunctions are the tools of R. Each one helps us to do a different task.\nTake for example the function that we use to round a number to a certain number of digits - this function is called round\nHere’s an example:\n\nround(x  = 2.4326782647, digits = 2)\n\nWe start the command with the function name round. The name is followed by parentheses (). Within these we place the arguments for the function, each of which is separated by a comma.\nThe arguments:\n\nx = 2.4326782647 (the number we would like to round)\ndigits = 2 (the number of decimal places we would like to round to)\n\nArguments are the inputs we give to a function. These arguments are in the form name = value the name specifies the argument, and the value is what we are providing to define the input. That is the first argument x is the number we would like to round, it has a value of 2.4326782647. The second argument digits is how we would like the number to be rounded and we specify 2. There is no limit to how many arguments a function could have.\n\n\n\nCopy and paste the following code into the console.\n\n\n\n\nhelp(round)\n\nThe help documentation for round()should appear in the bottom right help panel. In the usage section, we see that round()takes the following form:\n\nround(x, digits = 0)\n\nIn the arguments section, there are explanations for each of the arguments. xis the number or vector where we wish to round values. digits is the number of decimal places to be used. In the description we can see that if no value is supplied for digits it will default to 0 or whole number rounding.\nRead the ‘Details’ section to find out what happens when rounding when the last digit is a 5.\nLet’s try an example and just change the required argument digits\n\n\n\nCopy and paste the following code into the console.\n\n\n\n\nround(x  = 2.4326782647)\n\n[1] 2\n\n\nNow we can change the additional arguments to produce a different set of numbers.\n\nround(x  = 2.4326782647, digits = 2)\n\n[1] 2.43\n\n\nThis time R has still rounded the number, but it has done so to a set number of ‘decimal places’.\nAlways remember to use the help documentation to help you understand what arguments a function requires.\n\nA.15.1 Storing the output of functions\nWhat if we need the answer from a function in a later calculation. The answer is to use the assignment operator again &lt;-.\nIn this example we assign values to two R objects that we can then call inside our R function as though we were putting numbers in directly.\n\n\n\nCopy and paste the following code into the console.\n\n\n\n\nnumber_of_digits &lt;- 3\n\nmy_number &lt;- 2.4326782647\n\nrounded_number &lt;- round(x  = my_number, \n                        digits = number_of_digits)\n\nWhat value is assigned to the R object rounded_number ?\n\n\nSolution\n\n\n[1] 2.433\n\n\n\nA.15.2 More fun with functions\nCopy and paste this:\n\nround(2.4326782647, 2)\n\nLooks like we don’t even have to give the names of arguments for a function to still work. This works because the function round expects us to give the number value first, and the argument for rounding digits second. But this assumes we know the expected ordering within a function, this might be the case for functions we use a lot. If you give arguments their proper names then you can actually introduce them in any order you want.\nTry this:\n\nround(digits = 2, x  = 2.4326782647)\n\nBut this gives a different answer\n\nround(2, 2.4326782647)\n\n\n\n\nRemember naming arguments overrides the position defaults\n\n\n\nHow do we know the argument orders and defaults? Well we get to know how a lot of functions work through practice, but we can also use help() .\n\nA.16 Packages\nWhen you install R you will have access to a range of functions including options for data wrangling and statistical analysis. The functions that are included in the default installation are typically referred to as Base R and there is a useful cheat sheet that shows many Base R functions here\nHowever, the power of R is that it is extendable and open source - anyone can create a new package that extends the functions of R.\nAn R package is a container for various things including functions and data. These make it easy to do very complicated protocols by using custom-built functions. Later we will see how we can write our own simple functions. Packages are a lot like new apps extending the functionality of what your phone can do.\n\nA.16.1 Loading packages\nTo use the functions from a package in our script they must be loaded before we call on the functions or data they contain. So the most sensible place to put library calls for packages is at the very top of our script.\n\nlibrary(package_name)\n\n\nA.16.2 Calling Functions from Packages\nAfter loading a package, you can call its functions using either function() or the full package_name::function_name() syntax. This allows you to specify the package explicitly when using a particular function.\n\nlibrary(dplyr)\n\nfilter(dataframe, condition)\n\ndplyr::filter(dataframe, conditions)\n\nCalling a function explicitly via its package can be useful for\n\nAvoiding Conflicts:\n\nSometimes, multiple packages may have functions with the same name. By explicitly specifying the package with package_name::, you avoid naming conflicts and ensure that R uses the function from the intended package.\n\nClarity:\n\nIt can make your code more transparent and easier to understand, especially in cases where the function’s origin is not immediately obvious. This is helpful for both yourself and others who read your code.\n\nThough it is still good practice to comment at the top of your script that this package is required even if you don’t include library(package)\n\n\nDebugging:\n\nWhen troubleshooting issues or debugging code, specifying the package source of a function can help pinpoint problems and ensure that the correct function is being used.\n\nA.17 Error\nThings will go wrong eventually, they always do…\nR is very pedantic, even the smallest typo can result in failure and typos are impossilbe to avoid. So we will make mistakes. One type of mistake we will make is an error. The code fails to run. The most common causes for an error are:\n\ntypos\nmissing commas\nmissing brackets\n\nThere’s nothing wrong with making lots of errors. The trick is not to panic or get frustrated, but to read the error message and our script carefully and start to debug (more on this later)…\n… and sometimes we need to walk away and come back later!\n\n\n\nTry typing the command help() into the R console, it should open a new tab on the bottom right.\n\n\nPut a function or package into the brackets to get help with a specific topic\n\n\n\n\n\n\n\ncourtesy of Allison Horst\n\n\n\nTo load packages we use the function library(). Typically you would start any analysis script by loading all of the packages you need.\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. This means the functions across the tidyverse are all designed to work together and make the process of data science easier.\n\nA.18 Using packages\nRun the below code to load the tidyverse package. You can do this regardless of whether you are using your own computer or the cloud.\n\nlibrary(tidyverse)\n\nYou will get what looks like an error message - it’s not. It’s just R telling you what it’s done. You should read this it gives you a full list of the packages it has made available to you.\nNow that we’ve loaded the tidyverse package we can use any of the functions it contains but remember, you need to run the library() function every time you start R.\n\n\nInstall the tidyverse. You DO NOT need to do this on RStudio Cloud.\n\nIn order to use a package, you must first install it. The following code installs the package tidyverse, a package we will use very frequently.\nIf you are working on your own computer, use the below code to install the tidyverse.\n\ninstall.packages(\"tidyverse\")\n\nYou only need to install a package once, however, each time you start R you need to load the packages you want to use, in a similar way that you need to install an app on your phone once, but you need to open it every time you want to use it.\n\n\n\n\n\nIf you get an error message that says something like “WARNING: Rtools is required to build R packages” you may need to download and install an extra bit of software called Rtools.\n\n\n\n\nA.19 Package updates\nIn addition to updates to R and R Studio, the creators of packages also sometimes update their code. This can be to add functions to a package, or it can be to fix errors. One thing to avoid is unintentionally updating an installed package. When you run install.packages() it will always install the latest version of the package and it will overwrite any older versions you may have installed. Sometimes this isn’t a problem, however, sometimes you will find that the update means your code no longer works as the package has changed substantially. It is possible to revert back to an older version of a package but try to avoid this anyway.\n\n\n\nTo avoid accidentally overwriting a package with a later version, you should never include install.packages() in your analysis scripts in case you, or someone else runs the code by mistake. Remember, the server will already have all of the packages you need for this course so you only need to install packages if you are using your own machine.\n\n\n\n\nA.20 Package conflicts\nThere are thousands of different R packages with even more functions. Unfortunately, sometimes different packages have the same function names. For example, the packages dplyr and MASS both have a function named select(). If you load both of these packages, R will produce a warning telling you that there is a conflict.\n\nlibrary(dplyr)\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    survey\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\npackage �dplyr� was built under R version 3.6.3\nAttaching package: �dplyr�\n\nThe following objects are masked from �package:stats�:\n\n    filter, lag\n\nThe following objects are masked from �package:base�:\n\n    intersect, setdiff, setequal, union\n\n\nAttaching package: �MASS�\n\nThe following object is masked from �package:dplyr�:\n\n    select\nIn this case, R is telling you that the function select() in the dplyr package is being hidden (or ‘masked’) by another function with the same name. If you were to try and use select(), R would use the function from the package that was loaded most recently - in this case it would use the function from MASS.\nIf you want to specify which package you want to use for a particular function you can use code in the format package::function, for example:\n\ndplyr::select()\nMASS::select()\n\n\n\n\nWhy do we get naming conflicts?\n\n\nThis is because R is open source software. Anyone can write and submit useful R packages. As a result it is impossible to make sure that there are NEVER any functions with identical names.\n\n\n\n\nA.21 Objects\nA large part of your coding will involve creating and manipulating objects. Objects contain stuff, and we made our first R objects in the previous chapter. The values contained in an object can be numbers, words, or the result of operations and analyses.You assign content to an object using &lt;-.\n\nA.21.1 Activity 1: Create some objects\nCopy and paste the following code into the console, change the code so that it uses your own name and age and run it. You should see that name, age, today, new_year, and data appear in the environment pane.\n\nname &lt;- \"emily\"\nage &lt;- 16 + 19 \ntoday &lt;- Sys.Date()\nnew_year &lt;- as.Date(\"2022-01-01\")\ndata &lt;- rnorm(n = 10, mean = 15, sd = 3)\n\nWhat command should we use if you need help to understand the function rnorm()?\n\n`\n\n\n\n\nObjects in the environment\n\n\n\nNote that in these examples, name,age, and new_year would always contain the values emily, 35, and the date of New Year’s Day 2021, however, today will draw the date from the operating system and data will be a randomly generated set of data so the values of these objects will not be static.\nAs a side note, if you ever have to teach programming and statistics, don’t use your age as an example because every time you have to update your teaching materials you get a reminder of the fragility of existence and your advancing age. 2021 update: I have now given up updating my age, I will remain forever 35.\nImportantly, objects can be involved in calculations and can interact with each other. For example:\n\nage + 10\nnew_year - today\nmean(data)\n\n[1] 45\nTime difference of -999 days\n[1] 15.31317\n\n\nFinally, you can store the result of these operations in a new object:\n\ndecade &lt;- age + 10\n\n\n\n\nYou may find it helpful to read &lt;- as contains, e.g., name contains the text emily.\n\n\n\nYou will constantly be creating objects throughout this course and you will learn more about them and how they behave as we go along, however, for now it is enough to understand that they are a way of saving values, that these values can be numbers, text, or the result of operations, and that they can be used in further operations to create new variables.\n\n\n\nYou may also see objects referred to as ‘variables’. There is a difference between the two in programming terms, however, they are used synonymously very frequently.\n\n\n\n\nA.22 Vectors\nWe have been working with R objects containing a single element of data, but we will more commonly work with vectors. A vector is a sequence of elements, all of the same data type. These could be logical, numerical, character etc.\n\nnumeric_vector &lt;- c(1,2,3)\n\ncharacter_vector &lt;- c(\"fruits\", \"vegetables\", \"seeds\")\n\nlogical_vector &lt;- c(TRUE, TRUE, FALSE)\n\nThe function c lets you ‘concatenate’ or link each of these separate elements together into a single vector.\n\nA.23 Dataframes and tibbles\nNo we have looked at R objects that contain:\n\nsingle elements of data\nmultiple elements of the same data type - vectors\n\nBut most often when we import data into R it is put into an object called a tibble which is a type of dataframe.\n\n\n\nA dataframe is data structure that organises data into a table. Dataframes can have a mix of different types of data in them. Each column in a dataframe is a different vector, and each row is a different element within the vectors.\n\n\n\nLet’s have a quick go at making our own tibble from scratch.\n\n# make some variables/ vectors\nperson &lt;- c(\"Mark\", \"Phil\", \"Becky\", \"Tony\")\n\nhobby &lt;- c(\"kickboxing\", \"coding\", \"dog walking\", \"car boot sales\")\n\nawesomeness &lt;- c(1,100,1,1)\n\n\n\n\nUse str() on an object or vector to find out important information, like the data type of each vector and how many elements it contains.\n\n\n\nNow we put these vectors together, where they become the variables in a new tibble using the function tibble()\n\n# make a tibble\nmy_data &lt;- tibble(person, hobby, awesomeness)\nmy_data\n\n# A tibble: 4 x 3\n  person hobby          awesomeness\n  &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;\n1 Mark   kickboxing               1\n2 Phil   coding                 100\n3 Becky  dog walking              1\n4 Tony   car boot sales           1\nHave a go at messing about with your script and figure out what each of the functions below does.\n\n# Some R functions for looking at tibbles and dataframes\n\nhead(my_data, n=2)\ntail(my_data, n=1)\nnrow(my_data)\nncol(my_data)\ncolnames(my_data)\nview(my_data)\nglimpse(my_data)\nstr(my_data)\n\n\nA.24 Organising data in wide and long formats\nThere are two main conventions for dataframes in R, these are wide and long formats.\n\nA wide data format does not repeat values in the first column, data relating to the same “measured thing” are found in different columns\nA long data format is where we have a different column for each type of thing we have measures in our data. Each variable has a unique column.\n\n\n\n\n\nA visual representation of long and wide format data shapes\n\n\n\nWhile neither wide or long data is more correct than the other, we will work with long data as it is clearer how many distinct types of variables there are in our data and the tools we will be using from the tidyverse are designed to work with long data.\n\nA.25 How to cite R and RStudio\nYou may be some way off writing a scientific report where you have to cite and reference R, however, when the time comes it is important to do so to the people who built it (most of them for free!) credit. You should provide separate citations for R, RStudio, and the packages you use.\nTo get the citation for the version of R you are using, simply run the citation() function which will always provide you with he most recent citation.\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2024). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2024},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nTo generate the citation for any packages you are using, you can also use the citation() function with the name of the package you wish to cite.\n\ncitation(\"tidyverse\")\n\nTo cite package 'tidyverse' in publications use:\n\n  Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R,\n  Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller\n  E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V,\n  Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to\n  the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686.\n  doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }\n\n\nTo generate the citation for the version of RStudio you are using, you can use the RStudio.Version() function:\n\nRStudio.Version()\n\nFinally, here’s an example of how that might look in the write-up of your method section:\n\nAnalysis was conducted using R ver 4.0.0 (R Core Team, 2020), RStudio (Rstudio Team, 2020), and the tidyverse range of packages (Wickham, 2017).\n\nAs noted, you may not have to do this for a while, but come back to this when you do as it’s important to give the open-source community credit for their work.\n\nA.26 Help and additional resources\n\n\n\n\nThe truth about programming\n\n\n\nGetting good at programming really means getting good trying stuff out, searching for help online, and finding examples of code to copy. If you are having difficulty with any of the exercises contained in this book then you can ask for help on Teams, however, learning to problem-solve effectively is a key skill that you need to develop throughout this course.\n\nUse the help documentation. If you’re struggling to understand how a function works, remember the ?function and help() command.\nIf you get an error message, copy and paste it in to Google - it’s very likely someone else has had the same problem.\nIf you are struggling to produce a particular output or process - try organising your google searches to include key terms such as “in R” or “tidyverse”. - e.g. “how to change character strings into NA values with tidyverse”\nThe official Cheatsheets are a great resource to keep bookmarked.\nRemember to ask for help\n\nIn addition to these course materials there are a number of excellent resources for learning R:\n\nStackOverflow\nR for Data Science\nSearch or use the #rstats hashtag on Twitter\n\n\n\nA.27 Debugging tips\nA large part of coding is trying to figure why your code doesn’t work and this is true whether you are a novice or an expert. As you progress through this course you should keep a record of mistakes you make and how you fixed them. In each chapter we will provide a number of common mistakes to look out for but you will undoubtedly make (and fix!) new mistakes yourself.\n\nA.27.1 Prevent errors\nRead console outputs as you go\nCheck that functions are producing the output you expect\nBuild complex code in simple stages\n\nA.27.2 Fix errors\n\nHave you loaded the correct packages for the functions you are trying to use? One very common mistake is to write the code to load the package, e.g., library(tidyverse) but then forget to run it.\nHave you made a typo? Remember data is not the same as DATA and t.test is not the same as t_test.\nIs there a package conflict? Have you tried specifying the package and function with package::function?\nIs it definitely an error? Not all red text in R means an error - sometimes it is just giving you a message with information.\n\nA.28 Activity 7: Test yourself\nQuestion 1. Why should you never include the code install.packages() in your analysis scripts? \nYou should use library() instead\nPackages are already part of Base R\nYou (or someone else) may accidentally install a package update that stops your code working\nYou already have the latest version of the package\n\n\nExplain This Answer\n\n\nRemember, when you run install.packages() it will always install the latest version of the package and it will overwrite any older versions of the package you may have installed.\n\n\nQuestion 2. What will the following code produce?\n\nrnorm(6, 50, 10)\n\n\nA dataset with 10 numbers that has a mean of 6 and an SD of 50\nA dataset with 6 numbers that has a mean of 50 and an SD of 10\nA dataset with 50 numbers that has a mean of 10 and an SD of 6\nA dataset with 50 numbers that has a mean of 10 and an SD of 6\n\n\nExplain This Answer\n\n\nThe default form for rnorm() is rnorm(n, mean, sd). If you need help remembering what each argument of a function does, look up the help documentation by running ?rnorm\n\n\nQuestion 3. If you have two packages that have functions with the same name and you want to specify exactly which package to use, what code would you use?\n\npackage::function\nfunction::package\nlibrary(package)\ninstall.packages(package)\n\n\nExplain This Answer\n\n\nYou should use the form package::function, for example dplyr::select. Remember that when you first load your packages R will warn you if any functions have the same name - remember to look out for this!\n\n\nQuestion 4. Which of the following is most likely to be an argument? \n&lt;-\nread_csv()\n35\nQuestion 5. An easy way to spot functions is to look for \nnumbers\ncomputers\nbrackets.\nQuestion 6. The job of &lt;- is to send the output from the function to a/an \nargument\nobject\nassignment.\nQuestion 7. A vector must always contain elements of the same data type (e.g logical, character, numeric) \nTRUE\nFALSE.\nQuestion 8. A dataframe/tibble must always contain elements of the same data t",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#matrices",
    "href": "r-basics.html#matrices",
    "title": "Appendix A — R Basics",
    "section": "\nA.11 Matrices",
    "text": "A.11 Matrices\nMatrices can be thought of as vectors with an added dimension attribute. This dimension attribute is a two-element integer vector specifying the number of rows and columns, which defines the shape and structure of the matrix.\n\n\n\nData frames are also two-dimensional but can store columns of different data types - matrices are simpler as they consist of elements of the same data type.\n\n\n\nMatrices are constructed “columns-first” so entries start in the “upper left” and and run down columns.\n\nm &lt;- matrix(1:6, nrow = 2, ncol = 3) \nm\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\nattributes(m)\n\n$dim\n[1] 2 3\n\n\nWe can create matrices in several ways:\n\nAdding a dim() to existing vectors\nColumn/row-binding vectors with cbind() and rbind()\n\n\nm &lt;- 1:6\n\ndim(m) &lt;- c(2,3)\n\nm\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\na &lt;- 1:2\nb &lt;- 3:4\nc &lt;- 5:6\n\nm &lt;- cbind(a,b,c)\nm\n\n     a b c\n[1,] 1 3 5\n[2,] 2 4 6\n\n\nYou will see how in this last operation column names were added to the matrix, we can add, change or remove column and rownames on a matrix with colnames() and rownames()\n\nrownames(m) &lt;- c(\"y\",\"z\")\nm\n\n  a b c\ny 1 3 5\nz 2 4 6",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#lists",
    "href": "r-basics.html#lists",
    "title": "Appendix A — R Basics",
    "section": "\nA.12 Lists",
    "text": "A.12 Lists\nLists are a versatile and fundamental data type in R. They set themselves apart from regular vectors by allowing you to store elements of different classes within the same list. This flexibility is what makes lists so powerful for various data structures and data manipulation tasks.\nYou can create lists explicitly using the list() function, which can take an arbitrary number of arguments. Lists, when combined with functions like the “apply” family, enable you to perform complex and versatile data manipulations and analyses in R. Lists are often used to represent heterogeneous data structures, such as datasets where different columns can have different data types and structures.\n\nl &lt;- list(1, \"apple\", TRUE )\nl\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"apple\"\n\n[[3]]\n[1] TRUE\n\n\nWe can also create empty lists of set lengths with the vector() function, this can be useful for preallocating memory for iterations - as we will see later\n\nl &lt;- vector(\"list\", length = 3)\nl\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n\nLists can also have names\n\nnames(l) &lt;- c(\"apple\",\"orange\",\"pear\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#dataframes",
    "href": "r-basics.html#dataframes",
    "title": "Appendix A — R Basics",
    "section": "\nA.13 Dataframes",
    "text": "A.13 Dataframes\nData frames are essential for storing tabular data in R and find extensive use in various statistical modeling and data analysis applications. They offer a structured way to manage and work with data in R, and packages like dplyr, developed by Hadley Wickham, provide optimized functions for efficient data manipulation with data frames.\nHere are some key characteristics and advantages of data frames:\n\nTabular Structure: Data frames are a type of list, where each element in the list represents a column. The number of rows in each column is the same, and this tabular structure makes them suitable for working with datasets.\nMixed Data Types: Unlike matrices, data frames can contain columns with different classes of objects. This flexibility allows you to handle real-world datasets that often include variables of different data types.\nColumn and Row Names: Data frames include column names, which describe the variables or predictors. Additionally, they have a special attribute called “row.names” that provides information about each row in the data frame.\nCreation and Conversion: Data frames can be created in various ways, such as reading data from files using functions like read.table() and read.csv(). You can also create data frames explicitly with data.frame().\nWorking with Data: Data frames are especially useful when working with datasets that require data cleaning, transformation, or merging. They provide a high level of data organization, and many R packages are designed to work seamlessly with data frames.\ndplyr: The dplyr package is optimized for efficient data manipulation with data frames. It offers a set of functions to perform data operations quickly and intuitively.\n\nData frames are a fundamental structure for managing tabular data in R. They excel in handling datasets with mixed data types and are essential for various data analysis and modeling tasks.\nTo create a dataframe from vectors we use the data.frame() function\n\nsurvey &lt;- data.frame(\"index\" = c(1, 2, 3, 4, 5),\n                     \"sex\" = c(\"m\", \"m\", \"m\", \"f\", \"f\"),\n                     \"age\" = c(99, 46, 23, 54, 23))\n\nThere is one key argument to data.frame() and similar functions called stringsAsFactors. By default, the data.frame() function will automatically convert any string columns to a specific type of object called a factor in R. A factor is a nominal variable that has a well-specified possible set of values that it can take on. For example, one can create a factor sex that can only take on the values “male” and “female”.\n\n\n\nSince R ver 4.0 release, stringsAsFactors is set FALSE by default!\n\n\n\nHowever, as I’m sure you’ll discover, having R automatically convert your string data to factors can lead to lots of strange results. For example: if you have a factor of sex data, but then you want to add a new value called other, R will yell at you and return an error. I hate, hate, HATE when this happens. While there are very, very rare cases when I find factors useful, I almost always don’t want or need them. For this reason, I avoid them at all costs.\nTo tell R to not convert your string columns to factors, you need to include the argument stringsAsFactors = FALSE when using functions such as data.frame()\n\nstr(survey)\n\n'data.frame':   5 obs. of  3 variables:\n $ index: num  1 2 3 4 5\n $ sex  : chr  \"m\" \"m\" \"m\" \"f\" ...\n $ age  : num  99 46 23 54 23\n\n\nTo access a specific column in a dataframe by name, you use the $ operator in the form df$name where df is the name of the dataframe, and name is the name of the column you are interested in. This operation will then return the column you want as a vector.\n\nsurvey$sex\n\n[1] \"m\" \"m\" \"m\" \"f\" \"f\"\n\n\nBecause the $ operator returns a vector, you can easily calculate descriptive statistics on columns of a dataframe by applying your favorite vector function (like mean()).\n\nmean(survey$age)\n\n[1] 49\n\n\nWe can also use the $ to add new vectors to a dataframe\n\nsurvey$follow_up &lt;- c(T,F,T,F,F)\nsurvey\n\n\n\n\nindex\nsex\nage\nfollow_up\n\n\n\n1\nm\n99\nTRUE\n\n\n2\nm\n46\nFALSE\n\n\n3\nm\n23\nTRUE\n\n\n4\nf\n54\nFALSE\n\n\n5\nf\n23\nFALSE\n\n\n\n\n\n\nChanging column names is easy with a combination of names() and indexing\n\nnames(survey)[1] &lt;- \"ID\"\n\nsurvey\n\n\n\n\nID\nsex\nage\nfollow_up\n\n\n\n1\nm\n99\nTRUE\n\n\n2\nm\n46\nFALSE\n\n\n3\nm\n23\nTRUE\n\n\n4\nf\n54\nFALSE\n\n\n5\nf\n23\nFALSE\n\n\n\n\n\n\n\nA.13.1 Slice dataframes\nMatrices and dataframes can be sliced with [,]\n# Return row 1\ndf[1, ]\n\n\n# Return column 5 as vector\ndf[, 5]\n\n# Return column as data.frame\ndf[5]\n\n# Rows 1:5 and column 2\ndf[1:5, 2]\n\n# Single element\ndf[[1,2]]\n\nOr slice with subset\n\nsurvey_slice &lt;- subset(x = survey,\n      subset = age &lt; 50 &\n               sex == \"m\")\n\nsurvey_slice\n\n\n\n\n\nID\nsex\nage\nfollow_up\n\n\n\n2\n2\nm\n46\nFALSE\n\n\n3\n3\nm\n23\nTRUE\n\n\n\n\n\n\n\nA.13.2 Tibbles\n“Tibbles” are a new modern data frame. It keeps many important features of the original data frame\n\nA tibble never changes the input type.\nA tibble can have columns that are lists.\n\nA tibble can have non-standard variable names.\n\ncan start with a number or contain spaces. -to use this refer to these in a backtick.\n\n\nTibbles only print the first 10 rows and all the columns that fit on a screen. - Each column displays its data type\n\nThe way we make tibbles is very similar to making dataframes\n\nsurvey_tibble &lt;- tibble(\"index\" = c(1, 2, 3, 4, 5),\n                     \"sex\" = c(\"m\", \"m\", \"m\", \"f\", \"f\"),\n                     \"age\" = c(99, 46, 23, 54, 23))\n\n\n# Some R functions for looking at tibbles and dataframes\n\nhead(survey_tibble, n=2)\ntail(survey_tibble, n=1)\nnrow(survey_tibble)\nncol(survey_tibble)\ncolnames(survey_tibble)\nview(survey_tibble)\nglimpse(survey_tibble)\nstr(survey_tibble)\n\n\nA.13.3 Brackets with tibbles\nThe behaviour of single [] indexing with tibbles is slightly different.\nIn a dataframe [,1] extracts a single column as a vector, but with a tibble this conversion does not occur. Instead it returns as a tibble with a single column, not a vector.\nTo extract a vector we must use:\n\n# pull function\npull(survey_tibble, sex)\n\n# double brackets\nsurvey_tibble[[2]]\n\nhttps://tibble.tidyverse.org/\nhttps://cran.r-project.org/web/packages/tibble/vignettes/tibble.html",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#matrix-dataframe-tibble-functions",
    "href": "r-basics.html#matrix-dataframe-tibble-functions",
    "title": "Appendix A — R Basics",
    "section": "\nA.14 Matrix, dataframe, tibble functions",
    "text": "A.14 Matrix, dataframe, tibble functions\nImportant functions for understanding matrices and dataframes.\n\n\n\n\n\n\nFunction\nDescription\n\n\n\nhead(x), tail(x)\nPrint the first few rows (or last few rows).\n\n\nView(x)\nOpen the entire object in a new window.\n\n\nnrow(x), ncol(x), dim(x)\nCount the number of rows and columns.\n\n\nrownames(), colnames(), names()\nShow the row (or column) names.\n\n\nstr(x), summary(x)\nShow the structure of the dataframe (i.e., dimensions and classes) and summary statistics.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#functions",
    "href": "r-basics.html#functions",
    "title": "Appendix A — R Basics",
    "section": "\nA.15 Functions",
    "text": "A.15 Functions\nFunctions are the tools of R. Each one helps us to do a different task.\nTake for example the function that we use to round a number to a certain number of digits - this function is called round\nHere’s an example:\n\nround(x  = 2.4326782647, digits = 2)\n\nWe start the command with the function name round. The name is followed by parentheses (). Within these we place the arguments for the function, each of which is separated by a comma.\nThe arguments:\n\nx = 2.4326782647 (the number we would like to round)\ndigits = 2 (the number of decimal places we would like to round to)\n\nArguments are the inputs we give to a function. These arguments are in the form name = value the name specifies the argument, and the value is what we are providing to define the input. That is the first argument x is the number we would like to round, it has a value of 2.4326782647. The second argument digits is how we would like the number to be rounded and we specify 2. There is no limit to how many arguments a function could have.\n\n\n\nCopy and paste the following code into the console.\n\n\n\n\nhelp(round)\n\nThe help documentation for round()should appear in the bottom right help panel. In the usage section, we see that round()takes the following form:\n\nround(x, digits = 0)\n\nIn the arguments section, there are explanations for each of the arguments. xis the number or vector where we wish to round values. digits is the number of decimal places to be used. In the description we can see that if no value is supplied for digits it will default to 0 or whole number rounding.\nRead the ‘Details’ section to find out what happens when rounding when the last digit is a 5.\nLet’s try an example and just change the required argument digits\n\n\n\nCopy and paste the following code into the console.\n\n\n\n\nround(x  = 2.4326782647)\n\n[1] 2\n\n\nNow we can change the additional arguments to produce a different set of numbers.\n\nround(x  = 2.4326782647, digits = 2)\n\n[1] 2.43\n\n\nThis time R has still rounded the number, but it has done so to a set number of ‘decimal places’.\nAlways remember to use the help documentation to help you understand what arguments a function requires.\n\nA.15.1 Storing the output of functions\nWhat if we need the answer from a function in a later calculation. The answer is to use the assignment operator again &lt;-.\nIn this example we assign values to two R objects that we can then call inside our R function as though we were putting numbers in directly.\n\n\n\nCopy and paste the following code into the console.\n\n\n\n\nnumber_of_digits &lt;- 3\n\nmy_number &lt;- 2.4326782647\n\nrounded_number &lt;- round(x  = my_number, \n                        digits = number_of_digits)\n\nWhat value is assigned to the R object rounded_number ?\n\n\nSolution\n\n\n[1] 2.433\n\n\n\nA.15.2 More fun with functions\nCopy and paste this:\n\nround(2.4326782647, 2)\n\nLooks like we don’t even have to give the names of arguments for a function to still work. This works because the function round expects us to give the number value first, and the argument for rounding digits second. But this assumes we know the expected ordering within a function, this might be the case for functions we use a lot. If you give arguments their proper names then you can actually introduce them in any order you want.\nTry this:\n\nround(digits = 2, x  = 2.4326782647)\n\nBut this gives a different answer\n\nround(2, 2.4326782647)\n\n\n\n\nRemember naming arguments overrides the position defaults\n\n\n\nHow do we know the argument orders and defaults? Well we get to know how a lot of functions work through practice, but we can also use help() .",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#packages",
    "href": "r-basics.html#packages",
    "title": "Appendix A — R Basics",
    "section": "\nA.16 Packages",
    "text": "A.16 Packages\nWhen you install R you will have access to a range of functions including options for data wrangling and statistical analysis. The functions that are included in the default installation are typically referred to as Base R and there is a useful cheat sheet that shows many Base R functions here\nHowever, the power of R is that it is extendable and open source - anyone can create a new package that extends the functions of R.\nAn R package is a container for various things including functions and data. These make it easy to do very complicated protocols by using custom-built functions. Later we will see how we can write our own simple functions. Packages are a lot like new apps extending the functionality of what your phone can do.\n\nA.16.1 Loading packages\nTo use the functions from a package in our script they must be loaded before we call on the functions or data they contain. So the most sensible place to put library calls for packages is at the very top of our script.\n\nlibrary(package_name)\n\n\nA.16.2 Calling Functions from Packages\nAfter loading a package, you can call its functions using either function() or the full package_name::function_name() syntax. This allows you to specify the package explicitly when using a particular function.\n\nlibrary(dplyr)\n\nfilter(dataframe, condition)\n\ndplyr::filter(dataframe, conditions)\n\nCalling a function explicitly via its package can be useful for\n\nAvoiding Conflicts:\n\nSometimes, multiple packages may have functions with the same name. By explicitly specifying the package with package_name::, you avoid naming conflicts and ensure that R uses the function from the intended package.\n\nClarity:\n\nIt can make your code more transparent and easier to understand, especially in cases where the function’s origin is not immediately obvious. This is helpful for both yourself and others who read your code.\n\nThough it is still good practice to comment at the top of your script that this package is required even if you don’t include library(package)\n\n\nDebugging:\n\nWhen troubleshooting issues or debugging code, specifying the package source of a function can help pinpoint problems and ensure that the correct function is being used.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#error",
    "href": "r-basics.html#error",
    "title": "Appendix A — R Basics",
    "section": "\nA.17 Error",
    "text": "A.17 Error\nThings will go wrong eventually, they always do…\nR is very pedantic, even the smallest typo can result in failure and typos are impossilbe to avoid. So we will make mistakes. One type of mistake we will make is an error. The code fails to run. The most common causes for an error are:\n\ntypos\nmissing commas\nmissing brackets\n\nThere’s nothing wrong with making lots of errors. The trick is not to panic or get frustrated, but to read the error message and our script carefully and start to debug (more on this later)…\n… and sometimes we need to walk away and come back later!\n\n\n\nTry typing the command help() into the R console, it should open a new tab on the bottom right.\n\n\nPut a function or package into the brackets to get help with a specific topic\n\n\n\n\n\n\n\ncourtesy of Allison Horst\n\n\n\nTo load packages we use the function library(). Typically you would start any analysis script by loading all of the packages you need.\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. This means the functions across the tidyverse are all designed to work together and make the process of data science easier.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#using-packages",
    "href": "r-basics.html#using-packages",
    "title": "Appendix A — R Basics",
    "section": "\nA.18 Using packages",
    "text": "A.18 Using packages\nRun the below code to load the tidyverse package. You can do this regardless of whether you are using your own computer or the cloud.\n\nlibrary(tidyverse)\n\nYou will get what looks like an error message - it’s not. It’s just R telling you what it’s done. You should read this it gives you a full list of the packages it has made available to you.\nNow that we’ve loaded the tidyverse package we can use any of the functions it contains but remember, you need to run the library() function every time you start R.\n\n\nInstall the tidyverse. You DO NOT need to do this on RStudio Cloud.\n\nIn order to use a package, you must first install it. The following code installs the package tidyverse, a package we will use very frequently.\nIf you are working on your own computer, use the below code to install the tidyverse.\n\ninstall.packages(\"tidyverse\")\n\nYou only need to install a package once, however, each time you start R you need to load the packages you want to use, in a similar way that you need to install an app on your phone once, but you need to open it every time you want to use it.\n\n\n\n\n\nIf you get an error message that says something like “WARNING: Rtools is required to build R packages” you may need to download and install an extra bit of software called Rtools.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#package-updates",
    "href": "r-basics.html#package-updates",
    "title": "Appendix A — R Basics",
    "section": "\nA.19 Package updates",
    "text": "A.19 Package updates\nIn addition to updates to R and R Studio, the creators of packages also sometimes update their code. This can be to add functions to a package, or it can be to fix errors. One thing to avoid is unintentionally updating an installed package. When you run install.packages() it will always install the latest version of the package and it will overwrite any older versions you may have installed. Sometimes this isn’t a problem, however, sometimes you will find that the update means your code no longer works as the package has changed substantially. It is possible to revert back to an older version of a package but try to avoid this anyway.\n\n\n\nTo avoid accidentally overwriting a package with a later version, you should never include install.packages() in your analysis scripts in case you, or someone else runs the code by mistake. Remember, the server will already have all of the packages you need for this course so you only need to install packages if you are using your own machine.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#package-conflicts",
    "href": "r-basics.html#package-conflicts",
    "title": "Appendix A — R Basics",
    "section": "\nA.20 Package conflicts",
    "text": "A.20 Package conflicts\nThere are thousands of different R packages with even more functions. Unfortunately, sometimes different packages have the same function names. For example, the packages dplyr and MASS both have a function named select(). If you load both of these packages, R will produce a warning telling you that there is a conflict.\n\nlibrary(dplyr)\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    survey\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\npackage �dplyr� was built under R version 3.6.3\nAttaching package: �dplyr�\n\nThe following objects are masked from �package:stats�:\n\n    filter, lag\n\nThe following objects are masked from �package:base�:\n\n    intersect, setdiff, setequal, union\n\n\nAttaching package: �MASS�\n\nThe following object is masked from �package:dplyr�:\n\n    select\nIn this case, R is telling you that the function select() in the dplyr package is being hidden (or ‘masked’) by another function with the same name. If you were to try and use select(), R would use the function from the package that was loaded most recently - in this case it would use the function from MASS.\nIf you want to specify which package you want to use for a particular function you can use code in the format package::function, for example:\n\ndplyr::select()\nMASS::select()\n\n\n\n\nWhy do we get naming conflicts?\n\n\nThis is because R is open source software. Anyone can write and submit useful R packages. As a result it is impossible to make sure that there are NEVER any functions with identical names.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#objects",
    "href": "r-basics.html#objects",
    "title": "Appendix A — R Basics",
    "section": "\nA.21 Objects",
    "text": "A.21 Objects\nA large part of your coding will involve creating and manipulating objects. Objects contain stuff, and we made our first R objects in the previous chapter. The values contained in an object can be numbers, words, or the result of operations and analyses.You assign content to an object using &lt;-.\n\nA.21.1 Activity 1: Create some objects\nCopy and paste the following code into the console, change the code so that it uses your own name and age and run it. You should see that name, age, today, new_year, and data appear in the environment pane.\n\nname &lt;- \"emily\"\nage &lt;- 16 + 19 \ntoday &lt;- Sys.Date()\nnew_year &lt;- as.Date(\"2022-01-01\")\ndata &lt;- rnorm(n = 10, mean = 15, sd = 3)\n\nWhat command should we use if you need help to understand the function rnorm()?\n\n`\n\n\n\n\nObjects in the environment\n\n\n\nNote that in these examples, name,age, and new_year would always contain the values emily, 35, and the date of New Year’s Day 2021, however, today will draw the date from the operating system and data will be a randomly generated set of data so the values of these objects will not be static.\nAs a side note, if you ever have to teach programming and statistics, don’t use your age as an example because every time you have to update your teaching materials you get a reminder of the fragility of existence and your advancing age. 2021 update: I have now given up updating my age, I will remain forever 35.\nImportantly, objects can be involved in calculations and can interact with each other. For example:\n\nage + 10\nnew_year - today\nmean(data)\n\n[1] 45\nTime difference of -999 days\n[1] 15.31317\n\n\nFinally, you can store the result of these operations in a new object:\n\ndecade &lt;- age + 10\n\n\n\n\nYou may find it helpful to read &lt;- as contains, e.g., name contains the text emily.\n\n\n\nYou will constantly be creating objects throughout this course and you will learn more about them and how they behave as we go along, however, for now it is enough to understand that they are a way of saving values, that these values can be numbers, text, or the result of operations, and that they can be used in further operations to create new variables.\n\n\n\nYou may also see objects referred to as ‘variables’. There is a difference between the two in programming terms, however, they are used synonymously very frequently.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#vectors-1",
    "href": "r-basics.html#vectors-1",
    "title": "Appendix A — R Basics",
    "section": "\nA.22 Vectors",
    "text": "A.22 Vectors\nWe have been working with R objects containing a single element of data, but we will more commonly work with vectors. A vector is a sequence of elements, all of the same data type. These could be logical, numerical, character etc.\n\nnumeric_vector &lt;- c(1,2,3)\n\ncharacter_vector &lt;- c(\"fruits\", \"vegetables\", \"seeds\")\n\nlogical_vector &lt;- c(TRUE, TRUE, FALSE)\n\nThe function c lets you ‘concatenate’ or link each of these separate elements together into a single vector.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#dataframes-and-tibbles",
    "href": "r-basics.html#dataframes-and-tibbles",
    "title": "Appendix A — R Basics",
    "section": "\nA.23 Dataframes and tibbles",
    "text": "A.23 Dataframes and tibbles\nNo we have looked at R objects that contain:\n\nsingle elements of data\nmultiple elements of the same data type - vectors\n\nBut most often when we import data into R it is put into an object called a tibble which is a type of dataframe.\n\n\n\nA dataframe is data structure that organises data into a table. Dataframes can have a mix of different types of data in them. Each column in a dataframe is a different vector, and each row is a different element within the vectors.\n\n\n\nLet’s have a quick go at making our own tibble from scratch.\n\n# make some variables/ vectors\nperson &lt;- c(\"Mark\", \"Phil\", \"Becky\", \"Tony\")\n\nhobby &lt;- c(\"kickboxing\", \"coding\", \"dog walking\", \"car boot sales\")\n\nawesomeness &lt;- c(1,100,1,1)\n\n\n\n\nUse str() on an object or vector to find out important information, like the data type of each vector and how many elements it contains.\n\n\n\nNow we put these vectors together, where they become the variables in a new tibble using the function tibble()\n\n# make a tibble\nmy_data &lt;- tibble(person, hobby, awesomeness)\nmy_data\n\n# A tibble: 4 x 3\n  person hobby          awesomeness\n  &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;\n1 Mark   kickboxing               1\n2 Phil   coding                 100\n3 Becky  dog walking              1\n4 Tony   car boot sales           1\nHave a go at messing about with your script and figure out what each of the functions below does.\n\n# Some R functions for looking at tibbles and dataframes\n\nhead(my_data, n=2)\ntail(my_data, n=1)\nnrow(my_data)\nncol(my_data)\ncolnames(my_data)\nview(my_data)\nglimpse(my_data)\nstr(my_data)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#organising-data-in-wide-and-long-formats",
    "href": "r-basics.html#organising-data-in-wide-and-long-formats",
    "title": "Appendix A — R Basics",
    "section": "\nA.24 Organising data in wide and long formats",
    "text": "A.24 Organising data in wide and long formats\nThere are two main conventions for dataframes in R, these are wide and long formats.\n\nA wide data format does not repeat values in the first column, data relating to the same “measured thing” are found in different columns\nA long data format is where we have a different column for each type of thing we have measures in our data. Each variable has a unique column.\n\n\n\n\n\nA visual representation of long and wide format data shapes\n\n\n\nWhile neither wide or long data is more correct than the other, we will work with long data as it is clearer how many distinct types of variables there are in our data and the tools we will be using from the tidyverse are designed to work with long data.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#how-to-cite-r-and-rstudio",
    "href": "r-basics.html#how-to-cite-r-and-rstudio",
    "title": "Appendix A — R Basics",
    "section": "\nA.25 How to cite R and RStudio",
    "text": "A.25 How to cite R and RStudio\nYou may be some way off writing a scientific report where you have to cite and reference R, however, when the time comes it is important to do so to the people who built it (most of them for free!) credit. You should provide separate citations for R, RStudio, and the packages you use.\nTo get the citation for the version of R you are using, simply run the citation() function which will always provide you with he most recent citation.\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2024). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2024},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nTo generate the citation for any packages you are using, you can also use the citation() function with the name of the package you wish to cite.\n\ncitation(\"tidyverse\")\n\nTo cite package 'tidyverse' in publications use:\n\n  Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R,\n  Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller\n  E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V,\n  Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to\n  the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686.\n  doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }\n\n\nTo generate the citation for the version of RStudio you are using, you can use the RStudio.Version() function:\n\nRStudio.Version()\n\nFinally, here’s an example of how that might look in the write-up of your method section:\n\nAnalysis was conducted using R ver 4.0.0 (R Core Team, 2020), RStudio (Rstudio Team, 2020), and the tidyverse range of packages (Wickham, 2017).\n\nAs noted, you may not have to do this for a while, but come back to this when you do as it’s important to give the open-source community credit for their work.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#help-and-additional-resources",
    "href": "r-basics.html#help-and-additional-resources",
    "title": "Appendix A — R Basics",
    "section": "\nA.26 Help and additional resources",
    "text": "A.26 Help and additional resources\n\n\n\n\nThe truth about programming\n\n\n\nGetting good at programming really means getting good trying stuff out, searching for help online, and finding examples of code to copy. If you are having difficulty with any of the exercises contained in this book then you can ask for help on Teams, however, learning to problem-solve effectively is a key skill that you need to develop throughout this course.\n\nUse the help documentation. If you’re struggling to understand how a function works, remember the ?function and help() command.\nIf you get an error message, copy and paste it in to Google - it’s very likely someone else has had the same problem.\nIf you are struggling to produce a particular output or process - try organising your google searches to include key terms such as “in R” or “tidyverse”. - e.g. “how to change character strings into NA values with tidyverse”\nThe official Cheatsheets are a great resource to keep bookmarked.\nRemember to ask for help\n\nIn addition to these course materials there are a number of excellent resources for learning R:\n\nStackOverflow\nR for Data Science\nSearch or use the #rstats hashtag on Twitter",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#debugging-tips",
    "href": "r-basics.html#debugging-tips",
    "title": "Appendix A — R Basics",
    "section": "\nA.27 Debugging tips",
    "text": "A.27 Debugging tips\nA large part of coding is trying to figure why your code doesn’t work and this is true whether you are a novice or an expert. As you progress through this course you should keep a record of mistakes you make and how you fixed them. In each chapter we will provide a number of common mistakes to look out for but you will undoubtedly make (and fix!) new mistakes yourself.\n\nA.27.1 Prevent errors\nRead console outputs as you go\nCheck that functions are producing the output you expect\nBuild complex code in simple stages\n\nA.27.2 Fix errors\n\nHave you loaded the correct packages for the functions you are trying to use? One very common mistake is to write the code to load the package, e.g., library(tidyverse) but then forget to run it.\nHave you made a typo? Remember data is not the same as DATA and t.test is not the same as t_test.\nIs there a package conflict? Have you tried specifying the package and function with package::function?\nIs it definitely an error? Not all red text in R means an error - sometimes it is just giving you a message with information.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "r-basics.html#activity-7-test-yourself",
    "href": "r-basics.html#activity-7-test-yourself",
    "title": "Appendix A — R Basics",
    "section": "\nA.28 Activity 7: Test yourself",
    "text": "A.28 Activity 7: Test yourself\nQuestion 1. Why should you never include the code install.packages() in your analysis scripts? \nYou should use library() instead\nPackages are already part of Base R\nYou (or someone else) may accidentally install a package update that stops your code working\nYou already have the latest version of the package\n\n\nExplain This Answer\n\n\nRemember, when you run install.packages() it will always install the latest version of the package and it will overwrite any older versions of the package you may have installed.\n\n\nQuestion 2. What will the following code produce?\n\nrnorm(6, 50, 10)\n\n\nA dataset with 10 numbers that has a mean of 6 and an SD of 50\nA dataset with 6 numbers that has a mean of 50 and an SD of 10\nA dataset with 50 numbers that has a mean of 10 and an SD of 6\nA dataset with 50 numbers that has a mean of 10 and an SD of 6\n\n\nExplain This Answer\n\n\nThe default form for rnorm() is rnorm(n, mean, sd). If you need help remembering what each argument of a function does, look up the help documentation by running ?rnorm\n\n\nQuestion 3. If you have two packages that have functions with the same name and you want to specify exactly which package to use, what code would you use?\n\npackage::function\nfunction::package\nlibrary(package)\ninstall.packages(package)\n\n\nExplain This Answer\n\n\nYou should use the form package::function, for example dplyr::select. Remember that when you first load your packages R will warn you if any functions have the same name - remember to look out for this!\n\n\nQuestion 4. Which of the following is most likely to be an argument? \n&lt;-\nread_csv()\n35\nQuestion 5. An easy way to spot functions is to look for \nnumbers\ncomputers\nbrackets.\nQuestion 6. The job of &lt;- is to send the output from the function to a/an \nargument\nobject\nassignment.\nQuestion 7. A vector must always contain elements of the same data type (e.g logical, character, numeric) \nTRUE\nFALSE.\nQuestion 8. A dataframe/tibble must always contain elements of the same data t",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Appendix B — Project workflows",
    "section": "",
    "text": "B.1 Setting up a new project\nYou should start a new R project when you begin working on a distinct task, research project, or analysis. This ensures that your work is well-organized, and it’s especially beneficial when you need to collaborate, share, or revisit the project later.\nTo create and open an R project in RStudio:\nThe new project will be created with a .Rproj file. You can open it by double-clicking on this file or by using the “File” menu in RStudio.\nThis will set up a dedicated workspace for your project, ensuring that the working directory and file paths are appropriately managed.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Project workflows</span>"
    ]
  },
  {
    "objectID": "projects.html#setting-up-a-new-project",
    "href": "projects.html#setting-up-a-new-project",
    "title": "Appendix B — Project workflows",
    "section": "",
    "text": "Go to “File” in the RStudio menu.\nSelect “New Project…”\nChoose a project type or create a new directory for the project.\nClick “Create Project.”\n\n\n\n\n\n\n\n\n\nAvoiding setwd() and Promoting Safe File Paths:\n\n\n\nTo maintain a clean and efficient workflow in R, it’s advisable to avoid using setwd() at the beginning of each script. This practice promotes the use of safe file paths and is particularly important for projects with multiple collaborators or when working across different computers.\n\n\n\nB.1.1 Absolute vs. Relative Paths:\nWhile absolute file paths provide an explicit way to locate resources, they have significant drawbacks, such as incompatibility and reduced reproducibility. Relative file paths, on the other hand, are relative to the current working directory, making them shorter, more portable, and more reproducible.\nAn Absolute file path is a path that contains the entire path to a file or directory starting from your Home directory and ending at the file or directory you wish to access e.g.\n/home/your-username/project/data/penguins_raw.csv\n\nIf you share files, another user won’t have the same directory structure as you, so they will need to recreate the file paths\nIf you alter your directory structure, you’ll need to rewrite the paths\nAn absolute file path will likely be longer than a relative path, more of the backslashes will need to be edited, so there is more scope for error.\n\nA Relative filepath is the path that is relative to the working directory location on your computer.\nWhen you use RStudio Projects, wherever the .Rproj file is located is set to the working directory. This means that if the .Rproj file is located in your project folder then the relative path to your data is:\ndata/penguins_raw.csv\nThis filepath is shorter and it means you could share your project with someone else and the script would run without any editing.\n\nB.1.2 Organizing Projects:\nA key aspect of this workflow is organizing each logical project into a separate folder on your computer. This ensures that files and scripts are well-structured, making it easier to manage your work.\n\nB.1.3 The here Package:\nTo further enhance this organization and ensure that file paths are independent of specific working directories, the here package comes into play. The here::here() function provided by this package builds file paths relative to the top-level directory of your project.\nmy_project.RProj/\n    |- data/\n    |   |- raw/\n    |       |- penguins_raw.csv\n    |   |- processed/\n    |- scripts/\n    |   |- analysis.R\n    |- results/\n\n\nIn the above project example you have raw data files in the data/raw directory, scripts in the scripts directory, and you want to save processed data in the data/processed directory.\nTo access this data using a relative filepath we need:\n\nraw_data &lt;- read.csv(\"data/raw/penguins_raw.csv\")\n\nTo access this data with here we provide the directories and desired file, and here() builds the required filepath starting at the top level of our project each time\n\nlibrary(here)\n\nraw_data &lt;- read.csv(here(\"data\", \"raw\", \"penguins.csv\"))\n\n\n\n\n\n\n\nhere and Rmarkdown\n\n\n\nOne quirk of working in a .Rmd Rmarkdown file is that when you “knit” all code is compiled with the working directory as the folder that .Rmd file lives in, but if you are working in a script .R or in a live session then the default working directory is the top level of the project file. This frustrating and confusing process can lead to errors when attempting to compile documents.\nBUT if you use the here package then this default behaviour is overridden. The working directory when knitting will be the top-level .Rproj location again!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Project workflows</span>"
    ]
  },
  {
    "objectID": "projects.html#blank-slates",
    "href": "projects.html#blank-slates",
    "title": "Appendix B — Project workflows",
    "section": "\nB.2 Blank slates",
    "text": "B.2 Blank slates\nWhen working on data analysis and coding projects in R, it’s crucial to ensure that your analysis remains clean, reproducible, and free from hidden dependencies.\nHidden dependencies are elements in your R session that might not be immediately apparent but can significantly impact the reliability and predictability of your work.\nFor example many data analysis scripts start with the command rm(list = ls()). While this command clears user-created objects from the workspace, it leaves hidden dependencies as it does not reset the R session, and can cause issues such as:\n\nHidden Dependencies: Users might unintentionally rely on packages or settings applied in the current session.\nIncomplete Reset: Package attachments made with library() persist, and customized options remain set.\nWorking Directory: The working directory is not affected, potentially causing path-related problems in future scripts.\n\n\nB.2.1 Restart R sessions\nRestarting R sessions and using scripts as your history is a best practice for maintaining a clean, reproducible, and efficient workflow. It addresses the limitations of rm(list = ls()) by ensuring a complete reset and minimizing hidden dependencies, enhancing code organization, and ensuring your analysis remains robust and predictable across sessions and when shared with others.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Project workflows</span>"
    ]
  },
  {
    "objectID": "projects.html#export",
    "href": "projects.html#export",
    "title": "Appendix B — Project workflows",
    "section": "\nC.1 Export",
    "text": "C.1 Export\nEach of these packages and functions has the inverse “write” function to produce files in a variety of formats from R objects.\n\n\n\n\n\n\nFunction\nDescription\n\n\n\nwrite_csv()\nCSV file format\n\n\nwrite_tsv()\nTSV (Tab-Separated Values) file format\n\n\nwrite_delim()\nUser-specified delimited files",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Project workflows</span>"
    ]
  },
  {
    "objectID": "projects.html#scripts",
    "href": "projects.html#scripts",
    "title": "Appendix B — Project workflows",
    "section": "\nC.2 Scripts",
    "text": "C.2 Scripts\nTo ensure clarity and understanding, begin your script with a brief description of its purpose. This description will serve as a reference point for anyone who accesses your script. Even if you make updates later on, having this initial description will help maintain clarity and context, preventing confusion when revisiting the code in the future.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Project workflows</span>"
    ]
  },
  {
    "objectID": "projects.html#organised-scripts",
    "href": "projects.html#organised-scripts",
    "title": "Appendix B — Project workflows",
    "section": "\nC.3 Organised scripts",
    "text": "C.3 Organised scripts\nLoad all necessary packages at the beginning of your script. It’s common to start with basic packages and then add more specialized libraries as your analysis progresses. However, it’s crucial to load all required packages at the beginning of your script. This practice ensures that when you or someone else needs to run the script again, all necessary libraries are readily available, preventing issues in the middle of execution due to unrecognized functions. Small coding details matter.\nName your code sections and use them for quick navigation. As your code grows, it may become extensive and challenging to manage. To keep it organized, divide your code into sections, each with a specific name, which can be folded or unfolded for easy navigation. You can also use the ‘drop-up’ menu at the bottom of the script screen to move between sections.\nTo create a new code section, insert “####” or “—-” at the end of a comment that marks the beginning of a new section.\n\n\n\n\n\n\n\n\nI understand, we all have good intentions, but we often neglect the task of thoroughly commenting our code. I’ve made that promise to myself many times, but even now, I struggle to do it consistently. Why, you ask? Here are a few reasons:\n\nI often tell myself that the analysis itself is more crucial.\nI believe I understand my own code.\nI usually don’t have immediate collaborators who need to use my code.\n\nHowever, these arguments are somewhat shortsighted. The reality is that:\n\nThe most valuable and relevant analysis loses its value if neither you nor others can understand it. (More on this below)\nWhile you may know what you’re doing at the moment, it won’t feel the same way in a month or two when you’ve moved on to another project, and someone innocently asks you about how you defined a critical variable. Our memory is unreliable. It’s important not to rely on it for every piece of code you produce.\nEven if you don’t have active collaborators at the time of your analysis, someone will eventually need to use your code. You won’t be in the same position forever. You’re creating a legacy that, someday, someone will rely on, no matter how distant that day may seem right now.\n\nSo, what makes code good and reproducible?\n\nThoughtful and clear comments.\nCode that is logical and efficient.\nCode that has been appropriately timed and tested.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Project workflows</span>"
    ]
  },
  {
    "objectID": "projects.html#use-style-guides",
    "href": "projects.html#use-style-guides",
    "title": "Appendix B — Project workflows",
    "section": "\nC.4 Use style guides",
    "text": "C.4 Use style guides\nConsider using a style guide, such as the tidyverse style guide, is a beneficial practice for several reasons:\nConsistency: A style guide enforces consistent code formatting and naming conventions throughout your project. This consistency improves code readability and makes it easier for you and others to understand the code. When you have multiple people working on a project, a shared style guide ensures that everyone’s code looks similar, reducing confusion and errors.\nReadability: Following a style guide leads to more readable code. Code is often read more frequently than it is written, so making it easy to understand is crucial. The tidyverse style guide, for example, emphasizes clear and self-explanatory code, improving comprehension for both current and future users. Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread\nCollaboration: When working with a team, adhering to a common style guide makes it easier to collaborate. It reduces the friction associated with different team members using varying coding styles and preferences. This streamlines code reviews and simplifies the process of maintaining and extending the codebase.\nError Reduction: A style guide can help identify and prevent common coding errors. It promotes best practices and can include guidelines for avoiding pitfalls and potential issues. This reduces the likelihood of bugs and enhances the overall quality of the code.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Project workflows</span>"
    ]
  },
  {
    "objectID": "projects.html#separate-your-scripts",
    "href": "projects.html#separate-your-scripts",
    "title": "Appendix B — Project workflows",
    "section": "\nC.5 Separate your scripts",
    "text": "C.5 Separate your scripts\nSeparating your analysis into distinct scripts for different steps is a sound practice in data analysis. Each script can focus on a specific task or step, making your work more organized and understandable.\nYou can use the source() function in R to run previous dependencies, ensuring that you can reproduce your work easily. Additionally, for computationally intensive processes or when dealing with large datasets, you can save and load intermediate results in RDS format. This approach not only conserves memory but also saves time when re-running your analysis.\nproject_folder/\n│\n├── data/\n│   ├── data.csv\n│   ├── processed_data.rds\n│\n├── scripts/\n│   ├── data_preparation.R\n│   ├── data_analysis.R\n│   ├── visualization.R\n│   ├── helper_functions.R\n│\n├── output/\n│   ├── result.csv\n│\n├── README.md\n│\n├── project.Rproj",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Project workflows</span>"
    ]
  },
  {
    "objectID": "projects.html#activity",
    "href": "projects.html#activity",
    "title": "Appendix B — Project workflows",
    "section": "\nC.6 Activity",
    "text": "C.6 Activity\nUsing the Tidyverse style guide for help, how could you improve the layout and readability of this script?\n\n# Install and load necessary packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\npenguins_clean &lt;- janitor::clean_names(penguins_raw)\n\n\n## Data is selected by species, island, culmen length and depth and flipper, then NAs are dropped and a new column is made of length/depth and the mean is summaries for flipper length and length/depth ratio\npenguins_clean |&gt; select(species, island, culmen_length_mm, culmen_depth_mm, flipper_length_mm)  |&gt; drop_na(culmen_length_mm, culmen_depth_mm, flipper_length_mm) |&gt; mutate(culmen_ratio = culmen_length_mm / culmen_depth_mm) |&gt; group_by(species, island) |&gt; summarise(mean_flipper_length = mean(flipper_length_mm), mean_culmen_ratio = mean(culmen_ratio)) |&gt; arrange(species, island) -&gt; penguins_culmen_ratio\n\n## View summary table\nprint(penguins_culmen_ratio)\n\n\n### Data visualization \npenguins_clean |&gt;\n  ggplot(aes(x = culmen_length_mm, y = culmen_depth_mm, color = species)) +\n          geom_point() +\n                labs(x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\") +\n                      theme_minimal()\n\n\n\nCheck your script\n\n\n# Packages ----\n# Install and load necessary packages\nlibrary(tidyverse)\nlibrary(janitor)\n# Loads the penguins dataset\nlibrary(palmerpenguins)\n\n\n# Clean the data ----\npenguins_raw &lt;- janitor::clean_names(penguins_raw)\n\n# Analysis----\n# Data exploration and manipulation to make culmen ratio\npenguins_culmen_ratio &lt;- penguins_raw |&gt; \n  select(species, island, \n         culmen_length_mm, \n         culmen_depth_mm, \n         flipper_length_mm)  |&gt; \n  drop_na(culmen_length_mm, \n          culmen_depth_mm, \n          flipper_length_mm) |&gt; \n  mutate(culmen_ratio = culmen_length_mm / culmen_depth_mm) |&gt;\n  group_by(species, island) |&gt;\n  summarise(mean_flipper_length = mean(flipper_length_mm), \n            mean_culmen_ratio = mean(culmen_ratio)) |&gt;\n  arrange(species, island)\n\n# View summary table\nprint(penguins_culmen_ratio)\n\n# Plots----\n# Data visualization using ggplot2\npenguins_clean |&gt;\n  ggplot(aes(x = culmen_length_mm, \n             y = culmen_depth_mm, \n             color = species)) +\n  geom_point() +\n  labs(x = \"Culmen Length (mm)\", \n       y = \"Culmen Depth (mm)\") +\n  theme_minimal()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Project workflows</span>"
    ]
  },
  {
    "objectID": "projects.html#naming-things",
    "href": "projects.html#naming-things",
    "title": "Appendix B — Project workflows",
    "section": "\nC.7 Naming things",
    "text": "C.7 Naming things\nSo as we are reading things into and out of our environment we come to filenames.\nSo, what makes a good file name? Well, there are a few key principles to keep in mind:\n1. Machine Readable: Your file names should be machine-readable, meaning they work well with regular expressions and globbing. This allows you to search for files using keywords, with the help of regex and the stringr package. To achieve this, avoid spaces, punctuation, accented characters, and case sensitivity. This makes searching for files and filtering lists based on names easier in the future.\n\n\n\n\n\n\n\n\n2. Easy to Compute On: File names should be structured consistently, with each part of the name serving a distinct purpose and separated by delimiters. This structure makes it easy to extract information from file names, such as splitting them into meaningful components.\n\n\n\n\n\n\n\n\n3. Human Readable: A good file name should be human-readable. It should provide a clear indication of what the file contains, just by looking at its name. It’s important that even someone unfamiliar with your work can easily understand the file’s content.\n\n\n\n\n\n\n\n\n4. Compatible with Default Ordering: Your computer will automatically sort your files, whether you like it or not. To ensure files are sorted sensibly, consider the following:\n\nPut something numeric at the beginning of the file name. If the order of sourcing files matters, state when the file was created. If not, indicate the logical order of the files.\nUse the YYYY-MM-DD format for dates (it’s an ISO 8601 standard). This format helps maintain chronological order, even for Americans.\nLeft-pad numbers with zeroes to avoid incorrect sorting (e.g., 01 not 1).\n\n\n\n\n\n\n\n\n\nTaking these simple but effective steps can significantly enhance your workflow and help your colleagues as well. Remember, good file names are a small change that can make a big difference in your productivity.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Project workflows</span>"
    ]
  },
  {
    "objectID": "projects.html#reading",
    "href": "projects.html#reading",
    "title": "Appendix B — Project workflows",
    "section": "\nC.8 Reading",
    "text": "C.8 Reading\n\nUsing the here package\nData Organisation in Spreadsheets",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Project workflows</span>"
    ]
  },
  {
    "objectID": "tidy-data.html",
    "href": "tidy-data.html",
    "title": "Appendix C — Tidy data",
    "section": "",
    "text": "C.1 Why tidy data?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "tidy-data.html#why-tidy-data",
    "href": "tidy-data.html#why-tidy-data",
    "title": "Appendix C — Tidy data",
    "section": "",
    "text": "The data cleaning and analysis tools in R work best with data that is “tidy”\n“Tidy” data has a clear and consistent structure, untidy data can be “messy” in lots of different ways",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "tidy-data.html#using-pivot-functions",
    "href": "tidy-data.html#using-pivot-functions",
    "title": "Appendix C — Tidy data",
    "section": "\nC.2 Using pivot functions",
    "text": "C.2 Using pivot functions\nWhat do we do if the data we are working with in R isn’t “tidy”?\nThere are functions found as part of the tidyverse that can help us to reshape data.\n\ntidyr::pivot_wider() - from long to wide format\ntidyr::pivot_longer() - from wide to long format\n\n\n\n\n\nReshaping data with pivot\n\n\n\n\n country &lt;- c(\"x\", \"y\", \"z\")\n yr1960 &lt;-  c(10, 20, 30)\n yr1970 &lt;-  c(13, 23, 33)\n yr2010 &lt;-  c(15, 25, 35)\n\ncountry_data &lt;- tibble(country, yr1960, yr1970, yr2010)\ncountry_data\n\n\n\n\ncountry\nyr1960\nyr1970\nyr2010\n\n\n\nx\n10\n13\n15\n\n\ny\n20\n23\n25\n\n\nz\n30\n33\n35\n\n\n\n\n\n\n\npivot_longer(data = country_data,\n             cols = yr1960:yr2010,\n             names_to = \"year\",\n             names_prefix = \"yr\",\n             values_to = \"metric\")\n\n\n\n\n\nReshaping data with pivot\n\n\n\nTo save these changes to your data format, you must assign this to an object, and you have two options\n\nUse the same name as the original R object, this will overwrite the original with the new format\nUse a new name for the reformatted data both R objects will exist in your Environment\n\nNeither is more correct than the other but be aware of what you are doing.\n\nC.2.1 Overwrite the original object\n\ncountry_data &lt;- pivot_longer(data = country_data,\n             cols = yr1960:yr2010,\n             names_to = \"year\",\n             names_prefix = \"yr\",\n             values_to = \"metric\")\n\n\nC.2.2 Create a new r object\n\nlong_country_data &lt;- pivot_longer(data = country_data,\n             cols = yr1960:yr2010,\n             names_to = \"year\",\n             names_prefix = \"yr\",\n             values_to = \"metric\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "tidy-data.html#reading",
    "href": "tidy-data.html#reading",
    "title": "Appendix C — Tidy data",
    "section": "\nC.3 Reading",
    "text": "C.3 Reading\n\nTidy data\n\n\n\n\n\nWickham, H. (2023). Tidyverse: Easily install and load the tidyverse. https://tidyverse.tidyverse.org",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Gorman, K., Williams, T., & Fraser, W. (2014). Ecological sexual\ndimorphism and environmental variability within a community of antarctic\npenguins (genus pygoscelis). PLos One, 9(3), e90081.\nhttps://doi.org/10.1371/journal.pone.0090081\n\n\nHorst, A., Hill, A., & Gorman, K. (2022). Palmerpenguins: Palmer\narchipelago (antarctica) penguin data. https://allisonhorst.github.io/palmerpenguins/\n\n\nRobinson, D., Hayes, A., & Couch, S. (2024). Broom: Convert\nstatistical objects into tidy tibbles. https://broom.tidymodels.org/\n\n\nWickham, H. (2023). Tidyverse: Easily install and load the\ntidyverse. https://tidyverse.tidyverse.org",
    "crumbs": [
      "Appendices",
      "References"
    ]
  }
]